{
    "code": 1,
    "content": "  This is the second in a series of posts on how to build a Data Science Portfolio.  If you like this and want to know when the next post in the series is released, you can subscribe at the bottom of the page. You can read the first post in this series here: Building a data science portfolio: Storytelling with data. Blogging can be a fantastic way to demonstrate your skills, learn topics in more depth, and build an audience.  There are quite a few examples of data science and programming blogs that have helped their authors land jobs or make important connections.  Blogging is one of the most important things that any aspiring programmer or data scientist should be doing on a regular basis. Unfortunately, one very arbitrary barrier to blogging can be knowing how to setup a blog in the first place.  In this post, we\u2019ll cover how to create a blog using Python, how to create posts using Jupyter notebook, and how to deploy the blog live using Github Pages.  After reading this post, you\u2019ll be able to create your own data science blog, and author posts in a familiar and simple interface. Fundamentally, a static site is just a folder full of HTML files.  We can run a server that allows others to connect to this folder and retrieve files.  The nice thing about this is that it doesn\u2019t require a database or any other moving parts, and it\u2019s very easy to host on sites like Github.  It\u2019s a great idea to have your blog be a static site, because it makes maintaining it very simple.  One way to create a static site is to manually edit HTML, then upload the folder full of HTML to a server.  In this scenario, you would at the minimum need an index.html file.  If your website URL was thebestblog.com, and visitors visited http://www.thebestblog.com, they would be shown the contents of index.html.  Here\u2019s how a folder of HTML might look for thebestblog.com: On the above site, visiting http://www.thebestblog.com/first-post.html would show you the content in first-post.html, and so on.  first-post.html might look like this: You might immediately notice a few problems with manually editing HTML: Generally, when you\u2019re blogging, you want to focus on the content, not spend time fighting with HTML.  Thankfully, you can create a blog without hand editing HTML using tools known as static site generators. Static site generators allow you to write blog posts in simple formats, usually markdown, then define some settings.  The generators then convert your posts into HTML automatically.  Using a static site generator, we\u2019d be able to dramatically simplify first-post.html into first-post.md: This is much easier to manage than the HTML file!  Common elements, like the title and the footer, can be placed into templates, so they can be easily changed. There are a few different static site generators.  The most popular is called Jekyll, and is written in Ruby.  Since we\u2019ll be making a data science blog, we want a static site generator that can process Jupyter notebooks. Pelican is a static site generator that is written in Python that can take in Jupyter notebook files and convert them to HTML blog posts.  Pelican also makes it easy to deploy our blog to Github Pages, where other people can read our blog. Before we get started, here\u2019s a repo that\u2019s an example of what we\u2019ll eventually get to. If you don\u2019t have Python installed, you\u2019ll need to do some preliminary setup before we get started.  Here are setup instructions for Python.  We recommend using Python 3.5.  Once you have Python installed: Once you\u2019ve done the preliminary setup, you\u2019re ready to create your blog!  Run pelican-quickstart in jupyter-blog to start an interactive setup sequence for your blog.  You\u2019ll get a sequence of questions that will help you setup your blog properly.  For most of the questions, it\u2019s okay to just hit Enter and accept the default value.  The only ones you should fill out are the title of the website, the author of the website, n for the URL prefix, and the timezone.  Here\u2019s an example: After running pelican-quickstart, you should have two new folders in jupyter-blog, content, and output, along with several files, such as pelicanconf.py and publishconf.py.  Here\u2019s an example of what should be in the folder: Pelican doesn\u2019t support writing blog posts using Jupyter by default \u2013 we\u2019ll need to install a plugin that enables this behavior.  We\u2019ll install the plugin as a git submodule to make it easier to manage.  If you don\u2019t have git installed, you can find instructions here.  Once you have git installed: You should now have a .gitmodules file and a plugins folder: In order to activate the plugin, we\u2019ll need to modify pelicanconf.py and add these lines at the bottom: The lines tell Pelican to activate the plugin when generating HTML. Once the plugin is installed, we can create a first post: Here\u2019s an explanation of the fields: You\u2019ll need to copy in a notebook file, and create an ipynb-meta file whenever you want to add a new post to your blog. Once you\u2019ve created the notebook and the meta file, you\u2019re ready to generate your blog HTML files.  Here\u2019s an example of what the jupyter-blog folder should look like now: In order to generate HTML from our post, we\u2019ll need to run Pelican to convert the notebooks to HTML, then run a local server to be able to view them: You should be able to browse a listing of all the posts in your blog, along with the specific post you created. Github Pages is a feature of Github that allows you to quickly deploy a static site and let anyone access it using a unique URL.  In order to set it up, you\u2019ll need to: A Github page will display whatever HTML files are pushed up to the master branch of the repository username.github.io at the URL username.github.io (the repository name and the URL are the same). First, we\u2019ll need to modify Pelican so that URLs point to the right spot: If you want to store your actual notebooks and other files in the same Git repo as a Github Page, you can use git branches. We\u2019ll need to add the content of the blog to the master branch for Github Pages to work properly.  Currently, the HTML content is inside the folder output, but we need it to be at the root of the repository, not in a subfolder.  We can use the ghp-import tool for this: Whenever you make a change to your blog, just re-run the pelican content -s publishconf.py, ghp-import and git push commands above, and your Github Page will be updated. We\u2019ve come a long way!  You now should be able to author blog posts and push them to Github Pages.  Anyone should be able to access your blog at username.github.io (replacing username with your Github username).  This gives you a great way to show off your data science portfolio. As you write more posts and gain an audience, you may want to dive more into a few areas: If you liked this, you might like to read the other posts in our \u2018Build a Data Science Porfolio\u2019 series: ",
    "link": "https://www.dataquest.io/blog/how-to-setup-a-data-science-blog/",
    "title": "Building a data science portfolio: Making a data science blog"
}{
    "code": 0,
    "content": "  Brad Klingenberg is the Director of Styling Algorithms at Stitch Fix in San Francisco. His team uses data and algorithms to improve the selection of merchandise sent to clients. Prior to joining Stitch Fix, Brad worked with data and predictive analytics at financial and technology companies. He studied applied mathematics at the University of Colorado at Boulder and earned his PhD in Statistics at Stanford University in 2012.  Brad: Stitch Fix is an online personal styling service for women. Our mission is to help our clients look, feel and be their best selves. When clients sign up for the service they tell us about their preferences for fit and style and then we send them a personalized selection of five items that we call a \u201cFix\u201d. They keep only what they like and send back the rest. We choose what to send by combining recommendation algorithms and human curation to pick the perfect inventory for our clients. Brad:I lead the styling algorithms team at Stitch Fix. Broadly speaking our goal is help pick items that our clients will love. We develop algorithms for recommending inventory to our stylists using what we know about our clients, our inventory, and feedback from past Fixes. We also study problems like finding the best stylist for a client and understanding, measuring, and optimizing the role of human selection in a recommendation system.  Dataquest: People have labeled Stitch Fix as the Netflix for clothes and you guys clearly have a lot of talented folks from Netflix.  Brad: Several people at Stitch Fix, including Chief Algorithms Officer Eric Colson, came from Netflix. I met Eric while consulting there as a graduate student. The parallel in recommendations between the two companies is probably clear. There are, however, a number of ways in which the Stitch Fix problem is different. For example, at Stitch Fix we commit to our recommendations through the physical delivery of merchandise to clients - there\u2019s a big cost to being wrong. We also use the expert judgement of human stylists to curate our recommendations. There\u2019s a lot to admire about the Netflix culture. At Stitch Fix, we\u2019ve cultivated a passionate focus on our clients. As a data scientist you have the opportunity to directly improve outcomes for our clients in a variety of ways - from helping us make decisions about buying inventory, to optimizing our operations, and of course through influencing what we send to clients, to name just a few examples. The opportunity to impact the business in such tangible ways helps attract talented data scientists and engineers. Dataquest: Stitch Fix is somewhat unique in the approach towards human-computer symbiosis since a human actually make the final recommendation not a computer.  Brad: Personal styling is a complicated problem. Selecting the best items for a client depends on using a variety of structured and unstructured data. In this process we\u2019ve found humans and machines to be complementary. For example, statistical models are great for making predictions from subtle patterns in large datasets in a way that a human never could. On the other hand, humans excel at processing unstructured data like images and written requests from clients, and at thinking about the client in a holistic way. Our stylists also cultivate personal relationships with the clients, by adding a personalized note to every Fix, for example. Having humans in the loop is very effective, and it\u2019s great for our clients. But it\u2019s also complicated. The selections of our stylists add another feedback loop that we can use to improve our algorithms, while also introducing bias. For example, suppose that our stylists never send heavy winter sweaters to clients in hot climates. This is very likely the right to do for our clients, but we\u2019d never actually observe how the sweaters would have done - this data would be effectively censored by our stylists. This can be challenging when it comes time to train a model. Datquest: A notorious problem when working with recommendation systems is the cold start problem.  Brad: It would be hard to build a realistic recommendation system in a vacuum. There are variety of popular approaches (e.g. content-based, collaborative filtering, and factorization-based methods) that are better suited to some problems than others. It really depends on your data. For example, the level of sparsity or co-occurrence in your data will likely influence whether collaborative filtering or factorization methods are viable. For the aspiring data scientist I would recommend trying to work with real data. There are some freely available datasets like MovieLens. It is also worth remembering that making recommendations shares many similarities with other supervised learning problems. General experience with building statistical models and the cycle of selecting, implementing and evaluating models will be valuable, almost regardless of the domain. Dataquest: You have a PhD in Statistics and have spent several years working as a quantitative analyst.  Brad: Learn the basics of applied statistics! When building models it is important to be practical - start with simple things. If you\u2019re still a student try to get an internship in industry at a company where you\u2019ll get to work with interesting data (http://multithreaded.stitchfix.com/blog/2015/03/31/advice-for-data-scientists/).  A well-rounded data scientist will also be a good enough engineer to get things done. Try to get experience with languages like R and python. Understanding the basics of convex optimization will help you make practical choices when fitting models. It is useful to write some optimization code yourself, if you haven\u2019t before. An aptitude for framing problems is as important as technical skills. Seeking out experience will help you develop this skill. Try to learn about different problems where different tools are preferred. How are they different? In academia many research problems offer good experience for framing business problems. Try to develop a comfort with ambiguity. Dataquest: Data science is incredibly broad and it\u2019s a field that attracts people of all backgrounds.  Brad: The data science team at Stitch Fix has grown dramatically over the last two years. We have hired people from a variety of backgrounds - from physics to psychology. In many cases a specific background is less important than a history of working with data and a solid foundation in building and applying statistical models. Our team at Stitch Fix is very collaborative and our new hires get to work alongside the many other data scientists on the team while they get up to speed.  Several recent hires have written about their experiences: Brad: I think it is easy to be attracted to complexity, and that foundational tools like linear models with their extensions and basic inference are often underappreciated. These are really the workhorses for most of data science. As I\u2019ve talked about on our tech blog, linear models have many virtues in their simplicity - they are interpretable, easy to extend and and easy to scale. While seemingly primitive, they are also a surprisingly effective tool in an enormous range of problems. Dataquest: Stitch Fix has blogged a bit about the Julia language, which has a bold mission of building the best language for technical computing from the ground up.  Brad:Julia is not widely used in production at Stitch Fix. We do have some Julia enthusiasts and have had great guest lectures by folks like Jonn Myles White. While I don\u2019t personally use Julia in a serious way I think it\u2019s an exciting project and I hope it continues to offer more competition to R and Python as the dominant languages of data science. Brad: A great place to start is Multithreaded - our tech blog. We also have an associated speaker series in at our offices in San Francisco that\u2019s open to public. You\u2019ll can also often find at a number of conferences like PyData, MLconf, HCOMP, Strata and others. Brad: Yes, we\u2019re hiring! https://www.stitchfix.com/careers Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/data-science-at-stitch-fix/",
    "title": "Data Science at Stitch Fix: An interview with Brad Klingenberg"
}{
    "code": 0,
    "content": "  To highlight how Dataquest has changed people\u2019s lives,  we\u2019ve started a new blog series called User Stories where we interview our users to learn more about their personal journeys. \nIn this post, we interview Helena Tan, Data Analyst at Fitbit.  Helena worked in a variety of roles in private equity and investment banking after graduating from the University of Waterloo.  She wanted to become more technical, and work on more challenging problems.  She started learning Python and data analysis using Dataquest.  Her progress has been incredible, and she landed a job at Fitbit soon after she started learning.  She's been continuing to learn on Dataquest, and is continually improving her skillset.  She's very passionate about sharing what she's learned, and recently presented at the Grace Hopper Conference.\n I have always been passionate about using data to tell a good story. Back in my finance career, I loved coming up with investment recommendations based on industry trends and company specific financial data. I moved to Silicon Valley in 2013 where I got the opportunity to work with some of the most brilliant data scientists and machine learning experts as a Business Intelligence Analyst at a startup. At the same time, I had a chance to witness how the company applied machine learning to bring tremendous value to its clients. As a result, I became very inspired to use data to build great products and solve problems that make a difference to people\u2019s life! Based on my understanding, there are three major aspects of Data Science: programming skills, statistics knowledge and product sense/domain knowledge. Fortunately, I have a degree in Statistics, so I didn\u2019t have to start from scratch on the statistics part. However, after I was inspired to use data to build products, it didn\u2019t take long to realize I couldn\u2019t go very far in pursuing my passion without picking up a programming language like Python. That\u2019s how the learning journey started and that\u2019s when I discovered Dataquest!  If I had to summarize it to one word, it would be \u201cpersistence\u201d. I don\u2019t remember how many jobs I applied before I landed my first job in Silicon Valley, but I know it was a lot! Each data science job is unique. It varies by the team, company and industry. It takes time to find a good match among one\u2019s skill sets, career goals and the requirements from the role/team.  As I was on my journey to pick up the programming skills necessary to create data products, the Fitbit opportunity came up. I still remember my first round of technical interviews with Fitbit. It was my very first Python programming test and it happened only 3 weeks after I started the Python course at Dataquest! Luckily, I was able to apply what I learned at Dataquest at the interview and eventually got the job. I don\u2019t have a definite answer in terms of the minimum level of skills. It depends on the team, company and one\u2019s past experience. For someone who is completely new to the tech industry, I would suggest they start with learning SQL and make sure to demonstrate strong quantitative analytical skills during interviews.  Technical interviews for Data Science jobs can have different formats depending on the team. Common formats include real-time coding, case studies and take-home projects. For real-time coding and case studies, candidates are often given some simple data sets and hypothetical scenarios, then they are asked to work through a problem using their preferred coding language in the interview. The take-home projects are usually more comprehensive. Candidates are given a more complex set of data and they are often asked to discover insights and build predictive models based on the data. For people who are trying to break in the field, the first couple of technical interviews could be hard, but with a lot of practice, they get easier! I am still in the learning process. There are so many interesting things to explore in this domain. For my own process, I enjoy learning by doing. It keeps me motivated. I have a list of product ideas that I feel excited about, so I go find the learning materials and pick up the techniques required to build them. I find learning this way most fun and enjoyable. However, sometimes I need to switch back to a more structured learning process to make sure I understand the theoretical concepts behind the applications. Data Science has become a very generic term. A data science role can vary from data engineer, machine learning engineer to business analyst. As a result, candidates nowadays need to spend more time to understand what type of problems they want to tackle instead of focusing on a job title. I am excited about improving my skills to do two things: process and analyze more open data from the web, and visualize the results in an interactive fashion. I think Dataquest got me covered! :) Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/user-story-helena-tan/",
    "title": "How Helena Tan went from Investment Banking to Data Science"
}{
    "code": 0,
    "content": "  To highlight how Dataquest has changed people\u2019s lives,  we\u2019ve started a new blog series called User Stories where we interview our users to learn more about their personal journey and how we\u2019ve helped them get where they needed to. \nIn this post, we interview Rob Hipps, Data Analyst at 3M.  Rob went through a Master's program in Data Science, and has an interesting take on how in-person and online learning intersect.  He later went through a data science internship at Verisk before landing his first fulltime data science role at 3M.\n It happened almost organically. I would see a data set and provide answers to the questions that were being posed. I found myself wanting to dive a bit deeper into the data, and I wanted to learn skills that would allow me to do that. In my efforts to gain more knowledge about data science, I found that there were some clear holes in what I was learning. I looked for resources to help me. and I found Dataquest. For me, I need to be able to be interact with the data in a hands-on way, and Dataquest offers this style of learning. After about an hour or two I signed up for a premium membership.  Dataquest has helped me become a more complete data analyst and helped me with understanding how to take a project from start to finish. The way the missions are set up makes it easy to see how real data problems should be solved. Further, if I run into problems with a project, I can almost always find a quick module that helps tackle the issue. I applied to the internship and was invited to an on-site interview. We discussed a number of analytical areas and how I would fit with their team.  Thanks! It was a pretty long interview process that covered a number of skills. Most important to them was SQL and an ability to work with data analysis tools such as R and Python. They have some interesting ideas about how to leverage these skills, and I am excited to bring what I have learned to the team. The internship was paramount in landing the job. What I will be doing for 3M draws many parallels to my role at Verisk as an intern.  I found that both experiences complimented each other, and I relied on both to gain the skills I was looking for to further my understanding of data science. For example, in my Master\u2019s program R is being taught more heavily than Python. I wanted to become more balanced, so I sought out a way to strengthen my skills with Python. Dataquest helped me learn Python and even strengthened my skills with R. What makes Dataquest unique is the ability to choose what you want to learn and when, opposed to having hard deadlines on material that is catered to a larger audience.  I think people make the assumption that programming is too difficult for them to learn. Programming is like anything else, if you put in the time you will see results. I would challenge anyone who thinks programming is beyond them to try Dataquest. I am most excited for the new content you guys are continuing to roll out! Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/user-story-rob-hipps/",
    "title": "How Rob Hipps used Dataquest to land a data analysis job"
}{
    "code": 1,
    "content": "  There are times when working on data science problems with your local machine just doesn\u2019t cut it anymore.  Maybe your computer is old, and can\u2019t work with larger datasets.  Or maybe you want to be able to access your work from anywhere, and collaborate with others.  Or maybe you have an analysis that will take a long time to run, and you don\u2019t want to tie up your own computer.  In these cases, it is useful to run Jupyter on a server, so you can access it through a browser. We can do this easily by using Docker.  See our earlier post on how to setup a data science environment using Docker for background.  This post builds on that one, and sets up Docker and Jupyter on a server. The first step is to initialize a server.  You can requisition servers in the cloud using sites like Amazon Web Services, or DigitalOcean.  Both of these are cloud hosting providers \u2013 they have a pool of servers, and they rent them out by the hour to people who want to run programs.  When you rent a server, you get full access to it, and can install and run programs, just like on your local computer.  A nice way to think of a server is as a computer that is physically located somewhere else.  There\u2019s nothing special about a server, it\u2019s just another computer running an operating system that you can access over the internet. To initialize our server, we\u2019ll use DigitalOcean, because it\u2019s a bit cheaper and simpler than Amazon Web Services. With DigitalOcean, starting a server is fast and easy.  The first step is to sign up for an account here.  The second step is to create a droplet, which is the DigitalOcean term for a server.  There\u2019s a good tutorial on how to do that here.  As you go through the tutorial, make sure you select a droplet with at least 2GB of RAM, pick Ubuntu 14.04 64-bit as the OS, and make sure you add an ssh key by following this tutorial. Once you create the server, be sure to note down the IP address.  It should be at the top left of the page after you create the server, and will look like 162.243.1.205.  This address is how your server is located on the internet, and you\u2019ll be using it to access it later. The IP address should be at the top left Once the server is setup, you can login to it via SSH, or Secure Shell.  SSH enables you to remotely login to a machine using a special key to authenticate (you generated this key in the tutorial earlier).  Once you\u2019re logged in, you\u2019ll be given access to the command line on the server.  You can execute commands just like you could on your local computer. See this tutorial for how to SSH into your server.  If you get an authentication error, you may need to add the right SSH key first.  You can do this using the ssh-add command on Linux and OSX.  Type ssh-add /home/vik/.ssh/id_rsa (replace /home/vik/.ssh/id_rsa with the path to your key) to add the right ssh key, and retry the tutorial steps to ssh into the server. We\u2019re currently logged into the server as the root user.  This user has full access to everything on the system.  This is fine for an initial login, but can have security issues as we install software using the root user.  We\u2019ll instead create a new user, called ds (short for data science).  Follow this tutorial to create a new user.  Remember to use the user name ds instead of demo, as it is in the tutorial. When you\u2019re done making a new user, quit the SSH session by typing exit.  Login to the server again as the new user ds by typing ssh [email\u00a0protected]/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */_IP (replace SERVER_IP with the IP address of your server) . Once you\u2019re connected to the server via ssh as the user ds, you should see a command prompt.  This prompt will let you execute any bash shell commands, such as cd to change directories, and mv to move files.  The server is running Ubuntu 14.04, which is based on Linux. The first thing we\u2019ll do is install Docker.  Follow the instructions here to install Docker on the server. Make sure to run sudo usermod -aG docker ds after installing Docker, then exit the ssh session using exit, and ssh back in with ssh [email\u00a0protected]/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */_IP. The second step is to make a directory on the machine to hold your notebook files.  You should be logged in as the ds user, so just type mkdir -p /home/ds/notebooks to create the notebooks directory. The next step is to download the Docker image you want.  Here are our currently available data science images: You can download the images by typing docker pull IMAGE_NAME. One the image is downloaded, you can start your Docker container with docker run -d -p 8888:8888 -v /home/ds/notebooks:/home/ds/notebooks dataquestio/python3-starter.  Replace dataquestio/python3-starter with the name of the image you want to use. Once this is executed, we\u2019ll have a Jupyter server running at port 8888 on our local machine. Nginx is an HTTP and reverse proxy server.  What this means is that is can take requests from the internet and pass them through to our Jupyter server.  Nginx can make security and other things needed to have a public-facing web application easier. The first thing we\u2019ll need to do is install nginx, which can be done with sudo apt-get install nginx. You should now be able to type your server IP address into the browser\u2019s address bar, and see a generic message that says \u201cWelcome to nginx\u201d. We\u2019ll want to encrypt traffic between our browser and Jupyter notebook.  This will prevent anyone from intercepting sensitive data or passwords we send back and forth. In order to do this, we\u2019ll need to generate an SSL Certificate.  This certificate enables our browser to establish a secure link to a remote server. We\u2019ll have to generate and install the certificate on our server.  You can do this by following step one in this guide.  Be sure to specify your server IP address instead of the domain name.  Make sure to stop after step one \u2013 we have a different nginx configuration we\u2019ll need to do. We\u2019ll also generate a password for nginx, so only you can access your Jupyter notebooks. To do this, we\u2019ll run: The final step is to setup the nginx configuration. First, we\u2019ll remove the default nginx welcome message by typing sudo rm /etc/nginx/sites-enabled/default. Then, we\u2019ll make our own configuration file.  Type sudo nano /etc/nginx/sites-enabled/ds.  This will open a text editor.  Paste the following into the text editor: Finally, close and save the file by hitting Control + X and then Y. Then, type sudo service nginx to restart nginx. Now, you can visit your server IP address in your browser.  You\u2019ll automatically be redirected to the secure version of the site.  You may see a screen like this (this is from Chrome): You may see this error This is happening because we used a self-signed SSL certificate.  Most sites use certificates signed by certificate authorities, but that costs money, and we don\u2019t want to spend that on a site few people will use.  Bypass this screen to keep connecting to the site \u2013 with Chrome, you can do this by clicking Advanced, then Proceed at the bottom. Once you click proceed, you\u2019ll see a prompt for a password.  Type ds as the username, and enter the password you setup for nginx in an earlier step. You'll see this prompt You\u2019ve now finished all the needed setup!  You should be through to the Jupyter server, which will look like this: You'll see this prompt You can upload and download data files and notebooks via the Jupyter interface, and should be able to start doing data science in the cloud. Please read our earlier post for information on how to install new packages or modify the Docker containers. If you want to build on the images we\u2019ve discussed in this post, you can contribute to our Github repository here, which contains the Dockerfiles.  We welcome improvements to our current images, or the addition of new images focusing on tools other than Python.",
    "link": "https://www.dataquest.io/blog/digitalocean-docker-data-science/",
    "title": "DigitalOcean & Docker for Data Science"
}{
    "code": 1,
    "content": "  This is the first in a series of posts on how to build a Data Science Portfolio.  If you like this and want to know when the next post in the series is released, you can subscribe at the bottom of the page. Data science companies are increasingly looking at portfolios when making hiring decisions.  One of the reasons for this is that a portfolio is the best way to judge someone\u2019s real-world skills.  The good news for you is that a portfolio is entirely within your control.  If you put some work in, you can make a great portfolio that companies are impressed by. The first step in making a high-quality portfolio is to know what skills to demonstrate.  The primary skills that companies want in data scientists, and thus the primary skills they want a portfolio to demonstrate, are: Any good portfolio will be composed of multiple projects, each of which may demonstrate 1-2 of the above points.  This is the first post in a series that will cover how to make a well-rounded data science portfolio.  In this post, we\u2019ll cover how to make your first project for a data science portfolio, and how to tell an effective story using data.  At the end, you\u2019ll have a project that will help demonstrate your ability to communicate, and your ability to reason about data. Data science is fundamentally about communication.  You\u2019ll discover some insight in the data, then figure out an effective way to communicate that insight to others, then sell them on the course of action you propose.  One of the most critical skills in data science is being able to tell an effective story using data.  An effective story can make your insights much more compelling, and help others understand your ideas. A story in the data science context is a narrative around what you found, how you found it, and what it means.  An example might be the discovery that your company\u2019s revenue has dropped 20% in the last year.  It\u2019s not enough to just state that fact \u2013 you\u2019ll have to communicate why revenue dropped, and how to potentially fix it. The main components of storytelling with data are: The best tool to effectively tell a story with data is Jupyter notebook.  If you\u2019re unfamiliar, here\u2019s a good tutorial.  Jupyter notebook allows you to interactively explore data, then share your results on various sites, including Github.  Sharing your results is helpful both for collaboration, and so others can extend your analysis. We\u2019ll use Jupyter notebook, along with Python libraries like Pandas and matplotlib in this post. The first step in creating a project is to decide on your topic.  You want the topic to be something you\u2019re interested in, and are motivated to explore.  It\u2019s very obvious when people are making projects just to make them, and when people are making projects because they\u2019re genuinely interested in exploring the data.  It\u2019s worth spending extra time on this step, so ensure that you find something you\u2019re actually interested in. A good way to find a topic is to browse different datasets and seeing what looks interesting.  Here are some good sites to start with: In real-world data science, you often won\u2019t find a nice single dataset that you can browse.  You might have to aggregate disparate data sources, or do a good amount of data cleaning.  If a topic is very interesting to you, it\u2019s worth doing the same here, so you can show off your skills better. For the purposes of this post, we\u2019ll be using data about New York city public schools, which can be found here. It\u2019s important to be able to take the project from start to finish.  In order to do this, it can be helpful to restrict the scope of the project, and make it something we know we can finish.  It\u2019s easier to add to a finished project than to complete a project that you just can\u2019t seem to ever get enough motivation to finish. In this case, we\u2019ll look at the SAT scores of high schoolers, along with various demographic and other information about them.  The SAT, or Scholastic Aptitude Test, is a test that high schoolers take in the US before applying to college.  Colleges take the test scores into account when making admissions decisions, so it\u2019s fairly important to do well on.  The test is divided into 3 sections, each of which is scored out of 800 points.  The total score is out of 2400 (although this has changed back and forth a few times, the scores in this dataset are out of 2400).  High schools are often ranked by their average SAT scores, and high SAT scores are considered a sign of how good a school district is. There have been allegations about the SAT being unfair to certain racial groups in the US, so doing this analysis on New York City data will help shed some light on the fairness of the SAT. We have a dataset of SAT scores here, and a dataset that contains information on each high school here.  These will form the base of our project, but we\u2019ll need to add more information to create compelling analysis. Once you have a good topic, it\u2019s good to scope out other datasets that can enhance the topic or give you more depth to explore.  It\u2019s good to do this upfront, so you have as much data as possible to explore as you\u2019re building your project.  Having too little data might mean that you give up on your project too early. In this case, there are several related datasets on the same website that cover demographic information and test scores. Here are the links to all of the datasets we\u2019ll be using: All of these datasets are interrelated, and we\u2019ll be able to combine them before we do any analysis. Before diving into analyzing the data, it\u2019s useful to research some background information.  In this case, we know a few facts that will be useful: In order to really understand the context of the data, you\u2019ll want to spend time exploring and reading about the data.  In this case, each link above has a description of the data, along with the relevant columns.  It looks like we have data on the SAT scores of high schoolers, along with other datasets that contain demographic and other information. We can run some code to read in the data.  We\u2019ll be using Jupyter notebook to explore the data.  The below code will: Once we\u2019ve read the data in, we can use the head method on DataFrames to print the first 5 lines of each DataFrame: We can start to see some useful patterns in the datasets: In order to work with the data more easily, we\u2019ll need to unify all the individual datasets into a single one.  This will enable us to quickly compare columns across datasets.  In order to do this, we\u2019ll first need to find a common column to unify them on.  Looking at the output above, it appears that DBN might be that common column, as it appears in multiple datasets. If we google DBN New York City Schools, we end up here, which explains that the DBN is a unique code for each school.  When exploring datasets, particularly government ones, it\u2019s often necessary to do some detective work to figure out what each column means, or even what each dataset is. The problem now is that two of the datasets, class_size, and hs_directory, don\u2019t have a DBN field.  In the hs_directory data, it\u2019s just named dbn, so we can just rename the column, or copy it over into a new column called DBN.  In the class_size data, we\u2019ll need to try a different approach. The DBN column looks like this: If we look at the class_size data, here\u2019s what we\u2019d see in the first 5 rows: As you can see above, it looks like the DBN is actually a combination of CSD, BOROUGH, and SCHOOL CODE.  For those unfamiliar with New York City, it is composed of 5 boroughs.  Each borough is an organizational unit, and is about the same size as a fairly large US City.  DBN stands for District Borough Number.  It looks like CSD is the District, BOROUGH is the borough, and when combined with the SCHOOL CODE, forms the DBN.  There\u2019s no systematized way to find insights like this in data, and it requires some exploration and playing around to figure out. Now that we know how to construct the DBN, we can add it into the class_size and hs_directory datasets: One of the most potentially interesting datasets to look at is the dataset on student, parent, and teacher surveys about the quality of schools.  These surveys include information about the perceived safety of each school, academic standards, and more.  Before we combine our datasets, let\u2019s add in the survey data.  In real-world data science projects, you\u2019ll often come across interesting data when you\u2019re midway through your analysis, and will want to incorporate it.  Working with a flexible tool like Jupyter notebook will allow you to quickly add some additional code, and re-run your analysis. In this case, we\u2019ll add the survey data into our data dictionary, and then combine all the datasets afterwards.  The survey data consists of 2 files, one for all schools, and one for school district 75.  We\u2019ll need to write some code to combine them.  In the below code, we\u2019ll: Once we have the surveys combined, there\u2019s an additional complication.  We want to minimize the number of columns in our combined dataset so we can easily compare columns and figure out correlations.  Unfortunately, the survey data has many columns that aren\u2019t very useful to us: 5 rows \u00d7 2773 columns We can resolve this issue by looking at the data dictionary file that we downloaded along with the survey data.  The file tells us the important fields in the data: We can then remove any extraneous columns in survey: Making sure you understand what each dataset contains, and what the relevant columns are can save you lots of time and effort later on. If we take a look at some of the datasets, including class_size, we\u2019ll immediately see a problem: There are several rows for each high school (as you can see by the repeated DBN and SCHOOL NAME fields).  However, if we take a look at the sat_results dataset, it only has one row per high school: In order to combine these datasets, we\u2019ll need to find a way to condense datasets like class_size to the point where there\u2019s only a single row per high school.  If not, there won\u2019t be a way to compare SAT scores to class size.  We can accomplish this by first understanding the data better, then by doing some aggregation.  With the class_size dataset, it looks like GRADE and PROGRAM TYPE have multiple values for each school.  By restricting each field to a single value, we can filter most of the duplicate rows.  In the below code, we: Next, we\u2019ll need to condense the demographics dataset.  The data was collected for multiple years for the same schools, so there are duplicate rows for each school.  We\u2019ll only pick rows where the schoolyear field is the most recent available: We\u2019ll need to condense the math_test_results dataset.  This dataset is segmented by Grade and by Year.  We can select only a single grade from a single year: Finally, graduation needs to be condensed: Data cleaning and exploration is critical before working on the meat of the project.  Having a good, consistent dataset will help you do your analysis more quickly. Computing variables can help speed up our analysis by enabling us to make comparisons more quickly, and enable us to make comparisons that we otherwise wouldn\u2019t be able to do.  The first thing we can do is compute a total SAT score from the individual columns SAT Math Avg. Score, SAT Critical Reading Avg. Score, and SAT Writing Avg. Score.  In the below code, we: Next, we\u2019ll need to parse out the coordinate locations of each school, so we can make maps.  This will enable us to plot the location of each school.  In the below code, we: Now, we can print out each dataset to see what we have: Now that we\u2019ve done all the preliminaries, we can combine the datasets together using the DBN column.  At the end, we\u2019ll have a dataset with hundreds of columns, from each of the original datasets.  When we join them, it\u2019s important to note that some of the datasets are missing high schools that exist in the sat_results dataset.  To resolve this, we\u2019ll need to merge the datasets that have missing rows using the outer join strategy, so we don\u2019t lose data.  In real-world data analysis, it\u2019s common to have data be missing.  Being able to demonstrate the ability to reason about and handle missing data is an important part of building a portfolio.   You can read about different types of joins here. In the below code, we\u2019ll: Now that we have our full DataFrame, we have almost all the information we\u2019ll need to do our analysis.  There are a few missing pieces, though.  We may want to correlate the Advanced Placement exam results with SAT scores, but we\u2019ll need to first convert those columns to numbers, then fill in any missing values: Then, we\u2019ll need to calculate a school_dist column that indicates the school district of the school.  This will enable us to match up school districts and plot out district-level statistics using the district maps we downloaded earlier: Finally, we\u2019ll need to fill in any missing values in full with the mean of the column, so we can compute correlations: A good way to explore a dataset and see what columns are related to the one you care about is to compute correlations.  This will tell you which columns are closely related to the column you\u2019re interested in.  We can do this via the corr method on Pandas DataFrames.  The closer to 0 the correlation, the weaker the connection.  The closer to 1, the stronger the positive correlation, and the closer to -1, the stronger the negative correlation`: This gives us quite a few insights that we\u2019ll need to explore: Each of these items is a potential angle to explore and tell a story about using the data. Before we dive into exploring the data, we\u2019ll want to set the context, both for ourselves, and anyone else that reads our analysis.  One good way to do this is with exploratory charts or maps.  In this case, we\u2019ll map out the positions of the schools, which will help readers understand the problem we\u2019re exploring. In the below code, we:\n* Setup a map centered on New York City.\n* Add a marker to the map for each high school in the city.\n* Display the map. This map is helpful, but it\u2019s hard to see where the most schools are in NYC.  Instead, we\u2019ll make a heatmap: Heatmaps are good for mapping out gradients, but we\u2019ll want something with more structure to plot out differences in SAT score across the city.  School districts are a good way to visualize this information, as each district has its own administration.  New York City has several dozen school districts, and each district is a small geographic area. We can compute SAT score by school district, then plot this out on a map.  In the below code, we\u2019ll: We\u2019ll now we able to plot the average SAT score in each school district.  In order to do this, we\u2019ll read in data in GeoJSON format to get the shapes of each district, then match each district shape with the SAT score using the school_dist column, then finally create the plot: Now that we\u2019ve set the context by plotting out where the schools are, and SAT score by district, people viewing our analysis have a better idea of the context behind the dataset.  Now that we\u2019ve set the stage, we can move into exploring the angles we identified earlier, when we were finding correlations.  The first angle to explore is the relationship between the number of students enrolled in a school and SAT score. We can explore this with a scatter plot that compares total enrollment across all schools to SAT scores across all schools. As you can see, there\u2019s a cluster at the bottom left with low total enrollment and low SAT scores.  Other than this cluster, there appears to only be a slight positive correlation between SAT scores and total enrollment.  Graphing out correlations can reveal unexpected patterns. We can explore this further by getting the names of the schools with low enrollment and low SAT scores: Some searching on Google shows that most of these schools are for students who are learning English, and are low enrollment as a result.  This exploration showed us that it\u2019s not total enrollment that\u2019s correlated to SAT score \u2013 it\u2019s whether or not students in the school are learning English as a second language or not. Now that we know the percentage of English language learners in a school is correlated with lower SAT scores, we can explore the relationship.  The ell_percent column is the percentage of students in each school who are learning English.  We can make a scatterplot of this relationship: It looks like there are a group of schools with a high ell_percentage that also have low average SAT scores.  We can investigate this at the district level, by figuring out the percentage of English language learners in each district, and seeing it if matches our map of SAT scores by district: As we can see by looking at the two district level maps, districts with a low proportion of ELL learners tend to have high SAT scores, and vice versa. It would be fair to assume that the results of student, parent, and teacher surveys would have a large correlation with SAT scores.  It makes sense that schools with high academic expectations, for instance, would tend to have higher SAT scores.  To test this theory, lets plot out SAT scores and the various survey metrics: Surprisingly, the two factors that correlate the most are N_p and N_s, which are the counts of parents and students who responded to the surveys.  Both strongly correlate with total enrollment, so are likely biased by the ell_learners.  The other metric that correlates most is saf_t_11.  That is how safe students, parents, and teachers perceived the school to be.  It makes sense that the safer the school, the more comfortable students feel learning in the environment.  However, none of the other factors, like engagement, communication, and academic expectations, correlated with SAT scores.  This may indicate that NYC is asking the wrong questions in surveys, or thinking about the wrong factors (if their goal is to improve SAT scores, it may not be). One of the other angles to investigate involves race and SAT scores.  There was a large correlation differential, and plotting it out will help us understand what\u2019s happening: It looks like the higher percentages of white and asian students correlate with higher SAT scores, but higher percentages of black and hispanic students correlate with lower SAT scores.  For hispanic students, this may be due to the fact that there are more recent immigrants who are ELL learners.  We can map the hispanic percentage by district to eyeball the correlation: It looks like there is some correlation with ELL percentage, but it will be necessary to do some more digging into this and other racial differences in SAT scores. The final angle to explore is the relationship between gender and SAT score.  We noted that a higher percentage of females in a school tends to correlate with higher SAT scores.  We can visualize this with a bar graph: To dig more into the correlation, we can make a scatterplot of female_per and sat_score: It looks like there\u2019s a cluster of schools with a high percentage of females, and very high SAT scores (in the top right).  We can get the names of the schools in this cluster: Searching Google reveals that these are elite schools that focus on the performing arts.  These schools tend to have higher percentages of females, and higher SAT scores.  This likely accounts for the correlation between higher female percentages and SAT scores, and the inverse correlation between higher male percentages and lower SAT scores. So far, we\u2019ve looked at demographic angles.  One angle that we have the data to look at is the relationship between more students taking Advanced Placement exams and higher SAT scores.  It makes sense that they would be correlated, since students who are high academic achievers tend to do better on the SAT. It looks like there is indeed a strong correlation between the two.  An interesting cluster of schools is the one at the top right, which has high SAT scores and a high proportion of students that take the AP exams: Some Google searching reveals that these are mostly highly selective schools where you need to take a test to get in.  It makes sense that these schools would have high proportions of AP test takers. With data science, the story is never truly finished.  By releasing analysis to others, you enable them to extend and shape your analysis in whatever direction interests them.  For example, in this post, there are quite a few angles that we explored inmcompletely, and could have dived into more. One of the best ways to get started with telling stories using data is to try to extend or replicate the analysis someone else has done.  If you decide to take this route, you\u2019re welcome to extend the analysis in this post and see what you can find. If you do this, make sure to comment below so I can take a look. If you\u2019ve made it this far, you hopefully have a good understanding of how to tell a story with data, and how to build your first data science portfolio piece.  Once you\u2019re done with your data science project, it\u2019s a good idea to post it on Github so others can collaborate with you on it. If you liked this, you might like to read the other posts in our \u2018Build a Data Science Porfolio\u2019 series: ",
    "link": "https://www.dataquest.io/blog/data-science-portfolio-project/",
    "title": "Building a data science portfolio:  Storytelling with data"
}{
    "code": 0,
    "content": "  To highlight how Dataquest has changed people\u2019s lives,  we\u2019ve started a new blog series called User Stories where we interview our users to learn more about their personal journey and how we\u2019ve helped them get where they needed to. \nIn this post, we interview Yassine Alouini, Data Scientist at Qucit.  Yassine got into data science by freelancing, and has built up some impressive skills along the way.  He's done everything from analyzing data, creating predictive models, and making data pipelines to creating interactive visualizations and web applications.  You can see some of the projects he's currently working on by visiting his Github profile.\n To be honest, I have used multiple venues to get started with freelancing.  I subscribed to a few platforms where you create a profile and then you get contacted by interested clients. I have also asked friends if they knew some people that needed help with statistical analysis. This second technique has been the most successful. Overall, the hardest thing was getting started. Once I have found my first mission, the next ones where much easier to get. I gained more experience and had a portfolio of accomplishments to show. While freelancing I was also looking for jobs. Freelancing helped me a lot in three different ways: At the same time, I was competing in some Kaggle challenges and read a lot about machine learning, data visualization and data science in general. I mainly use these Python libraries : Pandas, Scikit-learn, Numpy and Statsmodels for machine learning and statistics, Matplotlib, Seaborn, Ggplot and Bokeh for visualization and SQLalchemy for interacting with databases. In general, the Python ecosystem for data science is excellent.  I also use D3.js for dynamic data visualization. At some point, I have used AngularJS for web and mobile (with ionic) development. When working, I tend to switch between Jupyter Notebooks for exploration phases and IPython alongside Atom (a text editor) for production code. I also use Jupyter Notebooks (slides mode) for presentations.  One neat thing about Jupyter Notebooks is that you can work with different languages on the same interface. You can also interact with your terminal without leaving the notebook. That is something awesome. I am a huge fan of online courses (I have done a lot of MOOCs while I was freelancing) and think this is the future for education.  One night I was looking for a new data science course but I wanted something different from the usual MOOC experience, something different from the \u201cyou watch videos and then take challenges and tests\u201d one. I wanted something more interactive. I then came across Dataquest. At the beginning (around April 2015), I used it lightly and took some of the Python visualization challenges but I was already hooked. Almost two months ago, I decided it was time to get a paid subscription. It has been a very rewarding experience so far :) Dataquest helped me to get a more in depth knowledge of data science subjects. For instance, I have been using Matplotlib for quite some time but never really understood the internals until recently. It has also helped me organize my thoughts and gain more confidence when working with data.  Finally, I usually think about the learning experience it provides as a game and it has been a very enjoyable one so far. Data science is hard and it becomes harder if one only relies on theory. One must practice to become better at the trade.  In fact, learning through projects is very rewarding.  For each new project, you encounter new challenges (data in a bad format, correlated features, data that is hard to visualize, overfit algorithms, etc) and you immediately gain actionable insights. If you learn data science techniques without confrontation through a project, then your experience is incomplete. As I have said above, data science is hard. Learning it is also hard for many reasons, mainly because the field is still in its infancy and the tools available are still maturing.  Some people assume that you can learn some Python courses and few MOOCs and become a great data scientist. This is of course far away from the truth. To be good at the trade of data science, one needs to constantly learn. That being said, people must not think that it is impossible to learn it on your own. The learning path is becoming clearer. More and more courses and resources are available. Finally, a lot of great leaders in the field are showing the way for the new data science generation. I have been reading a lot about Spark these recent months and want to learn to use it. I have noticed that there is a Dataquest section on it. I will try it out as soon as possible. In addition to Apache Spark, I am also getting more interested in deep learning (specifically deep reinforcement learning). I have started playing with a recurrent neural network. So far, I have managed to set up a GPU cloud instance with everything needed to train a network. I am planning to implement one using Theano (or maybe the newer TensorFlow) and write a blog post about it.   Finally, I have used boosted gradient trees (generally through the xgboost implementation) for few Kaggle challenges without understanding all the details. Thus, I am planning to learn more about these meta-models. Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/user-story-yassine-alouini/",
    "title": "How Data Scientist Yassine Alouini keeps his skills sharp"
}{
    "code": 0,
    "content": "  In a fast-paced and rapidly growing industry like data science, keeping up is essential.  Knowing what is trending is essential in helping you know what new tools to learn, to help you get a job, and much more. At the same time, there is so much content out there that it can be hard to know what to read and easy to be overwhelmed. The solution is to turn to email newsletters, which can help you keep a handle on the latest news, tools and tutorials. These great newsletters give you everything you need to know to keep up with the world of data science.  Their creators put in the hard work so that you don\u2019t have to. Here are my picks for the seven best data science newsletters.  Data Elixir is curated weekly by Lon Reisberg, self-confessed technology geek who has an extremely prestigious work history including working for NASA. Hitting your inbox every Friday, Data Elixir has been sending the best data science news and resources to data lovers since 2014. Data Elixir is a good all-rounder, with a mix of news, opinion and tutorials.  Lon also maintains a data science jobs board  and includes the latest job listings in each week\u2019s email.  RJMetrics is a data infrastructure and analytics platform that launched in 2008, and the RJMetrics team consistently produce great articles as well as some hilarious data journalism (see Chatroulette Is 89 Percent Male, 47 Percent American, And 13 Percent Perverts). Every Sunday, DS Roundup arrives in your inbox at the perfect time for some weekend downtime reading.  Rather than providing an extensive list of articles, DS Roundup usually includes just four or five great reads. DS Roundup is compiled by Tristan Hardy, and the one thing I really love is the synopsis he puts together of each article.  It\u2019s very plain spoken and easy to read, and it means that even if I don\u2019t have time to read every article, I can still get the key takeaways.  Since 2013, Hannah Brooks (business strategy expert) and Sebastian Gutierrez (data viz & D3.js ninja) have been compiling Data Science Weekly, hand selecting the very best articles and sending them out each Thursday morning. DSW also conduct some fantastic Data Science Interviews and maintain a really thorough list of Data Science Resources including books, meetups, datasets, blogs and data scientists on twitter. Each issue of Data Science weekly starts with their Editors Picks fo the best articles from the previous week, followed by longer lists for data science articles and tutorials, providing the perfect balance to help you keep your finger on the data science pulse.  There is something beautifully comforting about the canary yellow color scheme of KDnuggets.  Possibly the most comprehensive of all data science websites, KDnuggets has been lovingly maintained by Gregory Piatetsky-Shapiro for over 20 years. With over 649 issues dating back to the first on August 20, 1993, this is easily the oldest newsletter in our list.  Originally published twice a month, but now weekly on Wednesdays, KDnuggets News is a compilation of the more than 30 articles posted to KDnuggets each week. KDnuggets news is purely a list of links without any summaries, but even still it\u2019s one of the most comprehensive roundups of what has happened in the week before which makes it well worth subscribing to.  I remember my first O\u2019Reilly book, Programming Perl, which I purchased back in 1996. Many data scientists and software engineers learned to program from O\u2019Reilly publications, with their distinctive animal covers and their quality considered second to none. Since as far back as the 1980\u2019s, O\u2019Reilly have been publishing books on data, however in the last 6 years with the introduction of their Strata + Hadoop World conferences, O\u2019Reilly has become a powerhouse in the data world. Their weekly data newsletter features a great mix of news, editorial, how-tos and case studies, both from O\u2019Reilly themselves and others.  I particularly like how they group clusters of articles on a particular news story or feature together to provide a balanced perspective.  Yhat is known for their ScienceOps platform, and more recently for Rodeo, their Python IDE designed for data science.  Beyond that though, Yhat consistently produce great articles and tutorials on data science and related topics. Rather than a curated newsletter, Yhat\u2019s blog list gets an email every time they launch a new post, which is about three times a month.  Yhat\u2019s articles are always interesting, and cover a wide range of topic.  If you\u2019re interested in predicting customer churn, the code behind a FiveThirtyEight blog post, or building an autonomous drone, you\u2019ll want to subscribe.  At Dataquest, we teach data science from the ground up, entirely in your browser.  Our interactive learning enviroment helps you learn by using real-life data sets, and we teach you the theory behind the algorithms to give you a solid foundation. Our blog posts contain short tutorials designed to help you learn data science, improve your skills and find a data science job. Here are a few of our favorite and most read posts: To subscribe, simply fill in your email in the box at the bottom of the page. Is your favorite data science newsletter missing from the list?  Then leave a comment below and let me know!",
    "link": "https://www.dataquest.io/blog/data-science-newsletters/",
    "title": "7 awesome data science newsletters to keep you informed"
}{
    "code": 1,
    "content": "  This is the third in a series of posts on how to build a Data Science Portfolio.  If you like this and want to know when the next post in the series is released, you can subscribe at the bottom of the page. Data science companies are increasingly looking at portfolios when making hiring decisions.  One of the reasons for this is that a portfolio is the best way to judge someone\u2019s real-world skills.  The good news for you is that a portfolio is entirely within your control.  If you put some work in, you can make a great portfolio that companies are impressed by. The first step in making a high-quality portfolio is to know what skills to demonstrate.  The primary skills that companies want in data scientists, and thus the primary skills they want a portfolio to demonstrate, are: Any good portfolio will be composed of multiple projects, each of which may demonstrate 1-2 of the above points.  This is the third post in a series that will cover how to make a well-rounded data science portfolio.  In this post, we\u2019ll cover how to make the second project in your portfolio, and how to build an end to end machine learning project.  At the end, you\u2019ll have a project that shows your ability to reason about data, and your technical competence.  Here\u2019s the completed project if you want to take a look. As a data scientist, there are times when you\u2019ll be asked to take a dataset and figure out how to tell a story with it.  In times like this, it\u2019s important to communicate very well, and walk through your process.  Tools like Jupyter notebook, which we used in a previous post, are very good at helping you do this.  The expectation here is that the deliverable is a presentation or document summarizing your findings. However, there are other times when you\u2019ll be asked to create a project that has operational value.  A project with operational value directly impacts the day-to-day operations of a company, and will be used more than once, and often by multiple people.  A task like this might be \u201ccreate an algorithm to forecast our churn rate\u201d, or \u201ccreate a model that can automatically tag our articles\u201d.  In cases like this, storytelling is less important than technical competence.  You need to be able to take a dataset, understand it, then create a set of scripts that can process that data.  It\u2019s often important that these scripts run quickly, and use minimal system resources like memory.  It\u2019s very common that these scripts will be run several times, so the deliverable becomes the scripts themselves, not a presentation.  The deliverable is often integrated into operational flows, and may even be user-facing. The main components of building an end to end project are: In order to effectively create a project of this kind, we\u2019ll need to work with multiple files.  Using a text editor like Atom, or an IDE like PyCharm is highly recommended.  These tools will allow you to jump between files, and edit files of different types, like markdown files, Python files, and csv files.  Structuring your project so it\u2019s easy to version control and upload to collaborative coding tools like Github is also useful. This project on Github. We\u2019ll use our editing tools along with libraries like Pandas and scikit-learn in this post.  We\u2019ll make extensive use of Pandas DataFrames, which make it easy to read in and work with tabular data in Python. A good dataset for an end to end portfolio project can be hard to find.  The dataset needs to be sufficiently large that memory and performance constraints come into play.  It also needs to potentially be operationally useful.  For instance, this dataset, which contains data on the admission criteria, graduation rates, and graduate future earnings for US colleges would be a great dataset to use to tell a story.  However, as you think about the dataset, it becomes clear that there isn\u2019t enough nuance to build a good end to end project with it.  For example, you could tell someone their potential future earnings if they went to a specific college, but that would be a quick lookup without enough nuance to demonstrate technical competence.  You could also figure out if colleges with higher admissions standards tend to have graduates who earn more, but that would be more storytelling than operational. These memory and performance constraints tend to come into play when you have more than a gigabyte of data, and when you have some nuance to what you want to predict, which involves running algorithms over the dataset. A good operational dataset enables you to build a set of scripts that transform the data, and answer dynamic questions.  A good example would be a dataset of stock prices.  You would be able to predict the prices for the next day, and keep feeding new data to the algorithm as the markets closed.  This would enable you to make trades, and potentially even profit.  This wouldn\u2019t be telling a story \u2013 it would be adding direct value. Some good places to find datasets like this are: As you look through these datasets, think about what questions someone might want answered with the dataset, and think if those questions are one-time (\u201chow did housing prices correlate with the S&P 500?\u201d), or ongoing (\u201ccan you predict the stock market?\u201d).  The key here is to find questions that are ongoing, and require the same code to be run multiple times with different inputs (different data). For the purposes of this post, we\u2019ll look at Fannie Mae Loan Data.  Fannie Mae is a government sponsored enterprise in the US that buys mortgage loans from other lenders.  It then bundles these loans up into mortgage-backed securities and resells them.  This enables lenders to make more mortgage loans, and creates more liquidity in the market.  This theoretically leads to more homeownership, and better loan terms.  From a borrowers perspective, things stay largely the same, though. Fannie Mae releases two types of data \u2013 data on loans it acquires, and data on how those loans perform over time.  In the ideal case, someone borrows money from a lender, then repays the loan until the balance is zero.  However, some borrowers miss multiple payments, which can cause foreclosure.  Foreclosure is when the house is seized by the bank because mortgage payments cannot be made.  Fannie Mae tracks which loans have missed payments on them, and which loans needed to be foreclosed on.  This data is published quarterly, and lags the current date by 1 year.  As of this writing, the most recent dataset that\u2019s available is from the first quarter of 2015. Acquisition data, which is published when the loan is acquired by Fannie Mae, contains information on the borrower, including credit score, and information on their loan and home.  Performance data, which is published every quarter after the loan is acquired, contains information on the payments being made by the borrower, and the foreclosure status, if any.  A loan that is acquired may have dozens of rows in the performance data.  A good way to think of this is that the acquisition data tells you that Fannie Mae now controls the loan, and the performance data contains a series of status updates on the loan.  One of the status updates may tell us that the loan was foreclosed on during a certain quarter. A foreclosed home being sold. There are a few directions we could go in with the Fannie Mae dataset.  We could: The important thing is to stick to a single angle.  Trying to focus on too many things at once will make it hard to make an effective project.  It\u2019s also important to pick an angle that has sufficient nuance.  Here are examples of angles without much nuance: All of the above angles are interesting, and would be great if we were focused on storytelling, but aren\u2019t great fits for an operational project. With the Fannie Mae dataset, we\u2019ll try to predict whether a loan will be foreclosed on in the future by only using information that was available when the loan was acquired.  In effect, we\u2019ll create a \u201cscore\u201d for any mortgage that will tell us if Fannie Mae should buy it or not.  This will give us a nice foundation to build on, and will be a great portfolio piece. Let\u2019s take a quick look at the raw data files.  Here are the first few rows of the acquisition data from quarter 1 of 2012: Here are the first few rows of the performance data from quarter 1 of 2012: Before proceeding too far into coding, it\u2019s useful to take some time and really understand the data.  This is more critical in operational projects \u2013 because we aren\u2019t interactively exploring the data, it can be harder to spot certain nuances unless we find them upfront.  In this case, the first step is to read the materials on the Fannie Mae site: After reading through these files, we know some key facts that will help us: These small bits of information will save us a ton of time as we figure out how to structure our project and work with the data. Before we start downloading and exploring the data, it\u2019s important to think about how we\u2019ll structure the project.  When building an end-to-end project, our primary goals are: In order to achieve these goals, we\u2019ll need to structure our project well.  A well structured project follows a few principles: Our file structure will look something like this shortly: To start with, we\u2019ll need to create a loan-prediction folder.  Inside that folder, we\u2019ll need to make a data folder and a processed folder.  The first will store our raw data, and the second will store any intermediate calculated values. Next, we\u2019ll make a .gitignore file.  A .gitignore file will make sure certain files are ignored by git and not pushed to Github.  One good example of such a file is the .DS_Store file created by OSX in every folder.  A good starting point for a .gitignore file is here.  We\u2019ll also want to ignore the data files because they are very large, and the Fannie Mae terms prevent us from redistributing them, so we should add two lines to the end of our file: Here\u2019s an example .gitignore file for this project. Next, we\u2019ll need to create README.md, which will help people understand the project.  .md indicates that the file is in markdown format.  Markdown enables you write plain text, but also add some fancy formatting if you want.  Here\u2019s a guide on markdown.  If you upload a file called README.md to Github, Github will automatically process the markdown, and show it to anyone who views the project.  Here\u2019s an example. For now, we just need to put a simple description in README.md: Now, we can create a requirements.txt file.  This will make it easy for other people to install our project.  We don\u2019t know exactly what libraries we\u2019ll be using yet, but here\u2019s a good starting point: The above libraries are the most commonly used for data analysis tasks in Python, and its fair to assume that we\u2019ll be using most of them.  Here\u2019s an example requirements file for this project. After creating requirements.txt, you should install the packages.  For this post, we\u2019ll be using Python 3.  If you don\u2019t have Python installed, you should look into using Anaconda, a Python installer that also installs all the packages listed above. Finally, we can just make a blank settings.py file, since we don\u2019t have any settings for our project yet. Once we have the skeleton of our project, we can get the raw data. Fannie Mae has some restrictions around acquiring the data, so you\u2019ll need to sign up for an account.  You can find the download page here.  After creating an account, you\u2019ll be able to download as few or as many loan data files as you want.  The files are in zip format, and are reasonably large after decompression. For the purposes of this blog post, we\u2019ll download everything from Q1 2012 to Q1 2015, inclusive.  We\u2019ll then need to unzip all of the files.  After unzipping the files, remove the original .zip files.  At the end, the loan-prediction folder should look something like this: After downloading the data, you can use the head and tail shell commands to look at the lines in the files.  Do you see any columns that aren\u2019t needed?  It might be useful to consult the pdf of column names while doing this. There are two issues that make our data hard to work with right now: Before we can get started on working with the data, we\u2019ll need to get to the point where we have one file for the acquisition data, and one file for the performance data.  Each of the files will need to contain only the columns we care about, and have the proper headers.  One wrinkle here is that the performance data is quite large, so we should try to trim some of the columns if we can. The first step is to add some variables to settings.py, which will contain the paths to our raw data and our processed data.  We\u2019ll also add a few other settings that will be useful later on: Putting the paths in settings.py will put them in a centralized place and make them easy to change down the line.  When referring to the same variables in multiple files, it\u2019s easier to put them in a central place than edit them in every file when you want to change them.  Here\u2019s an example settings.py file for this project. The second step is to create a file called assemble.py that will assemble all the pieces into 2 files.  When we run python assemble.py, we\u2019ll get 2 data files in the processed directory.   We\u2019ll then start writing code in assemble.py.  We\u2019ll first need to define the headers for each file, so we\u2019ll need to look at pdf of column names and create lists of the columns in each Acquisition and Performance file: The next step is to define the columns we want to keep.  Since all we\u2019re measuring on an ongoing basis about the loan is whether or not it was ever foreclosed on, we can discard many of the columns in the performance data.  We\u2019ll need to keep all the columns in the acquisition data, though, because we want to maximize the information we have about when the loan was acquired (after all, we\u2019re predicting if the loan will ever be foreclosed or not at the point it\u2019s acquired).   Discarding columns will enable us to save disk space and memory, while also speeding up our code.  Next, we\u2019ll write a function to concatenate the data sets.  The below code will: We can call the above function twice with the arguments Acquisition and Performance to concatenate all the acquisition and performance files together.  The below code will: We now have a nice, compartmentalized assemble.py that\u2019s easy to execute, and easy to build off of.  By decomposing the problem into pieces like this, we make it easy to build our project.  Instead of one messy script that does everything, we define the data that will pass between the scripts, and make them completely separate from each other.  When you\u2019re working on larger projects, it\u2019s a good idea to do this, because it makes it much easier to change individual pieces without having unexpected consequences on unrelated pieces of the project. Once we finish the assemble.py script, we can run python assemble.py.  You can find the complete assemble.py file here. This will result in two files in the processed directory: The next step we\u2019ll take is to calculate some values from processed/Performance.txt.  All we want to do is to predict whether or not a property is foreclosed on.  To figure this out, we just need to check if the performance data associated with a loan ever has a foreclosure_date.  If foreclosure_date is None, then the property was never foreclosed on.  In order to avoid including loans with little performance history in our sample, we\u2019ll also want to count up how many rows exist in the performance file for each loan.  This will let us filter loans without much performance history from our training data. One way to think of the loan data and the performance data is like this: As you can see above, each row in the Acquisition data can be related to multiple rows in the Performance data.  In the Performance data, foreclosure_date will appear in the quarter when the foreclosure happened, so it should be blank prior to that.  Some loans are never foreclosed on, so all the rows related to them in the Performance data have foreclosure_date blank. We need to compute foreclosure_status, which is a Boolean that indicates whether a particular loan id was ever foreclosed on, and performance_count, which is the number of rows in the performance data for each loan id. There are a few different ways to compute the counts we want: Loading in all the data will take quite a bit of memory, so let\u2019s go with the third option above.  All we need to do is to iterate through all the rows in the Performance data, while keeping a dictionary of counts per loan id.  In the dictionary, we\u2019ll keep track of how many times the id appears in the performance data, as well as if foreclosure_date is ever not None.  This will give us foreclosure_status and performance_count. We\u2019ll create a new file called annotate.py, and add in code that will enable us to compute these values.  In the below code, we\u2019ll: Once we create our counts dictionary, we can make a function that will extract values from the dictionary if a loan_id and a key are passed in: The above function will return the appropriate value from the counts dictionary, and will enable us to assign a foreclosure_status value and a performance_count value to each row in the Acquisition data.  The get method on dictionaries returns a default value if a key isn\u2019t found, so this enables us to return sensible default values if a key isn\u2019t found in the counts dictionary. We\u2019ve already added a few functions to annotate.py, but now we can get into the meat of the file.  We\u2019ll need to convert the acquisition data into a training dataset that can be used in a machine learning algorithm.  This involves a few things: Several of our columns are strings, which aren\u2019t useful to a machine learning algorithm.  However, they are actually categorical variables, where there are a few different category codes, like R, S, and so on.  We can convert these columns to numeric by assigning a number to each category label: Converting the columns this way will allow us to use them in our machine learning algorithm. Some of the columns also contain dates (first_payment_date and origination_date).  We can split these dates into 2 columns each: In the below code, we\u2019ll transform the Acquisition data.  We\u2019ll define a function that: We\u2019re almost ready to pull everything together, we just need to add a bit more code to annotate.py.  In the below code, we: Once you\u2019re done updating the file, make sure to run it with python annotate.py, to generate the train.csv file.  You can find the complete annotate.py file here. The folder should now look like this: We\u2019re done with generating our training dataset, and now we\u2019ll just need to do the final step, generating predictions.  We\u2019ll need to figure out an error metric, as well as how we want to evaluate our data.  In this case, there are many more loans that aren\u2019t foreclosed on than are, so typical accuracy measures don\u2019t make much sense. If we read in the training data, and check the counts in the foreclosure_status column, here\u2019s what we get: Since so few of the loans were foreclosed on, just checking the percentage of labels that were correctly predicted will mean that we can make a machine learning model that predicts False for every row, and still gets a very high accuracy.  Instead, we\u2019ll want to use a metric that takes the class imbalance into account, and ensures that we predict foreclosures accurately.  We don\u2019t want too many false positives, where we make predict that a loan will be foreclosed on even though it won\u2019t, or too many false negatives, where we predict that a loan won\u2019t be foreclosed on, but it is.  Of these two, false negatives are more costly for Fannie Mae, because they\u2019re buying loans where they may not be able to recoup their investment. We\u2019ll define false negative rate as the number of loans where the model predicts no foreclosure but the the loan was actually foreclosed on, divided by the number of total loans that were actually foreclosed on.  This is the percentage of actual foreclosures that the model \u201cMissed\u201d.  Here\u2019s a diagram: In the diagram above, 1 loan was predicted as not being foreclosed on, but it actually was.  If we divide this by the number of loans that were actually foreclosed on, 2, we get the false negative rate, 50%.  We\u2019ll use this as our error metric, so we can evaluate our model\u2019s performance. We\u2019ll use cross validation to make predictions.  With cross validation, we\u2019ll divide our data into 3 groups.  Then we\u2019ll do the following: Splitting it up into groups this way means that we never train a model using the same data we\u2019re making predictions for.  This avoids overfitting.  If we overfit, we\u2019ll get a falsely low false negative rate, which makes it hard to improve our algorithm or use it in the real world. Scikit-learn has a function called cross_val_predict which will make it easy to perform cross validation. We\u2019ll also need to pick an algorithm to use to make predictions.  We need a classifier that can do binary classification.  The target variable, foreclosure_status only has two values, True and False. We\u2019ll use logistic regression, because it works well for binary classification, runs extremely quickly, and uses little memory.  This is due to how the algorithm works \u2013 instead of constructing dozens of trees, like a random forest, or doing expensive transformations, like a support vector machine, logistic regression has far fewer steps involving fewer matrix operations. We can use the logistic regression classifier algorithm that\u2019s implemented in scikit-learn.  The only thing we need to pay attention to is the weights of each class.  If we weight the classes equally, the algorithm will predict False for every row, because it is trying to minimize errors.  However, we care much more about foreclosures than we do about loans that aren\u2019t foreclosed on.  Thus, we\u2019ll pass balanced to the class_weight keyword argument of the LogisticRegression class, to get the algorithm to weight the foreclosures more to account for the difference in the counts of each class.  This will ensure that the algorithm doesn\u2019t predict False for every row, and instead is penalized equally for making errors in predicting either class. Now that we have the preliminaries out of the way, we\u2019re ready to make predictions.  We\u2019ll create a new file called predict.py that will use the train.csv file we created in the last step.  The below code will: Now, we just need to write a few functions to compute error.  The below code will: Now, we just have to put the functions together in predict.py.  The below code will: Once you\u2019ve added the code, you can run python predict.py to generate predictions.  Running everything shows that our false negative rate is .26, which means that of the foreclosed loans, we missed predicting 26% of them.  This is a good start, but can use a lot of improvement! You can find the complete predict.py file here. Your file tree should now look like this: Now that we\u2019ve finished our end to end project, we just have to write up a README.md file so that other people know what we did, and how to replicate it.  A typical README.md for a project should include these sections: Here\u2019s a sample README.md for this project. Congratulations, you\u2019re done making an end to end machine learning project!  You can find a complete example project here.  It\u2019s a good idea to upload your project to Github once you\u2019ve finished it, so others can see it as part of your portfolio. There are still quite a few angles left to explore with this data.  Broadly, we can split them up into 3 categories \u2013 extending this project and making it more accurate, finding other columns to predict, and exploring the data.  Here are some ideas: If you build anything interesting, please let us know in the comments! If you liked this, you might like to read the other posts in our \u2018Build a Data Science Porfolio\u2019 series: ",
    "link": "https://www.dataquest.io/blog/data-science-portfolio-machine-learning/",
    "title": "Building a data science portfolio: Machine learning project"
}{
    "code": 1,
    "content": "  Python is becoming an increasingly popular language for data science, and with good reason.  It\u2019s easy to learn, has powerful data science libraries, and integrates well with databases and tools like Hadoop and Spark.  With Python, we can perform the full lifecycle of data science projects, including reading data in, analyzing data, visualizing data, and making predictions with machine learning. In this post, we\u2019ll walk through getting started with Python for data science.  If you want to dive more deeply into the topics we cover, visit Dataquest, where we teach every component of the Python data science lifecycle in depth.  We\u2019ll be working with a dataset of political contributions to candidates in the 2016 US presidential elections, which can be found here.  The file is in csv format, and each row in the dataset represents a single donation to the campaign of a single candidate.  The dataset has several interesting columns, including: The first step we\u2019ll need to take to analyze this data is to install Python.  Installing Python is an easy process with Anaconda, a tool that installs Python along with several popular data analysis libraries.  You can download Anaconda here.  It\u2019s recommended to install Python 3.5, which is the newest version of Python.  You can read more about Python 2 vs Python 3 here. Anaconda automatically installs several libraries that we\u2019ll be using in this post, including Jupyter, Pandas, scikit-learn, and matplotlib. Now that we have everything installed, we can launch Jupyter notebook (formerly known as IPython notebook).  Jupyter notebook is a powerful data analysis tool that enables you to quickly explore data, visualize your findings, and share your results.  It\u2019s used by data scientists at organizations like Google, IBM, and Microsoft to analyze data and collaborate. Start Jupyter by running ipython notebook in the terminal.  If you have trouble, check here. You should see a file browser interface that allows you to create new notebooks.  Create a Python 3 notebook, which we\u2019ll be using in our analysis.  If you need more help with installation, check out our guide here. Each Jupyter notebook is composed of multiple cells in which you can run code or write explanations.  Your notebook will only have one cell initially, but you can add more: If you want to learn more about Jupyter, check our our in-depth tutorial here. Pandas is a data analysis library for Python.  It enables us to read in data from a variety of formats, including csv, and then analyze that data efficiently.  We can read in the data using this code: In the cells above, we import the Pandas library using import pandas as pd, then use the read_csv() method to read political_donations.csv into the donations variable.  The donations variable is a Pandas DataFrame, which is an enhanced version of a matrix that has data analysis methods built in and allows different datatypes in each column. We access the shape property of donations variable to print out how many rows and columns it has.  When a statement or variable is placed on the last line of a notebook cell, its value or output is automatically rendered!  We then use the head() method on DataFrames to print out the first two rows of donations so we can inspect them. If you want to dive into more depth with Pandas, see our course here. We can compute per-candidate summary statistics using the Pandas groupby() method.  We can first use the groupby method to split donations into subsets based on cand_nm.  Then, we can compute statistics separately for each candidate.  The first summary statistic we compute will be total donations.  To get this, we just take the sum of the contb_receipt_amount column for each candidate. In the above code, we first split donations into groups based on cand_nm using the code donations.groupby(\"cand_nm\").  This returns a GroupBy object that has some special methods for aggregating data.  One of these methods is sum(), which we use to compute the sum of each column in each group.   Pandas automatically recognizes the data type of columns when data is read in, and only performs the sum operation on the numeric columns.  We end up with a DataFrame showing the sum of the contb_receipt_amt and file_num columns for each candidate.  In a final step, we use the sort() method on DataFrames to sort in ascending order to contb_receipt_amt.  This shows us how much each candidate has collected in donations. We can use matplotlib, the main Python data visualization library, to make plots.  Jupyter notebook even supports rendering matplotlib plots inline.  We\u2019ll need to activate the inline mode of matplotlib to do this.  We can use Jupyter magics to make matplotlib figures show up in the notebook.   Magics are commands that start with a % or %%, and affect the behavior of Jupyter notebook.  They are meant to be used as a way to change Jupyter configuration without the commands getting mixed up with Python code.  To enable matplotlib figures to show up inline, we need to run %matplotlib inline in a cell.  Read more about plotting with Jupyter here. Here we import the matplotlib library and activate inline mode: Pandas DataFrames have built-in visualization support, and you can call the plot() method to generate a matplotlib plot from a DataFrame.  This is often much quicker than using matplotlib directly.  First, we assign our DataFrame from earlier to a variable, total_donations.  Then, we use indexing to select a single column of the DataFrame, contb_receipt_amt.  This generates a Pandas Series.   Pandas Series have most of the same methods as DataFrames, but they store 1-dimensional data, like a single row or a single column.  We can then call the plot() method on the Series to generate a bar chart of each candidate\u2019s total donations. If you want to dive more into matplotlib, check out our course here. It\u2019s dead simple to find the mean donation size instead of the total donations.  We just swap the sum() method for the mean() method. Let\u2019s make a simple algorithm that can figure out how much someone will donate based on their state (contbr_st), occupation (contbr_occupation), and preferred candidate (cand_nm).  The first step is to make a separate Dataframe with just these columns and the contb_receipt_amt column, which we want to predict. Now we\u2019ll check the data types of each column in pdonations.  When Pandas reads in a csv file, it automatically assigns a data type to each column.  We can only use columns to make predictions if they are a numeric data type. Unfortunately, all of the columns we want to use to predict are object datatypes (strings).  This is because they are categorical data.  Each column has several options, but they are shown as text instead of using numeric codes.  We can convert each column into numeric data by converting to the categorical datatype, then to numeric.  Here\u2019s more on the categorical data type.  Essentially, the categorical data type assigns a numeric code behind the scenes to each unique value in a column.  We can replace the column with these codes to convert to numeric entirely. As you can see, we\u2019ve converted the contbr_st column to numeric values.  We\u2019ll need to repeat the same process with the contbr_occupation and cand_nm columns. We\u2019ll now be able to start leveraging scikit-learn, the primary Python machine learning library, to help us with the rest of the prediction workflow.  First, we\u2019ll split the data into two sets \u2013 one which we train our algorithm on called the training set, and one which we use to evaluate the performance of the model called the test set.  We do this to avoid overfitting, and getting a misleading error value. We can use the train_test_split() function to split pdonations into a train and a test set. The above code splits the columns we want to use to train the algorithm, and the column we want to make predictions on (contb_receipt_amt) each both train and test sets.  We take 33% of the data for the test set.  The rows are randomly assigned to the sets. We\u2019ll use the random forest algorithm to make our predictions.  It\u2019s an accurate and versatile algorithm, and it\u2019s implemented by scikit-learn via the RandomForestRegressor class.  This class makes it simple to train the model, then make predictions with it. First, we\u2019ll train the model using train and y_train: One of the great things about scikit-learn is that it has a consistent API across all the algorithms it implements.  You can train a linear regression the exact same way you train a random forest.  We now have a fit model, so we can make predictions with it. It\u2019s very easy to make predictions with scikit-learn.  We just pass in our test data to the fitted model. Now that we have predictions, we can calculate error.  Our error will let us know how well our model is performing, and give us a way to evaluate it as we make tweaks.  We\u2019ll use mean squared error, a common error metric. If you want to learn more about scikit-learn, check out our tutorials here. Taking the square root of the error you got will give you an error value that\u2019s easier to think about in terms of donation size.  If you don\u2019t take the square root, you\u2019ll have the average squared error, which doesn\u2019t directly mean anything in the context of our data.  Either way, the error is large, and there are many things you can do to lower it. Here are some other interesting data explorations you could do: If you want to dive more deeply into the concepts mentioned here, check out our lessons on Python for Data Science. Image Credit: Erlenmeyer Flask by Anthony Bossard from the Noun Project.",
    "link": "https://www.dataquest.io/blog/python-data-science/",
    "title": "Python for data science: Getting started"
}{
    "code": 1,
    "content": "  Kaggle competitions are a fantastic way to learn data science and build your portfolio.  I personally used Kaggle to learn many data science concepts.  I started out with Kaggle a few months after learning programming, and later won several competitions. Doing well in a Kaggle competition requires more than just knowing machine learning algorithms.  It requires the right mindset, the willingness to learn, and a lot of data exploration.  Many of these aspects aren\u2019t typically emphasized in tutorials on getting started with Kaggle, though.  In this post, I\u2019ll cover how to get started with the Kaggle Expedia hotel recommendations competition, including establishing the right mindset, setting up testing infrastructure, exploring the data, creating features, and making predictions. At the end, we\u2019ll generate a submission file using the techniques in the this post.  As of this writing, the submission would rank in the top 15. Where this submission would rank as of this writing. The Expedia competition challenges you with predicting what hotel a user will book based on some attributes about the search the user is conducting on Expedia.  Before we dive into any coding, we\u2019ll need to put in time to understand both the problem and the data. The first step is to look at the description of the columns of the dataset.  You can find that here.  Towards the bottom of the page, you\u2019ll see a description of each column in the data.  Looking over this, it appears that we have quite a bit of data about the searches users are conducting on Expedia, along with data on what hotel cluster they eventually booked in test.csv and train.csv.  destinations.csv contains information about the regions users search in for hotels.  We won\u2019t worry about what we\u2019re predicting just yet, we\u2019ll focus on understanding the columns. Since the competition consists of event data from users booking hotels on Expedia, we\u2019ll need to spend some time understanding the Expedia site.  Looking at the booking flow will help us contextualize the fields in the data, and how they tie into using Expedia. The page you initially see when booking a hotel. The box labelled Going To maps to the srch_destination_type_id, hotel_continent, hotel_country, and hotel_market fields in the data. The box labelled Check-in maps to the srch_ci field in the data, and the box labelled Check out maps to the srch_co field in the data.   The box labelled Guests maps to the srch_adults_cnt, srch_children_cnt, and srch_rm_cnt fields in the data. The box labelled Add a Flight maps to the is_package field in the data. site_name is the name of the site you visited, whether it be the main Expedia.com site, or another.   user_location_country, user_location_region, user_location_city, is_mobile, channel is_booking, and cnt are all attributes that are determined by where the user it, what their device is, or their session on the Expedia site. Just by looking at one screen, we can immediately contextualize all the variables.  Playing around with the screen, filling in values, and going through the booking process can help further contextualize. Now that we have a handle on the data at a high level, we can do some exploration to take a deeper look. You can download the data here.  The datasets are fairly large, so you\u2019ll need a good amount of disk space.  You\u2019ll need to unzip the files to get raw .csv files instead of .csv.gz. Given the amount of memory on your system, it may or may not be feasible to read all the data in.  If it isn\u2019t, you should consider creating a machine on EC2 or DigitalOcean to process the data with.  Here\u2019s a tutorial on how to get started with that. Once we download the data, we can read it in using Pandas: Let\u2019s first look at how much data there is: We have about 37 million training set rows, and 2 million testing set rows, which will make this problem a bit challenging to work with. We can explore the first few rows of the data: There are a few things that immediately stick out: There are a few things we can take away from looking at test.csv: We\u2019ll be predicting which hotel_cluster a user will book after a given search.  According to the description, there are 100 clusters in total. The evaluation page says that we\u2019ll be scored using Mean Average Precision @ 5, which means that we\u2019ll need to make 5 cluster predictions for each row, and will be scored on whether or not the correct prediction appears in our list.  If the correct prediction comes earlier in the list, we get more points. For example, if the \u201ccorrect\u201d cluster is 3, and we predict 4, 43, 60, 3, 20, our score will be lower than if we predict 3, 4, 43, 60, 20.  We should put predictions we\u2019re more certain about earlier in our list of predictions. Now that we know what we\u2019re predicting, it\u2019s time to dive in and explore hotel_cluster.  We can use the value_counts method on Series to do this: The output above is truncated, but it shows that the number of hotels in each cluster is fairly evenly distributed.  There doesn\u2019t appear to be any relationship between cluster number and the number of items. Finally, we\u2019ll confirm our hypothesis that all the test user ids are found in the train DataFrame.  We can do this by finding the unique values for user_id in test, and seeing if they all exist in train. In the code below, we\u2019ll: Looks like our hypothesis is correct, which will make working with this data much easier! The entire train.csv dataset contains 37 million rows, which makes it hard to experiment with different techniques.  Ideally, we want a small enough dataset that lets us quickly iterate through different approaches but is still representative of the whole training data. We can do this by first randomly sampling rows from our data, then selecting new training and testing datasets from train.csv.  By selecting both sets from train.csv, we\u2019ll have the true hotel_cluster label for every row, and we\u2019ll be able to calculate our accuracy as we test techniques. The first step is to add month and year fields to train.  Because the train and test data is differentiated by date, we\u2019ll need to add date fields to allow us to segment our data into two sets the same way.  If we add year and month fields, we can split our data into training and testing sets using them. The code below will: Because the user ids in test are a subset of the user ids in train, we\u2019ll need to do our random sampling in a way that preserves the full data of each user.  We can accomplish this by selecting a certain number of users randomly, then only picking rows from train where user_id is in our random sample of user ids. The above code creates a DataFrame called sel_train that only contains data from 10000 users. We\u2019ll now need to pick new training and testing sets from sel_train.  We\u2019ll call these sets t1 and t2. In the original train and test DataFrames, test contained data from 2015, and train contained data from 2013 and 2014.  We split this data so that anything after July 2014 is in t2, and anything before is in t1.  This gives us smaller training and testing sets with similar characteristics to train and test. If is_booking is 0, it represents a click, and a 1 represents a booking.  test contains only booking events, so we\u2019ll need to sample t2 to only contain bookings as well. The most simple technique we could try on this data is to find the most common clusters across the data, then use them as predictions. We can again use the value_counts method to help us here: The above code will give us a list of the 5 most common clusters in train.  This is because the head method returns the first 5 rows by default, and the index property will return the index of the DataFrame, which is the hotel cluster after running the value_counts method. We can turn most_common_clusters into a list of predictions by making the same prediction for each row. This will create a list with as many elements as there are rows in t2.  Each element will be equal to most_common_clusters. In order to evaluate error, we\u2019ll first need to figure out how to compute Mean Average Precision.  Luckily, Ben Hamner has written an implementation that can be found here.  It can be installed as part of the ml_metrics package, and you can find installation instructions for how to install it here. We can compute our error metric with the mapk method in ml_metrics: Our target needs to be in list of lists format for mapk to work, so we convert the hotel_cluster column of t2 into a list of lists.  Then, we call the mapk method with our target, our predictions, and the number of predictions we want to evaluate (5). Our result here isn\u2019t great, but we\u2019ve just generated our first set of predictions, and evaluated our error!  The framework we\u2019ve built will allow us to quickly test out a variety of techniques and see how they score.  We\u2019re well on our way to building a good-performing solution for the leaderboard. Before we move on to creating a better algorithm, let\u2019s see if anything correlates well with hotel_cluster.  This will tell us if we should dive more into any particular columns. We can find linear correlations in the training set using the corr method: This tells us that no columns correlate linearly with hotel_cluster.  This makes sense, because there is no linear ordering to hotel_cluster.  For example, having a higher cluster number isn\u2019t tied to having a higher srch_destination_id. Unfortunately, this means that techniques like linear regression and logistic regression won\u2019t work well on our data, because they rely on linear correlations between predictors and targets. This data for this competition is quite difficult to make predictions on using machine learning for a few reasons: For these reasons, machine learning probably won\u2019t work well on our data, but we can try an algorithm and find out. The first step in applying machine learning is to generate features.  We can generate features using both what\u2019s available in the training data, and what\u2019s available in destinations.  We haven\u2019t looked at destinations yet, so let\u2019s take a quick peek. Destinations contains an id that corresponds to srch_destination_id, along with 149 columns of latent information about that destination.  Here\u2019s a sample: The competition doesn\u2019t tell us exactly what each latent feature is, but it\u2019s safe to assume that it\u2019s some combination of destination characteristics, like name, description, and more.  These latent features were converted to numbers, so they could be anonymized. We can use the destination information as features in a machine learning algorithm, but we\u2019ll need to compress the number of columns down first, to minimize runtime.  We can use PCA to do this.  PCA will reduce the number of columns in a matrix while trying to preserve the same amount of variance per row.  Ideally, PCA will compress all the information contained in all the columns into less, but in practice, some information is lost. In the code below, we: The above code compresses the 149 columns in destinations down to 3 columns, and creates a new DataFrame called dest_small.  We preserve most of the variance in destinations while doing this, so we don\u2019t lose a lot of information, but save a lot of runtime for a machine learning algorithm. Now that the preliminaries are done with, we can generate our features.  We\u2019ll do the following: The above will calculate features such as length of stay, check in day, and check out month.  These features will help us train a machine learning algorithm later on. Replacing missing values with -1 isn\u2019t the best choice, but it will work fine for now, and we can always optimize the behavior later on. Now that we have features for our training data, we can try machine learning.  We\u2019ll use 3-fold cross validation across the training set to generate a reliable error estimate.  Cross validation splits the training set up into 3 parts, then predicts hotel_cluster for each part using the other parts to train with. We\u2019ll generate predictions using the Random Forest algorithm.  Random forests build trees, which can fit to nonlinear tendencies in data.  This will enable us to make predictions, even though none of our columns are linearly related. We\u2019ll first initialize the model and compute cross validation scores: The above code doesn\u2019t give us very good accuracy, and confirms our original suspicion that machine learning isn\u2019t a great approach to this problem.  However, classifiers tend to have lower accuracy when there is a high cluster count.  We can instead try training 100 binary classifiers.  Each classifier will just determine if a row is in it\u2019s cluster, or not.  This will entail training one classifier per label in hotel_cluster. We\u2019ll again train Random Forests, but each forest will predict only a single hotel cluster.  We\u2019ll use 2 fold cross validation for speed, and only train 10 trees per label. In the code below, we: Our accuracy here is worse than before, and people on the leaderboard have much better accuracy scores.  We\u2019ll need to abandon machine learning and move to the next technique in order to compete.  Machine learning can be a powerful technique, but it isn\u2019t always the right approach to every problem.   There are a few Kaggle Scripts for the competition that involve aggregating hotel_cluster based on orig_destination_distance, or srch_destination_id.  Aggregating on orig_destination_distance will exploit a data leak in the competition, and attempt to match the same user together.  Aggregating on srch_destination_id will find the most popular hotel clusters for each destination.  We\u2019ll then be able to predict that a user who searches for a destination is going to one of the most popular hotel clusters for that destination.  Think of this as a more granular version of the most common clusters technique we used earlier. We can first generate scores for each hotel_cluster in each srch_destination_id.  We\u2019ll weight bookings higher than clicks.  This is because the test data is all booking data, and this is what we want to predict.  We want to include click information, but downweight it to reflect this.  Step by step, we\u2019ll: Here\u2019s the code to accomplish the above steps: At the end, we\u2019ll have a dictionary where each key is an srch_destination_id.  Each value in the dictionary will be another dictionary, containing hotel clusters as keys with scores as values.  Here\u2019s how it looks: We\u2019ll next want to transform this dictionary to find the top 5 hotel clusters for each srch_destination_id.  In order to do this, we\u2019ll: Here\u2019s the code: Once we know the top clusters for each srch_destination_id, we can quickly make predictions.  To make predictions, all we have to do is: Here\u2019s the code: At the end of the loop, preds will be a list of lists containing our predictions.  It will look like this: Once we have our predictions, we can compute our accuracy using the mapk function from earlier: We\u2019re doing pretty well!  We boosted our accuracy 4x over the best machine learning approach, and we did it with a far faster and simpler approach.   You may have noticed that this value is quite a bit lower than accuracies on the leaderboard.  Local testing results in a lower accuracy value than submitting, so this approach will actually do fairly well on the leaderboard.  Differences in leaderboard score and local score can come down to a few factors: The forums are very important in Kaggle, and can often help you find nuggets of information that will let you boost your score.  The Expedia competition is no exception.  This post details a data leak that allows you to match users in the training set from the testing set using a set of columns including user_location_country, and user_location_region. We\u2019ll use the information from the post to match users from the testing set back to the training set, which will boost our score.  Based on the forum thread, its okay to do this, and the competition won\u2019t be updated as a result of the leak. The first step is to find users in the training set that match users in the testing set. In order to do this, we need to: Here\u2019s the code to accomplish this: At the end of this loop, we\u2019ll have a list of lists that contain any exact matches between the training and the testing sets.  However, there aren\u2019t that many matches.  To accurately evaluate error, we\u2019ll have to combine these predictions with our earlier predictions.  Otherwise, we\u2019ll get a very low accuracy value, because most rows have empty lists for predictions. We can combine different lists of predictions to boost accuracy.  Doing so will also help us see how good our exact match strategy is.  To do this, we\u2019ll have to: Here\u2019s how we can do it: This is looking quite good in terms of error \u2013 we improved dramatically from earlier!  We could keep going and making more small improvements, but we\u2019re probably ready to submit now. Luckily, because of the way we wrote the code, all we have to do to submit is assign train to the variable t1, and test to the variable t2.  Then, we just have to re-run the code to make predictions.  Re-running the code over the train and test sets should take less than an hour. Once we have predictions, we just have to write them to a file: We\u2019ll then have a submission file in the right format to submit.  As of this writing, making this submission will get you into the top 15. We came a long way in this post!  We went from just looking at the data all the way to creating a submission and getting onto the leaderboard.  Along the way, some of the key steps we took were: These steps will serve you well in any Kaggle competition. In order to iterate quickly and explore techniques, speed is key.  This is difficult with this competition, but there are a few strategies to try: Writing fast, efficient code is a huge advantage in this competition. Once you have a stable foundation on which to run your code, there are a few avenues to explore in terms of techniques to boost accuracy: I hope you have fun with this competition!  I\u2019d love to hear any feedback you have.  If you want to learn more before diving into the competition, check out our courses on Dataquest to learn about data manipulation, statistics, machine learning, how to work with Spark, and more.",
    "link": "https://www.dataquest.io/blog/kaggle-tutorial/",
    "title": "How to get into the top 15 of a Kaggle competition using Python"
}{
    "code": 1,
    "content": "  The Museum of Modern Art is one of the most influential museums in the world and they have released a dataset on the artworks in their collection. The dataset has some data quality issues, however, and requires cleanup. In a previous post, we discussed how we used Python and Pandas to clean the dataset. In this post, we\u2019ll learn about how to use the csvkit library to acquire and explore tabular data. Great question! When working in cloud data science environments, you sometimes only have access to a server\u2019s shell. In these situations, proficiency with command line data science is a true superpower. As you become more proficient, using the command line for some data science tasks is much quicker than writing a Python script or a Hadoop job. Lastly, the command line has a rich ecosystem of tools and integration into the file system. This makes certain kinds of tasks, especially those involving multiple files, incredibly easy. Some experience working in the command line is expected for this post. If you\u2019re new to the command line, I recommend checking out our interactive command line course. csvkit is a library optimized for working with CSV files. It\u2019s written in Python but the primary interface is the command line.  You can install csvkit using pip: You\u2019ll need this library to follow along with this post. The MOMA artworks dataset is available on the museum\u2019s Github repo. Let\u2019s use curl to download Artworks.csv from Github. curl is a tool built into most shell environments that allows you to transfer data between servers. Github provides direct URLs for every file, which you can find by clicking on the Raw button. Lastly, we\u2019ll use the > operator to redirect the output from curl to a file named artworks.csv. We can use the head command to display the first n lines of a file (10 by default). To display the first 3 lines, either of these commands will work: As with most datasets, it looks like the first line contains the column headers. Other than that, the output is messy and it\u2019s hard to glean anything else from the default output. We need a tool that knows how to display CSV files in a readable way. csvlook is a tool within csvkit that allows you to display and preview a CSV file as a table. csvlook artworks.csv will display the entire dataset, which is cumbersome to explore. Let\u2019s instead pipe the stdout of head -5 artworks.csv to csvlook to explore the first 5 lines: Here\u2019s a diagram representing the pipeline: While the output is now easier to read, it\u2019s still a little difficult to explore. Let\u2019s now learn how to use csvcut to select just a few columns and display those. csvcut is a tool within csvkit that is referred to as the data scalpel since it allows you to slice and modify columns in a CSV. Let\u2019s first list all the columns using the -n flag: We can then use the -c flag to specify the columns we want. csvcut -c 1,2,3 artworks.csv | csvlook will return the first 3 columns. You can also use the column names themselves: csvcut -c Artist,ArtistBio,Date. Running either command will display the 3 columns for the entire dataset so we\u2019ll need to take advantage of piping to view just a few lines. We can consult the csvkit documentation to read about piping between csvkit utilities: All csvkit utilities accept an input file as \u201cstandard in\u201d, in addition to as a filename. This means that we can make the output of one csvkit utility become the input of the next. This means we can pipe the stdout of csvcut to the stdin of csvlook! We can build the following pipeline: When working with historical datasets, we need to make sure the date and time columns are formatted correctly (or even basic time series plots won\u2019t work). Let\u2019s explore the Date and DateAcquired columns: While the first 20 values in DateAcquired look fine, the Date column has some values that won\u2019t play nicely with most data visualization tools like 1976-77. We can easily deal with this by just selecting the first year in the range (e.g. 1976 from the range 1976-77). Before we do that, let\u2019s figure out how many lines match this pattern. We can use the csvgrep tool to extract all values in a column (or columns) that match a regular expression. We specify the columns we want csvgrep to match on using the -c flag. We specify the regular expression we want csvgrep to use using the -regex flag. The regex ^([0-9]*-[0-9]*) matches pairs of numeric values that are separated by a hyphen (-). Since we\u2019re searching for instances of the pattern on the Date column, we write the following: Let\u2019s modify and run the pipeline we\u2019ve built to incorporate csvgrep: Wonderful! Now that we know it works, let\u2019s apply the regular expression over the entire Date column (instead of just the first 10 lines) and determine how many lines match this pattern. The csvstat tool takes a CSV as an input (stdin) and computes summary statistics. We can use the --count flag to specify that we only want the line count. We can also remove csvcut, head, and csvlook since we don\u2019t need to display the output. It looks like 18,073 lines match this pattern. Let\u2019s now calculate: We can use the regex (^[0-9]{4}$) to find all 4 digit year values and pipe the results to csvstat: Finally, to get the total number of lines of a dataset, we can use the wc command with the -l flag (to display just the number of `lines): If we combine the number of lines that match the 4 digit year regex (76263) with the number of lines that match the year range regex (18073), we get (94336) lines. Given that there are 137,382 lines total, this is a great starting point for our analysis!",
    "link": "https://www.dataquest.io/blog/data-cleaning-command-line/",
    "title": "Cleaning CSV Data Using the Command Line and csvkit"
}{
    "code": 0,
    "content": "  To highlight how Dataquest has changed people\u2019s lives,  we\u2019ve started a new blog series called User Stories where we interview our users to learn more about their personal journey and how we\u2019ve helped them get where they needed to. \nIn this post, we interview Patrick Kennedy, Consultant, Author, and Entrepreneur.  Patrick has held executive roles at a variety of real estate firms.  He's now learning data science because he wants to explore its impact on existing industries.  He has quite a few ideas for data science companies and projects that he's working on.\n Hard to put into words.  It has been phenomenal.  It is exactly what I needed to bridge the gap between my former life as an experimental psychologist and my current life as someone focused on building businesses.  I knew much of the statistics and some of the coding but was completely out of the loop with the model building techniques housed in Sci-kit learn.  I was stuck knowing that a harder look at data would yield great results but was still manually building the applications to tie our financial reporting to our sales pipeline to our individual sales person goals to our budget to our performance the previous year.  It was a bear.  Now it is terribly exciting to think about what I can produce with these types of models.  Almost more importantly though, it has shown me a new way to think about developing intelligence. I learned about DataQuest from a recruiter at Galvanize. I learned about Galvanize from a meeting with another large commercial real estate firm that transacted their deal here in Austin.  Once I heard about Galvanize\u2019s offering, it formed the catalyst where I knew my future lay in connecting data with business in a way that I was not experiencing in my former position.  The recruiter at Galvanize passed me to a quora post by Galvanize\u2019s CTO who mentioned DataQuest as a great tool.  I laid out a plan to tear through as much of the DataQuest content as I could, as fast as I could.  It has been a struggle due to all the new content you all produce! I see it as a stepping stone.  From everything I have read, the data science immersive is going to kick my butt.  However, I get my feet wet with DQ so I can know what I am doing.  I can explore a variety of different topics and I can do so in a structured environment.  It is the best way to learn.  With the introduction of the guided projects, DQ takes the training wheels off a bit but still provides the basic outline of how to organize your thoughts in a data science process.  General Assembly provides a little less structure and a little more responsibility to figure it out on your own with a capstone project that you pick, investigate and present on your own.  GA provides assistance and teaching all sorts of aspects of data science in a hands on way to allow one to succeed in developing their project but ultimately it is up to the individual to get after it.  Galvanize seems to be throwing the individual in the fire and seeing what comes out cooked on the other end.  With adequate preparation before hand through DQ and GA, I think it is a perfect 1-2-3. My plan is two-fold.  First, I am poking at a variety of different business ideas to see what may launch, each with an aspect of data science to it, and some more than others.  Having sold my last business I\u2019m lucky to have the freedom to poke.  Second, and knowing that it is quite difficult to get a business afloat, I am leveraging Galvanize\u2019s outcome assistance to identify some good spots for me.  It is always nice when a group is contractually obligated to place you.  In the end I don\u2019t imagine I\u2019ll be doing just one thing - I get bored too easily.  I may even just write another book on the process of learning data science. Well it has been an interesting path so far - I wish I could say it was all premeditated.  But the real feel for data came as a PhD candidate in experimental psych at Columbia.  It was the first time I really dug into specific tools to analyze data as well as building software applications for experiments and structuring them in a way to make data collection easier on the back end.  It forced me to think about data in a soup to nuts way.  Plus being around a cohort of individuals all after the same sort of goal allowed me to figure out different ways of doing things.  My good friend Dave taught me about hierarchical mixed models and Baruch taught me how to analyze small data sets in meaningful ways.  The trouble for me is that once we had the insight, the goal was to publish, promote and move on to the next study.  No part of that involved helping others.  My research was on the feeling of being in the zone with aspects I was trying to involve in video game design, peak performance and employee engagement or burnout.  Time and time again I was told that my focus should be on research not applying the research. After meeting with a (now former) advisor about all the interesting applications the in-the-zone research could touch upon, she told me she knew nothing about business and that maybe I wasn\u2019t cut out for research.  So I took her up on that and left in the midst of pulling together my dissertation proposal.   Unfortunately for me I threw the baby out with the bathwater and tried to avoid strict data analysis at all costs, thinking that it was relegated to those Ivory Towers.  Instead I got into the start-up world, launched a couple of my own (one became a zombie, the other burned up quick), got a business degree and worked at a technology incubator.  At the incubator I kept seeing these companies I\u2019d help refine a pitch deck (made easy thanks to my years of teaching at Columbia) go on to raise a good amount of funds and graduate.  But I wanted more than just to usher a company into an adolescent stage. I wanted to be in one of those companies.  I did my best to weasel in but was told I could either be sales or engineering.  I didn\u2019t think of myself an engineer and sales put a sour taste in my mouth so I did what any non-sales, non-engineering person does: consult. One of the companies I consulted with ended up hiring me to join the firm as a director of operations.  It was a commercial real estate firm - something I had only a vague idea about.  What they needed seemed simple enough: help figuring out who did what.  It was confusing to me that this was even an issue but when you try to grow fast without an operations backbone you miss out on some of these structural components.  So I stepped in and immediately saw an application for all that research on being in the zone I conducted years before.  I could use my research on employee engagement to grow a culture and see how that may affect revenue down the line (hint: it affects it significantly).   So here I was using my old research and it was working.  It prompted the question for me - what else can I do here?  I worked like crazy developing all sorts of tools and procedures that resulted in my first published book, my promotion to COO and ultimately transacting the sale of the company to a global firm.  It was awesome.  Stressful, but awesome.  And it was made by an investigation into all sorts of data.  Building a real time sales funnel requires data collection, munging, analytic and presentation techniques.  I was manually doing much of it but continued to find new and improved ways to automate.  At about this time however things started to get more friction. The more I tried developing scalable techniques for the ultimate goal of being able to both predict a prospect\u2019s likelihood of signing a letter of intent with us and predict which broker would best maximize that likelihood, the more resistance I felt.  While I was put in charge of three offices sales teams of about 80 people, no one really cared about maximizing revenue.  It was an oddity that took me a while to wrap my head around.  The short story is that brokers typically make more than 50% of a sales commission allowing for a 3-5 year old broker to be making half a million dollars a year.  Given brokerage firms typically do not have much money left over after giving that large of an amount of money to the broker, this results in little to no management, or management by broker.  And one can easily assume what will happen if a broker/manager can make millions of dollars a year brokering, what they will spend their time doing. I was fighting a losing battle.  Trying to push brokers to make more, when they are able to play golf 4 days a week and make half a mil a year.  Why would they want to work instead of golf?  Sure they all recognized it was important but it was important for somebody else to take part in the system. Not them.  As with many ideas, my frustration led me to once again realize the power of data.  But data needs a context in which to thrive.  The data I was generating, manipulating and presenting was having little to no effect.  I needed a switch. There is considerable data involved in any sales organization from tonality, sentiment and word choice in an initial cold call to predictability in pipeline management to recommendations for site and office space location.  Heck there is abundant room in investigating negotiation techniques.  I decided that instead of fighting all around me for these more old school brokers to turn a new leaf, I wanted to learn whatever I could as fast as I could to investigate all these potential data sources.  I wanted to know how to collect, how to manipulate, how to get that data to talk to me in a meaningful way. This is what brought me into the data science fold.  And yes, this is a super long answer to the question. Data science can operate at almost any level of a business.  It could even run businesses, but that may be just me being influenced by this book I just finished reading (Darknet by Matthew Mather).  For operations, specifically, it is huge.  Whether you are running a business with heavy supply chain management or a predominantly sales outfit like I was in, data science is a crucial part.  Partly because the term data science encompasses so much but also because there is data everywhere.  Each successful application of a data science technique ought to result in at least two more opportunities for application.  As a for instance, we had used a CRM to manage all of our data.  But after spending more money than I care to admit, we had to hire someone to manage the database because no one wanted to use it directly and 99% of the time it was used merely as a digital Rolodex.  RelateIQ had done a great job of using natural language processing techniques to collect data from a variety of different sources (calendar, email, VoIP phones, etc) so that the data entry aspect of a CRM was obviated.  There was another tool (whose name I now forget), served to give you the likelihood of making a deal based on your email chain.  Now LinkedIn has their Sales Navigator tool which makes their entire platform a CRM. With internal operations, sales is a no brainer but management has plenty of opportunities for data science applications.  Goal setting is huge in organizations now.  Rather than standard performance reviews (which really only serve as a paper trail when you are angling to terminate someone), goal setting allows a manager to build rapport with their employees and encourage them along their path.  Most goal setting \u201cprograms\u201d in organizations are housed in excel files and the information lies dormant.  Instead this information can be fed into applications that provide assistance if an employee is stuck on a particular project and hasn\u2019t updated their status in a while or alternatively that recommend collaboration with others if other people are working on similar applications unbeknownst to them.  Much of management is inspiring employees and facilitating conversations.  Both of these roles can be automated to certain degrees. The first is that it is a fad.  There are many people here in Austin I have spoken to that roll their eyes when they hear the term \u2018data science\u2019.  I get it.  It is a hot new thing that companies say they need even though they don\u2019t really know why.  Sure that is annoying.  And statisticians, computer scientists and engineers have known aspects of these techniques for quite some time.  I was surprised when I learned much of the data science model building is done with regressions, considering that regressions were the models I used for most of my experiments! I just never knew the relation between model building in a experimental context and model building in a business or engineering context.  But underlying this is that data science itself is not a fad. It is the reorganization of a variety of different disciplines.  That is progress! To be able to take somewhat unrelated fields and put them together is great.  Sure the fervor may calm down a bit in the next few years but the importance of the work done in the field won\u2019t. The second is that it is difficult to learn.  Some people say they don\u2019t get code. Others say they are not math people.  And sure if you are going to want to be the best data scientist there ever was, you are probably going to want to know a bit about code and math.  But you don\u2019t have to be the best.  You just have to be good enough to get your goal accomplished.  So for those who are genuinely interested but are scared by the new language you will have to learn or doing math problems, think about what you want to be able to do.  You want to build a new type of movie recommendation service?  Start reading about services out there, what a recommendation is made from and what those recommendation services are coded in.  Break it down into chunks.  Don\u2019t start by picking up a book on Java or Python or whatever. You\u2019ll get turned off pretty quick because there is no goal pulling you forward.  I made a mistake when I was a guitar tutor in college.  I was tutoring a boy about 9 years old. He wanted to play Elvis songs.  I instead told him that he needed to learn his scales first so we spent the first three sessions of 30 minutes just going over scales.  His mom called me up before session four and said, sorry, he\u2019s not interested anymore.  I feel sorry that he may have lost interest in a musical instrument because of me but at least it taught me a valuable lesson: let the goal drive performance. To be perfectly frank, I want to learn how to win a Kaggle competition.  I\u2019m in the midst of one right now and am throwing my limited knowledge at how to build a better and better model.  But phew, there are some good people doing these competitions!  I have a hard time turning my competitiveness off\u2026 I want to win :-) Beyond that though, the most interesting aspect of data science to me is natural language processing.  So much of how we feel, think and interpret others depends on language.  A friend of mine did a clinical psych degree at Harvard where his research showed mobile therapeutic interventions helped alleviate anxious and depressive feelings.  Can you imagine Siri as your therapist?  There is a lot of good to be done out there and I think we are at the cusp of something great. Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/user-story-patrick-kennedy/",
    "title": "How Patrick Kennedy used Dataquest to transition into data science"
}{
    "code": 1,
    "content": "  Python has a variety of visualization libraries, including seaborn, networkx, and vispy.  Most Python visualization libraries are based wholly or partially on matplotlib, which often makes it the first resort for making simple plots, and the last resort for making plots too complex to create in other libraries. In this matplotlib tutorial, we\u2019ll cover the basics of the library, and walk through making some intermediate visualizations.   We\u2019ll be working with a dataset of approximately 240,000 tweets about Hillary Clinton, Donald Trump, and Bernie Sanders, all current candidates for president of the United States. The data was pulled from the Twitter Streaming API, and the csv of all 240,000 tweets can be downloaded here.  If you want to scrape more data yourself, you can look here for the scraper code. Before we get started with plotting, let\u2019s load in the data and do some basic exploration.  We can use Pandas, a Python library for data analysis, to help us with this.  In the below code, we\u2019ll: Here\u2019s a quick explanation of the important columns in the data: Most of the interesting things we can do with this dataset involve comparing the tweets about one candidate to the tweets about another candidate.  For example, we could compare how objective tweets about Donald Trump are to how objective tweets about Bernie Sanders are. In order to accomplish this, we first need to generate a column that tells us what candidates are mentioned in each tweet.  In the below code, we\u2019ll: Now that we have the preliminaries out the way, we\u2019re ready to draw our first plot using matplotlib.  In matplotlib, drawing a plot involves: Because of its flexible structure, you can draw multiple plots into a single image in matplotlib.  Each Axes object represents a single plot, like a bar plot or a histogram. This may sound complicated, but matplotlib has convenience methods that do all the work of setting up a Figure and Axes object for us. In order to use matplotlib, you\u2019ll need to first import the library using import matplotlib.pyplot as plt.  If you\u2019re using Jupyter notebook, you can setup matplotlib to work inside the notebook using %matplotlib inline. We import matplotlib.pyplot because this contains the plotting functions of matplotlib.  We rename it to plt for convenience, so it\u2019s faster to make plots. Once we\u2019ve imported matplotlib, we can make a bar plot of how many tweets mentioned each candidate.  In order to do this, we\u2019ll: It\u2019s pretty surprising how many more tweets are about Trump than are about Sanders or Clinton! You may notice that we don\u2019t create a Figure, or any Axes objects.  This is because calling plt.bar will automatically setup a Figure and a single Axes object, representing the bar plot.  Calling the plt.show method will show anything in the current figure.  In this case, it shows an image containing a bar plot. matplotlib has a few methods in the pyplot module that make creating common types of plots faster and more convenient because they automatically create a Figure and an Axes object.  The most widely used are: Calling any of these methods will automatically setup Figure and Axes objects, and draw the plot.  Each of these methods has different parameters that can be passed in to modify the resulting plot. Now that we\u2019ve made a basic first plot, we can move on to creating a more customized second plot.  We\u2019ll make a basic histogram, then modify it to add labels and other information. One of the things we can look at is the age of the user accounts that are tweeting.  We\u2019ll be able to find if there differences in when the accounts of users who tweet about Trump and when the accounts of users who tweet about Clinton were created.  One candidate having more user accounts created recently might imply some kind of manipulation of Twitter with fake accounts. In the code below, we\u2019ll: We can add titles and axis labels to matplotlib plots.  The common methods with which to do this are: Since all of the methods we discussed before, like bar and hist, automatically create a Figure and a single Axes object inside the figure, these labels will be added to the Axes object when the method is called. We can add labels to our previous histogram using the above methods.  In the code below, we\u2019ll: The current histogram does a nice job of telling us the account age of all tweeters, but it doesn\u2019t break it down by candidate, which might be more interesting.  We can leverage the additional options in the hist method to create a stacked histogram. In the below code, we\u2019ll: We can take advantage of matplotlibs ability to draw text over plots to add annotations.  Annotations point to a specific part of the chart, and let us add a snippet describing something to look at. In the code below, we\u2019ll make the same histogram as we did above, but we\u2019ll call the plt.annotate method to add an annotation to the plot. Here\u2019s a description of what the options passed into annotate do: As you can see, there are significantly more tweets about Trump then there are about other candidates, but there doesn\u2019t look to be a significant difference in account ages. So far, we\u2019ve been using methods like plt.bar and plt.hist, which automatically create a Figure object and an Axes object.  However, we can explicitly create these objects when we want more control over our plots.  One situation in which we would want more control is when we want to put multiple plots side by side in the same image. We can generate a Figure and multiple Axes objects by calling the plt.subplots methods.  We pass in two arguments, nrows, and ncols, which define the layout of the Axes objects in the Figure.  For example, plt.subplots(nrows=2, ncols=2) will generate ` 2x2 grid of Axes objects.  plt.subplots(nrows=2, ncols=1) will generate a 2x1` grid of Axes objects, and stack the two Axes vertically. Each Axes object supports most of the methods from pyplot.  For instance, we could call the bar method on an Axes object to generate a bar chart. We\u2019ll generate 4 plots that show the amount of the colors red and blue in the Twitter background colors of users tweeting about Trump.  This may show if tweeters who identify as Republican are more likely to put red in their profile. First, we\u2019ll generate two columns, red and blue, that tell us how much of each color is in each tweeter\u2019s profile background, from 0 to 1. In the code below, we\u2019ll: Once we have the data setup, we can create the plots.  Each plot will be a histogram showing how many tweeters have a profile background containing a certain amount of blue or red. In the below code, we: Twitter has default profile background colors that we should probably remove so we can cut through the noise and generate a more accurate plot.  The colors are in hexadecimal format, where #000000 is black, and #ffffff is white. Here\u2019s how to find the most common colors in background colors: Now, we can remove the three most common colors, and only plot out users who have unique background colors.  The code below is mostly what we did earlier, but we\u2019ll: As you can see, the distribution of blue and red in background colors for users that tweeted about Trump is almost identical to the distribution for all tweeters. We generated sentiment scores for each tweet using TextBlob, which are stored in the polarity column.  We can plot the mean value for each candidate, along with the standard deviation.  The standard deviation will tell us how wide the variation is between all the tweets, whereas the mean will tell us how the average tweet is. In order to do this, we can add 2 Axes to a single Figure, and plot the mean of polarity in one, and the standard deviation in the other.  Because there are a lot of text labels in these plots, we\u2019ll need to increase the size of the generated figure to match.  We can do this with the figsize option in the plt.subplots method. The code below will: We can plot tweet length by candidate using a bar plot.  We\u2019ll first split the tweets into short, medium, and long tweets.  Then, we\u2019ll count up how many tweets mentioning each candidate fall into each group.  Then, we\u2019ll generate a bar plot with bars for each candidate side by side. To plot the tweet lengths, we\u2019ll first have to categorize the tweets, then figure out how many tweets by each candidate fall into each bin. In the code below, we\u2019ll: Now that we have the data we want to plot, we can generate our side by side bar plot.  We\u2019ll use the bar method to plot the tweet lengths for each candidate on the same axis.  However, we\u2019ll use an offset to shift the bars to the right for the second and third candidates we plot.  This will give us three category areas, short, medium, and long, with one bar for each candidate in each area.  In the code below, we: We\u2019ve learned quite a bit about how matplotlib generates plots, and gone through a good bit of the dataset.  If you want to read more about how matplotlib plots internally, read this. You can make quite a few plots next: Hope this matplotlib tutorial was helpful, if you do any interesting analysis with this data please leave a comment and link below - we\u2019d love to know!",
    "link": "https://www.dataquest.io/blog/matplotlib-tutorial/",
    "title": "Matplotlib tutorial: Plotting tweets mentioning Trump, Clinton & Sanders"
}{
    "code": 1,
    "content": "  Working with large JSON datasets can be a pain, particularly when they are too large to fit into memory.  In cases like this, a combination of command line tools and Python can make for an efficient way to explore and analyze the data.  In this post, we\u2019ll look at how to leverage tools like Pandas to explore and map out police activity in Montgomery County, Maryland.  We\u2019ll start with a look at the JSON data, then segue into exploration and analysis of the JSON with Python. When data is stored in SQL databases, it tends to follow a rigid structure that looks like a table.  Here\u2019s an example from a SQLite database: As you can see, the data consists of rows and columns, where each column maps to a defined property, like id, or code.  In the dataset above, each row represents a country, and each column represents some fact about that country. But as the amount of data we capture increases, we often don\u2019t know the exact structure of the data at the time we store it.  This is called unstructured data.  A good example is a list of events from visitors on a website.  Here\u2019s an example of a list of events sent to a server: As you can see, three separate events are listed above.  Each event has different fields, and some of the fields are nested within other fields.  This type of data is very hard to store in a regular SQL database.  This unstructured data is often stored in a format called JavaScript Object Notation (JSON).  JSON is a way to encode data structures like lists and dictionaries to strings that ensures that they are easily readable by machines.  Even though JSON starts with the word Javascript, it\u2019s actually just a format, and can be read by any language. Python has great JSON support, with the json library.  We can both convert lists and dictionaries to JSON, and convert strings to lists and dictionaries.  JSON data looks much like a dictionary would in Python, with keys and values stored. In this post, we\u2019ll explore a JSON file on the command line, then import it into Python and work with it using Pandas. We\u2019ll be looking at a dataset that contains information on traffic violations in Montgomery County, Maryland.  You can download the data here.  The data contains information about where the violation happened, the type of car, demographics on the person receiving the violation, and some other interesting information.  There are quite a few questions we could answer using this dataset, including: Unfortunately, we don\u2019t know the structure of the JSON file upfront, so we\u2019ll need to do some exploration to figure it out.  We\u2019ll use Jupyter Notebook for this exploration. Even though the JSON file is only 600MB, we\u2019ll treat it like it\u2019s much larger so we can explore how analyzing a JSON file that doesn\u2019t fit into memory might work.  The first thing we\u2019ll do is take a look at the first few lines of the md_traffic.json file.  A JSON file is just an ordinary text file, so we can use all the standard command line tools to interact with it: From this, we can tell that the JSON data is a dictionary, and is well formatted.  meta is a top level key, and is indented two spaces.  We can get all of the top level keys by using the grep command to print any lines that have two leading spaces: This shows us that meta and data are top level keys in the md_traffic.json data.  A list of lists appears to be associated with data, and this likely contains each record in our traffic violations dataset.  Each inner list is a record, and the first record appears in the output from the grep command.  This is very similar to the kind of structured data we\u2019re used to working with when we operate on CSV files or SQL tables.  Here\u2019s a truncated view of how the data might look: This looks a lot like the rows and columns that we\u2019re used to working with.  We\u2019re just missing the headers that tell us what each column means.  We may be able to find this information under the meta key. meta usually refers to information about the data itself.  Let\u2019s dig a little more into meta and see what information is contained there.  From the head command, we know that there are at least 3 levels of keys, with meta containing a key view, which contains the keys id, name, averageRating and others.  We can print out the full key structure of the JSON file by using grep to print out any lines with 2-6 leading spaces: This shows us the full key structure associated with md_traffic.json, and tell us which parts of the JSON file are relevant for us.  In this case, the columns key looks interesting, as it potentially contains information on the columns in the list of lists in the data key. Now that we know which key contains information on the columns, we need to read that information in.  Because we\u2019re assuming that the JSON file won\u2019t fit in memory, we can\u2019t just directly read it in using the json library.  Instead, we\u2019ll need to iteratively read it in in a memory-efficient way. We can accomplish this using the ijson package.  ijson will iteratively parse the json file instead of reading it all in at once.  This is slower than directly reading the whole file in, but it enables us to work with large files that can\u2019t fit in memory.  To use ijson, we specify a file we want to extract data from, then we specify a key path to extract: In the above code, we open the md_traffic.json file, then we use the items method in ijson to extract a list from the file.  We specify the path to the list using the meta.view.columns notation.  Recall that meta is a top level key, which contains view inside, which contains columns inside it.  We then specify meta.view.columns.item to indicate that we should extract each individual item in the meta.view.columns list.  The items function will return a generator, so we use the list method to turn the generator into a Python list.  We can print out the first item in the list: From the above output, it looks like each item in columns is a dictionary that contains information about each column.  In order to get our header, it looks like fieldName is the relevant key to extract.  To get our column names, we just have to extract the fieldName key from each item in columns: Great!  Now that we have our columns names, we can move to extracting the data itself. You may recall that the data is locked away in a list of lists inside the data key.  We\u2019ll need to read this data into memory to manipulate it.  Fortunately, we can use the column names we just extracted to only grab the columns that are relevant.  This will save a ton of space.  If the dataset was larger, you could iteratively process batches of rows.  So read in the first 10000000 rows, do some processing, then the next 10000000, and so on.  In this case, we can define the columns we care about, and again use ijson to iteratively process the JSON file: Now that we\u2019ve read the data in, we can print out the first item in data: Now that we have the data as a list of lists, and the column headers as a list, we can create a Pandas Dataframe to analyze the data.  If you\u2019re unfamiliar with Pandas, it\u2019s a data analysis library that uses an efficient, tabular data structure called a Dataframe to represent your data. Pandas allows you to convert a list of lists into a Dataframe and specify the column names separately. Now that we have our data in a Dataframe, we can do some interesting analysis.  Here\u2019s a table of how many stops are made by car color: Camouflage appears to be a very popular car color.  Here\u2019s a table of what kind of police unit created the citation: With the rise of red light cameras and speed lasers, it\u2019s interesting that patrol cars are still by far the dominant source of citations. We\u2019re now almost ready to do some time and location based analysis, but we need to convert the longitude, latitude, and date columns from strings to floats first.  We can use the below code to convert latitude and longitude: Oddly enough, time of day and the date of the stop are stored in two separate columns, time_of_stop, and date_of_stop.  We\u2019ll parse both, and turn them into a single datetime column: We can now make a plot of which days result in the most traffic stops: In this plot, Monday is 0, and Sunday is 6.  It looks like Sunday has the most stops, and Monday has the least.  This could also be a data quality issue where invalid dates resulted in Sunday for some reason.  You\u2019ll have to dig more deeply into the date_of_stop column to figure it out definitively (this is beyond the scope of this post). We can also plot out the most common traffic stop times: It looks like the most stops happen around midnight, and the fewest happen around 5am.  This might make sense, as people are driving home from bars and dinners late and night, and may be impaired.  This may also be a data quality issue, and poking through the time_of_stop column will be necessary to get a full answer. Now that we\u2019ve converted the location and date columns, we can map out the traffic stops.  Because mapping is very intensive in terms of CPU resources and memory, we\u2019ll need to filter down the rows we use from stops first: In the above code, we selected all of the rows that came in the past year.  We can further narrow this down, and only select rows that occurred during rush hour \u2013 the morning period when everyone is going to work: Using the excellent folium package, we can now visualize where all the stops occurred.  Folium allows you to easily create interactive maps in Python by leveraging leaflet.  In order to preserve performance, we\u2019ll only visualize the first 1000 rows of morning_rush: This shows that many traffic stops are concentrated around the bottom right of the county.  We can extend our analysis further with a heatmap: In this post, we learned how to use Python to go from raw JSON data to fully functional maps using command line tools, ijson, Pandas, matplotlib, and folium.  If you want to learn more about these tools, check out our Data Analysis, Data Visualization, and Command Line courses on Dataquest. If you want to further explore this dataset, here are some interesting questions to answer:",
    "link": "https://www.dataquest.io/blog/python-json-tutorial/",
    "title": "Python & JSON: Working with large datasets using Pandas"
}{
    "code": 1,
    "content": "  The Python scientific stack is fairly mature, and there are libraries for a variety of use cases, including machine learning, and data analysis.  Data visualization is an important part of being able to explore data and communicate results, but has lagged a bit behind other tools such as R in the past. Luckily, many new Python data visualization libraries have been created in the past few years to close the gap.  matplotlib has emerged as the main data visualization library, but there are also libraries such as vispy, bokeh, seaborn, pygal, folium, and networkx that either build on matplotlib or have functionality that it doesn\u2019t support. In this post, we\u2019ll use a real-world dataset, and use each of these libraries to make visualizations.  As we do that, we\u2019ll discover what areas each library is best in, and how to leverage the Python data visualization ecosystem most effectively.   At Dataquest, we\u2019ve built an interactive course that teaches you about Python data visualization tools.  If you want to learn in more depth, check it out here. Before we dive into visualizing the data, let\u2019s take a quick look at the dataset we\u2019ll be working with.  We\u2019ll be using data from openflights.  We\u2019ll be using route, airport, and airline data.  Each row in the route data corresponds to an airline route between two airports.  Each row in the airport data corresponds to an airport in the world, and has information about it.  Each row in the airline data represents a single airline. We first read in the data: The data doesn\u2019t have column headers, so we add them in by assigning to the columns attribute.  We want to read every column in as a string \u2013 this will make comparing across dataframes easier later, when we want to match rows based on id.  We do this by setting the dtype parameter when reading in the data. We can take a quick look at each dataframe: We can do a variety of interesting explorations with each dataset individually, but it\u2019s through combining them that we\u2019ll see the most gains.  Pandas will aid us as we do our analysis because it can easily filter matrices or apply functions across them.  We\u2019ll dive into a few interesting metrics, such as analyzing airlines and routes. Before we can do so, we need to do a bit of data cleaning: This line ensures that we have only numeric data in the airline_id column. Now that we understand the structure of the data, we can go ahead and start making plots to explore it.  For our first plot, we\u2019ll use matplotlib.  matplotlib is a relatively low-level plotting library in the Python stack, so it generally takes more commands to make nice-looking plots than it does with other libraries.  On the other hand, you can make almost any kind of plot with matplotlib.  It\u2019s very flexible, but that flexibility comes at the cost of verbosity. We\u2019ll first make a histogram showing the distribution of route lengths by airlines.  A histogram divides all the route lengths into ranges (or \u201cbins\u201d), and counts how many routes fall into each range.  This can tell us if airlines fly more shorter routes, or more longer ones. In order to do this, we need to first calculate route lengths.  The first step is a distance formula.  We\u2019ll use haversine distance, which calculates the distance between latitude, longitude pairs. Then we can make a function that calculates distance between the source and dest airports for a single route.  To do this, we need to get the source_id and dest_id airports from the routes dataframe, then match them up with the id column in the airports dataframe to get the latitude and longitude of those airports.  Then, it\u2019s just a matter of doing the calculation.  Here\u2019s the function: The function can fail if there\u2019s an invalid value in the source_id or dest_id columns, so we\u2019ll add in a try/except block to catch these. Finally, we\u2019ll use pandas to apply the distance calculation function across the routes dataframe.  This will give us a pandas series containing all the route lengths.  The route lengths are all in kilometers. Now that we have a series of route lengths, we can create a histogram, which will bin the values into ranges and count how many routes fall into each range: We import the matplotlib plotting functions with import matplotlib.pyplot as plt.  We then setup matplotlib to show plots in an ipython notebook with %matplotlib inline.  Finally, we can make a histogram with plt.hist(route_lengths, bins=20).  As we can see, airlines fly more short routes than long routes. We can make a similar plot with seaborn, a higher-level plotting library for Python.  Seaborn builds on matplotlib and makes certain types of plots, usually having to do with statistical work, simpler.  We can use the distplot function to plot a histogram with a kernel density estimate on top of it.  A kernel density estimate is a curve \u2013 essentially a smoothed version of the histogram that\u2019s easier to see patterns in. As you can see, seaborn also has nicer default styles than matplotlib.  Seaborn doesn\u2019t have its own version of all the matplotlib plots, but it\u2019s a nice way to quickly get nice-looking plots that go into more depth than default matplotlib charts.  It\u2019s also a good library if you need to go more into depth and do more statistical work. Histograms are great, but maybe we want to see the average route length by airline.  We can instead use a bar chart \u2013 this will have an individual bar for each airline, telling us the average length by airline.  This will let us see which carriers are regional, and which are international.  We can use pandas, a python data analysis library, to figure out the average route length per airline. We first make a new dataframe with the route lengths and the airline ids.  We split route_length_df  into groups based on the airline_id, essentially making one dataframe per airline.  We then use the pandas aggregate function to take the mean of the length column in each airline dataframe, and recombine each result into a new dataframe.  We then sort the dataframe so that the airlines with the most routes come first. We can then plot this out with matplotlib: The matplotlib plt.bar method plots each airline against the average route length each airline flies(airline_route_lengths[\"length\"]). The problem with the plot above is that we can\u2019t easily see which airline has what route length.  In order to do this, we\u2019ll need to be able to see the axis labels.  This is a bit tough since there are so many airlines.  One way to make this easier to work with is to make the plot interactive, which will allow us to zoom in and out to see the labels.  We can use the bokeh library for this \u2013 it makes it simple to make interactive, zoomable plots. To use bokeh, we\u2019ll need to preprocess our data first: The code above will get the names for each row in airline_route_lengths, and add in the name column, which contains the name of each airline.  We also add in the id column so we can do this lookup (the apply function doesn\u2019t pass in an index). Finally, we reset the index column to have all unique values.  Bokeh doesn\u2019t work properly without this. Now, we can move on to the charting piece: We call output_notebook to setup bokeh to show a plot in an ipython notebook.  Then, we make a bar plot, using our dataframe and certain columns.  Finally, the show function shows the plot. The plot generated in your notebook isn\u2019t an image \u2013 it\u2019s actually a javascript widget.  Because of this, we\u2019re showing a screenshot below instead of the actual chart. With this plot, we can zoom in and see which airlines fly the longest routes.  The image above makes the labels looked crunched together, but they are much easier to see as you zoom in. Pygal is a python data analysis library that makes attractive charts quickly.  We can use it to make a breakdown of routes by length.  We\u2019ll first divide our routes into short, medium, and long, and calculate the percentage of each in our route_lengths. We can then plot each one as a bar in a pygal horizontal bar chart: Above, we first create an empty chart.  Then, we add elements, including a title and bars.  Each bar is passed a percentage value (out of 100) showing how common that type of route is. Finally, we render the chart to a file, and use IPython\u2019s SVG display capabilities to load and show the file.  This plot looks quite a bit nicer than the default matplotlib charts, but we did need to write more code to create it.  Pygal may be good for small presentation-quality graphics. Scatter plots enable us to compare columns of data.  We can make a simple scatter plot to compare airline id number to length of airline names: First we calculate the length of each name by using the pandas apply method.  This will find the number of characters long each airline name is. We then make a scatter plot comparing the airline ids to the name lengths using matplotlib.  When we plot, we convert the id column of airlines to an integer type.  If we don\u2019t do this, the plot won\u2019t work, as it needs numeric values on the x-axis.  We can see that quite a few of the longer names appear in the earlier ids.  This may mean that airlines founded earlier tend to have longer names. We can verify this hunch using seaborn.  Seaborn has an augmented version of a scatterplot, a joint plot, that shows how correlated the two variables are, as well as the individual distributions of each. The above plot shows that there isn\u2019t any real correlation between the two variables \u2013 the r squared value is low. Our data is inherently a good fit for mapping \u2013 we have latitude and longitude pairs for airports, and for source and destination airports. The first map we can make is one that shows all the airports all over the world.  We can do this with the basemap extension to matplotlib.  This enables drawing world maps and adding points, and is very customizable. In the above code, we first draw a map of the world, using a mercator projection.  A mercator projection is a way to project the whole plot of the world onto a 2-d surface.  Then, we draw the airports on top of the map, using red dots. The problem with the above map is that it\u2019s hard to see where each airport is \u2013 they just kind of merge into a red blob in areas with high airport density. Just like with bokeh, there\u2019s an interactive mapping library, folium, we can use to zoom into the map and help us find individual airports. Folium uses leaflet.js to make a fully interactive map.  You can click on each airport to see the name in the popup.  A screenshot is shown above, but the actual map is much more impressive.  Folium also lets you modify options pretty extensively to make nicer markers, or add more things to the map. It would be pretty cool to see all the air routes on a map.  Luckily, we can use basemap to do this.  We\u2019ll draw great circles connecting source and destination airports.  Each circle will show the route of a single airliner.  Unfortunately, there are so many routes that showing them all would be a mess.  Instead, we\u2019ll show the first 3000 routes. The above code will draw a map, then draw the routes on top of it.  We add in some filters to prevent overly long routes from obscuring the others. The final exploration we\u2019ll do is drawing a network diagram of airports.  Each airport will be a node in the network, and we\u2019ll draw edges between nodes if there\u2019s a route between the airports.  If there are multiple routes, we\u2019ll add to the edge weight, to show that the airports are more connected.  We\u2019ll use the networkx library to do this. First, we\u2019ll need to compute the edge weights between airports. Once the above code finishes running, the weights dictionary contains every edge between two airports that has a weight higher than 2.  So any airports that are connected by 2 or more routes will appear. Now, we need to draw the graph. There has been a proliferation of Python libraries for data visualization, and it\u2019s possible to make almost any kind of visualization.  Most libraries build on matplotlib and make certain use cases simpler.  If you want to learn in more depth how to visualize data using matplotlib, seaborn, and other tools, check out our interactive course here.",
    "link": "https://www.dataquest.io/blog/python-data-visualization-libraries/",
    "title": "Python data visualization: Comparing 7 tools"
}{
    "code": 1,
    "content": "  Configuring a data science environment can be a pain.  Dealing with inconsistent package versions, having to dive through obscure error messages, and having to wait hours for packages to compile can be frustrating.  This makes it hard to get started with data science in the first place, and is a completely arbitrary barrier to entry. The past few years have seen the rise of technologies that help with this by creating isolated environments.  We\u2019ll be exploring one in particular, Docker.  Docker makes it fast and easy to create new data science environments, and use tools such as Jupyter notebooks to explore your data. With Docker, we can download an image file that contains a set of packages and data science tools.  We can then boot up a data science environment using this image within seconds, without the need to manually install packages or wait around.  This environment is called a Docker container.  Containers eliminate configuration problems \u2013 when you start a Docker container, it has a known good state, and all the packages work properly. The Docker whale is here to help In addition to lowering the barriers to getting started with data science, Docker also makes it possible to quickly create isolated environments with different Python and package versions without having to wait for packages to install in virtual environments. In this post, we\u2019ll cover the basics of Docker, how to install it, and how to leverage Docker containers to quickly get started with data science on your own machine using a docker data science container. Software that creates virtual machines has existed for decades.  Virtual machines allow you to emulate other computing environments on your computer.  For example, you could run Linux in a virtual machine, even if your computer runs Windows.  This would let you use Linux without having to actually install it on your machine \u2013 it would be running virtually, so you would be able to access it from within Windows.  You\u2019d be able to essentially click a program, and a Linux desktop would pop up in a window.  Virtual machines use images to boot up \u2013 you have to start a virtual machine with an image that corresponds to the operating system you want to use.  If you want to use Linux, you\u2019d use an image that contains all of the necessary files to create a Linux environment. An example of using Windows in a virtual machine on a mac Although virtual machines enable Linux development to take place on Windows, for example, they have some downsides.  Virtual machines take a long time to boot up, they require significant system resources, and it\u2019s hard to create a virtual machine from an image, install some packages, and then create another image.  Linux containers solve this problem by enabling multiple isolated environments to run on a single machine.  Think of containers as a faster, easier way to get started with virtual machines. Unfortunately, containers are a bit tricky to use, and it\u2019s not easy to manage and distribute container images.  We want these features so we can quickly download and start data science environments with specific package and tool configurations.  For instance, you might want to be able to quickly start a container that has Jupyter notebook, spark, and pandas already installed. Docker containers are a layer over Linux containers that makes them easier to manage and distribute.  Docker makes it easy to download images that correspond to a specific set of packages, and start them quickly.  Docker is cross-platform, and works on Mac, Windows, and Linux. These same advantages also apply to virtual environments, a way to create isolated Python environments.  The primary advantages of Docker over virtual environments are: Running a Docker image creates a Docker container.  For our purposes, we can run Jupyter notebook inside this container, and use a web browser to work with our data. The first step is installing Docker.  There\u2019s a graphical installer for Windows and Mac that makes this easy.  Here are the instructions for each OS: As part of this installation process, you\u2019ll need to use a shell prompt.  The shell prompt, also called the terminal or the command line, is a way to run commands on your machine from a text interface instead of graphically.  For example, you can launch a text editor by double clicking on notepad in Windows, or by typing nano in a Linux shell session.  There\u2019s a special version of the shell that comes pre-configured for using Docker commands.  Here\u2019s how to open it: You\u2019ll need to use this same shell prompt whenever the rest of this post mentions having to run a Docker command or type a specific command. The next step is to download the image you want.  Here are our currently available data science images: You can download the images by typing docker pull IMAGE_NAME.  If you wanted to pull dataquestio/python3-starter, you\u2019d type docker pull dataquestio/python3-starter into a shell prompt.  This will download the images from Docker Hub, which is like Github, but for Docker images.  It will download the image files onto your machine, so you can start a container with the image. Make a folder on your local machine that will correspond to where you want the notebooks stored.  This folder will contain all of your work, and will persist on your local machine, even if you terminate the docker container.  For this example, we\u2019ll make this folder at /home/vik/notebooks. Once you download the image, you can run it using docker run.  We need to pass in a few options to ensure that it\u2019s configured properly.  The full command looks like docker run -d -p 8888:8888 -v /home/vik/notebooks:/home/ds/notebooks dataquestio/python3-starter. You should change /home/vik/notebooks to whatever folder you created to store your notebooks in.  You should change dataquestio/python3-starter to your preferred docker image. Executing docker run will create a Docker container.  This is isolated from your local machine, and it may be helpful to think of it as a separate computer.  Inside this container, Jupyter notebook will be running, and we\u2019ll be able to access many data science packages. The docker run command will print a long string.  This is the unique id of your container, and is used when modifying the container with other docker containers.  We\u2019ll refer to it as the container id from now on. If you\u2019re running Linux, the next step is easy \u2013 just go to localhost:8888, and you should see the notebook running.  If you\u2019re on Windows or OSX, and you followed the Docker installation instructions earlier, you used docker-machine in your docker installation process.  The name of your local machine is default, and running docker-machine ip default will tell you the ip of the docker container.  If you used a different name, like dev, just swap it for default in the command.  Then, you just visit CONTAINER_IP:8888 to see the notebook (replace CONTAINER_IP with the ip of your container). This is what you should see At this point, you can make a new Jupyter notebook to test how things are working.  Try running a scikit-learn example from here: If you want to add data files into your environment, you have three options.  The first is to place them in the folder you created earlier to use for notebooks.  Any files you place in there will automatically be accessible from inside your Jupyter notebooks. The second way is to use the docker cp command.  Docker cp can copy files from your machine to the container, and vice versa.  Let\u2019s say you want to copy a file at /home/vik/data.csv to a container with id 4greg24134.  You would type docker cp /home/vik/data.csv 4greg24134:/home/ds/notebooks.  This will copy the data.csv file into the notebooks directory in the container.  You can place files anywhere you want, but putting them in the notebooks directory makes them easily accessible from Jupyter notebook. The third way is to use the upload button at the top right of the Jupyter notebook main page.  This will let you select a file and upload it to the notebooks directory in the container. Regardless of which method you choose, here\u2019s how you would load the file inside a Jupyter notebook: You may also want to get files from the container onto your local machine.  The easiest way is to place the files in the /home/ds/notebooks folder, where they will be automatically mirrored into your local machine. Another way is to again use docker cp.  Let\u2019s say you want to copy a file at /home/ds/notebooks/data.csv from a container with id 4greg24134 to the folder /home/vik/ on your machine.  You would type docker cp 4greg24134:/home/ds/notebooks/data.csv /home/vik/data.csv. A final way is to use the download options in the Jupyter interface.  Clicking on a non-notebook file in the browser view will download it to your local machine.  If you\u2019re working on a notebook, clicking \u201cFile\u201d, then \u201cdownload as\u201d will download it to your machine. If you want to install your own packages inside the container, you can get into it and run any normal bash shell commands.  In order to get into a container, you\u2019ll need to run docker exec.  Docker exec takes a specific container id, and a command to run.  For instance, typing docker exec -it 4greg24134 /bin/bash will open a shell prompt in the container with id 4greg24134.  The -it flags ensure that we keep an input session open with the container, and can enter commands. After running docker exec, you\u2019ll be put into a shell prompt inside the container.  The container is running python in a virtual environment called ds, which should already be activated. To install packages, just type pip install PACKAGE_NAME.  You could install requests with pip install requests. When you want to exit the container shell prompt, just type exit. When you\u2019re done exploring your data, you can shut down the docker container.  Use docker rm -f CONTAINER_ID to stop the container.  You should have your container id from earlier.  If you don\u2019t, you can find it by running docker ps.  Your notebooks will still be available on your local machine, in the folder you created, even after you shut down the container. Docker images are created from Dockerfiles.  Dockerfiles specify which packages and tools should be installed in an image.  By modifying Dockerfiles, you can change which packages and tools come with the image by default. If you want to build on the Docker data science images we\u2019ve discussed in this post, you can contribute to our Github repository here, which contains the Dockerfiles.  We welcome improvements to our current images, or the addition of new images focusing on tools other than Python.",
    "link": "https://www.dataquest.io/blog/docker-data-science/",
    "title": "Docker: Data Science Environment with Jupyter"
}{
    "code": 1,
    "content": "  At Dataquest, we\u2019ve released an interactive course on Spark, with a focus on PySpark. We explore the fundamentals of Map-Reduce and how to utilize PySpark to clean, transform, and munge data. In this post, we\u2019ll dive into how to install PySpark locally on your own computer and how to integrate it into the Jupyter Notebbok workflow.  Some familarity with the command line will be necessary to complete the installation. At a high level, these are the steps to install PySpark and integrate it with Jupyter notebook: Spark requires Java 7+, which you can download from Oracle\u2019s website: Head to the Spark downloads page, keep the default options in steps 1 to 3, and download a zipped version (.tgz file) of Spark from the link in step 4. Once you\u2019ve downloaded Spark, we recommend unzipping the folder and moving the unzipped folder to your home directory. To test that Spark was built properly, run the following command in the same folder (where Spark resides): and the interactive PySpark shell should start up. This is the interactive PySpark shell, similar to Jupyter, but if you run sc in the shell, you\u2019ll see the SparkContext object already initialized. You can write and run commands interactively in this shell just like you can with Jupyter. Environment variables are global variables that any program on your computer can access and contain specific settings and pieces of information that you want all programs to have access to. In our case, we need to specify the location of Spark and add some special arguments which we reference later. Use nano or vim to open ~/.bash_profile and add the following lines at the end: Replace \"$HOME/spark-1.5.1\" with the location of the folder you unzipped Spark to (and also make sure the version numbers match!). The last step is to create a profile for Jupyter specifically for PySpark with some custom settings. To create this profile, run: Use nano or vim to create the following Python script at the following location: and then add the following to it: If you\u2019re using a later version than Spark 1.5, replace \u201cSpark 1.5\u201d with the version you\u2019re using, in the script. To start Jupyter Notebook with the pyspark profile, run: To test that PySpark was loaded properly, create a new notebook and run sc in one of the code cells to make sure the SparkContext object was initialized properly.",
    "link": "https://www.dataquest.io/blog/pyspark-installation-guide/",
    "title": "PySpark: How to install and Integrate with the Jupyter Notebook"
}{
    "code": 0,
    "content": "  It\u2019s an exciting time for data science.  The field is new, but growing quickly.  There\u2019s huge demand for data scientists \u2013 average compensation in SF is well north of 100 thousand dollars a year.  Where there\u2019s money, there are also people trying to earn it.  The data science skills gap means that many people are learning or trying to learn data science. The first step to learning data science is usually asking \u201chow do I learn data science?\u201d.  The response to this question tends to be a long list of courses to take and books to read, starting with linear algebra or statistics.  I went through this myself a few years ago when I was learning.  I had no programming background, but knew that I wanted to work with data. I can\u2019t fully explain how immensely unmotivating it is to be given a huge list of resources without any context.  It\u2019s akin to a teacher handing you a stack of textbooks and saying \u201cread all of these\u201d.  I struggled with this approach when I was in school.  If I had started learning data science this way, I never would have kept going. Some people learn best with a list of books, but I learn best by building and trying things.  I learn when I\u2019m motivated, and when I know why I\u2019m learning something.  Best of all, when you learn this way, you come out with immediately useful skills.  From my conversations with new learners over the years, I know many share these views. That\u2019s why I don\u2019t think your first goal should be to learn linear algebra or statistics.  If you want to learn data science, your first goal should be to learn to love data.  Interested in finding out how?  Read on to see how to actually learn data science.  \nAn example of the visualizations you can make with data science (via The Economist) Nobody ever talks about motivation in learning.  Data science is a broad and fuzzy field, which makes it hard to learn.  Really hard.  Without motivation, you\u2019ll end up stopping halfway through and believing you can\u2019t do it, when the fault isn\u2019t with you \u2013 it\u2019s with the teaching. You need something that will motivate you to keep learning, even when it\u2019s midnight, formulas are starting to look blurry, and you\u2019re wondering if this will be the night that neural networks finally make sense.   You need something that will make you find the linkages between statistics, linear algebra, and neural networks.  Something that will prevent you from struggling with the \u201cwhat do I learn next?\u201d question. My entry point to data science was predicting the stock market, although I didn\u2019t know it at the time.  Some of the first programs I coded to predict the stock market involved almost no statistics.  But I knew they weren\u2019t performing well, so I worked day and night to make them better. I was obsessed with improving the performance of my programs.  I was obsessed with the stock market.  I was learning to love data.  And because I was learning to love data, I was motivated to learn anything I needed to make my programs better. Not everyone is obsessed with predicting the stock market, I know.  But it\u2019s important to find that thing that make you want to learn. It can be figuring out new and interesting things about your city, mapping all the devices on the internet, finding the real positions NBA players play, mapping refugees by year, or anything else.  The great thing about data science is that there are infinite interesting things to work on \u2013 it\u2019s all about asking questions and finding a way to get answers. Take control of your learning by tailoring it to what you want to do, not the other way around. \nA map of all the devices on the internet Learning about neural networks, image recognition, and other cutting-edge techniques is important.  But most data science doesn\u2019t involve any of it: What all of this means is that the best way to learn is to work on projects.  By working on projects, you gain skills that are immediately applicable and useful.  You also have a nice way to build a portfolio.  One technique to start projects is to find a dataset you like.  Answer an interesting question about it.  Rinse and repeat. Here are some good places to find datasets to get you started: Another technique (and my technique) was to find a deep problem, predicting the stock market, that could be broken down into small steps.  I first connected to the yahoo finance API, and pulled down daily price data.  I then created some indicators, like average price over the past few days, and used them to predict the future (no real algorithms here, just technical analysis).  This didn\u2019t work so well, so I learned some statistics, and then used linear regression.  Then I connected to another API, scraped minute by minute data, and stored it in a SQL database.  And so on, until the algorithm worked well. The great thing about this is that I had context for my learning.  I didn\u2019t just learn SQL syntax \u2013 I used it to store price data, and thus learned 10x as much as I would have by just studying syntax.  Learning without application isn\u2019t going to be retained very well, and won\u2019t prepare you to do actual data science work. \nThis guy\u2019s trying to predict the stock market, but needs some data science, apparently (via DailyMail) Data scientists constantly need to present the results of their analysis to others.  Skill at doing this can be the difference between an okay and a great data scientist. Part of communicating insights is understanding the topic and theory well.   Another part is understanding how to clearly organize your results.  The final piece is being able to explain your analysis clearly. It\u2019s hard to get good at communicating complex concepts effectively, but here are some things you should try: It\u2019s amazing how much you can learn from working with others.  In data science, teamwork can also be very important in a job setting. Some ideas here: Are you completely comfortable with the project you\u2019re working on?  Was the last time you used a new concept a week ago?  It\u2019s time to work on something more difficult.  Data science is a steep mountain to climb, and if you stop climbing, it\u2019s easy to never make it. If you find yourself getting too comfortable, here are some ideas: This is less a roadmap of exactly what to do that it is a rough set of guidelines to follow as you learn data science.  If you do all of these things well, you\u2019ll find that you\u2019re naturally developing data science expertise. I generally dislike the \u201chere\u2019s a big list of stuff\u201d approach, because it makes it extremely hard to figure out what to do next.  I\u2019ve seen a lot of people give up learning when confronted with a giant list of textbooks and MOOCs. I personally believe that anyone can learn data science if they approach it with the right frame of mind. I\u2019m also the founder of Dataquest, a site that helps you learn data science in your browser.  It encapsulates a lot of the ideas discussed in this post to create a better learning experience.  You learn by analyzing interesting datasets like CIA documents and NBA player stats.  You also complete projects and build a portfolio.  It\u2019s not a problem if you don\u2019t know how to code \u2013 we teach you python.  We teach python because it\u2019s the most beginner-friendly language, is used in a lot of production data science work, and can be used for a variety of applications. As I worked on projects, I found these resources helpful.  Remember, resources on their own aren\u2019t useful \u2013 find a context for them: This post is adapted from my Quora answer on how to become a data scientist.",
    "link": "https://www.dataquest.io/blog/learn-data-science/",
    "title": "How to actually learn data science"
}{
    "code": 1,
    "content": "  Art is a messy business. Over centuries, artists have created everything from simple paintings to complex sculptures, and art historians have been cataloging everything they can along the way. The Museum of Modern Art, or MoMA for short, is considered one of the most influential museums in the world and recently released a dataset of all the artworks they\u2019ve cataloged in their collection. This dataset contains basic information on metadata for each artwork and is part of MoMA\u2019s push to make art more accessible to everyone. The museum has put out a disclaimer however that the dataset is still a work in progress - an evolving artwork in its own right perhaps. Because it\u2019s still in progress, the dataset has data quality issues and needs some cleanup before we can analyze it.  In this post, we\u2019ll introduce a few tools for cleaning data (or munging data, as it\u2019s sometimes called) in python. and discuss how we can use them to diagnose data quality issues and correct them. You can download the dataset for yourself on Github. Python is a programming language that institutions ranging from government agencies, like the SEC, to internet companies, like Dropbox, use to create powerful software. Python has become the dominant language used for data analysis within organizations that work heavily with data. While we won\u2019t dive into too much detail about how Python works in this post, we do have a Learn Python series we highly recommend if you\u2019re interested in learning more. For this post, we\u2019ll be working with just the first 100 rows of the dataset. We will first need to import the Pandas library into our environment and then read in the dataset into a Pandas DataFrame. A DataFrame is a speed-optimized representation of our dataset, built into Pandas, which we can then use to quickly explore and analyze our data. Once we\u2019ve read the dataset into a DataFrame object, artworks_100, we will then If you look carefully at the Date column, you may notice that some of the values are year ranges (1976-77) instead of individual years (1976). Year ranges are difficult to work with and we can\u2019t easily plot them like individual years. Let\u2019s utilize the value_counts() function from Pandas to look for any other oddities. First, to select a column, use the bracket notation and specify the column name in quotes artworks_100['Date'] and then attach .value_counts() to get the distribution of values artworks_100['Date'].value_counts(). In addition to year ranges, we have three other patterns we need to watch out for. Here\u2019s a quick summary of the types of irregular values in the Date column: If we come up with rules to deal with each pattern, we can write out the logic in Python and transform all of the irregular values into the appropriate format. Once we write out our logic, we can iterate over the DataFrame row by row and change the value for the Date column if necessary. While rows with pattern 1 or 2 have date values and are just poorly formatted for us, rows with pattern 3 and 4 actually don\u2019t have information on when the artworks were made. Therefore, our code for handling patterns 1 and 2 should focus on reformatting the values into clean dates, while our code for patterns for 3 and 4 should just identify those columns as missing date information. To keep things simple, we can leave the rows with pattern 3 as is (as \"Unknown\") and transform rows with pattern 4 to match pattern 3. Since all of the rows with pattern 1 are year ranges spanning only two years (e.g. 1980-81), we can select a year and have it replace the range. To keep things simple, let\u2019s select the first year in the range since it contains all four digits of the year (1980) while the second year in the range has only the last two digits (81). We also need a reliable way to identify which rows actually exhibit pattern 1 so we only update those rows and leave the others intact. We need to leave the others intact either because they are already in the proper date format or because they will need to be modified later using the logic we write for handling the other patterns. Since year ranges contain a hyphen - separating the two years, we can look for the - in each row\u2019s Date value and split it into two separate years. The core Python library contains a function named .split() which in this situation will return a list of the two years if a hyphen is found or the original value if it isn\u2019t. Since we are looking for just the first year, we can call .split(\"-\") on every row\u2019s Date, check to see if the resulting list contains two elements, and if it does, return the first element. Let\u2019s write a function clean_split_dates(row) which will do exactly that: We ran .value_counts() on the Date column at the end to verify that all year ranges were removed from the DataFrame.  If you\u2019re interested in seeing how we handle the rest of the patterns, as well as learning how to work with datasets like the MoMA\u2019s, check out our Python for Business Analysts course at Dataquest. The course uses the Python language, and its toolkits to take common Excel tasks like filtering, editing and graphing, and simplify them. Each mission in the course will walk through different data challenges inherent in large datasets and discuss the best ways to solve them. Best of all - no installation required! The course is taught entirely in an interactive environment where you can write your own code and see the result immediately in the browser. Image Credit: The Son of Man by Studio Fibonacci from the Noun Project.",
    "link": "https://www.dataquest.io/blog/data-cleaning-with-python/",
    "title": "Data Cleaning with Python - MoMA's Artwork Collection"
}{
    "code": 1,
    "content": "  Machine learning is a field that uses algorithms to learn from data and make predictions.  Practically, this means that we can feed data into an algorithm, and use it to make predictions about what might happen in the future.  This has a vast range of applications, from self-driving cars to stock price prediction.  Not only is machine learning interesting, it\u2019s also starting to be widely used, making it an extremely practical skill to learn. In this tutorial, we\u2019ll guide you through the basic principles of machine learning, and how to get started with machine learning with Python.  Luckily for us, Python has an amazing ecosystem of libraries that make machine learning easy to get started with.  We\u2019ll be using the excellent Scikit-learn, Pandas, and Matplotlib libraries in this tutorial.   If you want to dive more deeply into machine learning, and apply algorithms in your browser, check out our courses here. Before we dive into machine learning, we\u2019re going to explore a dataset, and figure out what might be interesting to predict.  The dataset is from BoardGameGeek, and contains data on 80000 board games.  Here\u2019s a single boardgame on the site.  This information was kindly scraped into csv format by Sean Beck, and can be downloaded here.   The dataset contains several data points about each board game.  Here\u2019s a list of the interesting ones: The first step in our exploration is to read in the data and print some quick summary statistics.  In order to do this, we\u2019ll us the Pandas library.  Pandas provides data structures and data analysis tools that make manipulating data in Python much quicker and more effective.  The most common data structure is called a dataframe.  A dataframe is an extension of a matrix, so we\u2019ll talk about what a matrix is before coming back to dataframes. Our data file looks like this (we removed some columns to make it easier to look at): This is in a format called csv, or comma-separated values, which you can read more about here.  Each row of the data is a different board game, and different data points about each board game are separated by commas within the row.  The first row is the header row, and describes what each data point is.  The entire set of one data point, going down, is a column. We can easily conceptualize a csv file as a matrix: We removed some of the columns here for display purposes, but you can still get a sense of how the data looks visually.  A matrix is a two-dimensional data structure, with rows and columns.  We can access elements in a matrix by position.  The first row starts with id, the second row starts with 12333, and the third row starts with 120677.  The first column is id, the second is type, and so on.  Matrices in Python can be used via the NumPy library. A matrix has some downsides, though.  You can\u2019t easily access columns and rows by name, and each column has to have the same datatype.  This means that we can\u2019t effectively store our board game data in a matrix \u2013 the name column contains strings, and the yearpublished column contains integers, which means that we can\u2019t store them both in the same matrix. A dataframe, on the other hand, can have different datatypes in each column.  It has has a lot of built-in niceities for analyzing data as well, such as looking up columns by name.  Pandas gives us access to these features, and generally makes working with data much simpler. We\u2019ll now read in our data from a csv file into a Pandas dataframe, using the read_csv method. The code above read the data in, and shows us all of the column names.  The columns that are in the data but aren\u2019t listed above should be fairly self-explanatory. We can also see the shape of the data, which shows that it has 81312 rows, or games, and 20 columns, or data points describing each game. It could be interesting to predict the average score that a human would give to a new, unreleased, board game.  This is stored in the average_rating column, which is the average of all the user ratings for a board game.  Predicting this column could be useful to board game manufacturers who are thinking of what kind of game to make next, for instance. We can access a column is a dataframe with Pandas using games[\"average_rating\"].  This will extract a single column from the dataframe. Let\u2019s plot a histogram of this column so we can visualize the distribution of ratings.  We\u2019ll use Matplotlib to generate the visualization.  Matplotlib is the main plotting infrastructure in Python, and most other plotting libraries, like seaborn and ggplot2 are built on top of Matplotlib. We import Matplotlib\u2019s plotting functions with import matplotlib.pyplot as plt.  We can then draw and show plots.  What we see here is that there are quite a few games with a 0 rating.  There\u2019s a fairly normal distribution of ratings, with some right skew, and a mean rating around 6 (if you remove the zeros). Are there truly so many terrible games that were given a 0 rating?  Or is something else happening?  We\u2019ll need to dive into the data bit more to check on this. With Pandas, we can select subsets of data using Boolean series (vectors, or one column/row of data, are known as series in Pandas).  Here\u2019s an example: The code above will create a new dataframe, with only the rows in games where the value of the average_rating column equals 0. We can then index the resulting dataframe to get the values we want.  There are two ways to index in Pandas \u2013 we can index by the name of the row or column, or we can index by position.  Indexing by names looks like games[\"average_rating\"] \u2013 this will return the whole average_rating column of games.  Indexing by position looks like games.iloc[0] \u2013 this will return the whole first row of the dataframe.  We can also pass in multiple index values at once \u2013 games.iloc[0,0] will return the first column in the first row of games.  Read more about Pandas indexing here. This shows us that the main difference between a game with a 0 rating and a game with a rating above 0 is that the 0 rated game has no reviews.  The users_rated column is 0.  By filtering out any board games with 0 reviews, we can remove much of the noise. We just filtered out all of the rows without user reviews.  While we were at it, we also took out any rows with missing values.  Many machine learning algorithms can\u2019t work with missing values, so we need some way to deal with them.  Filtering them out is one common technique, but it means that we may potentially lose valuable data.  Other techniques for dealing with missing data are listed here. We\u2019ve seen that there may be distinct sets of games.  One set (which we just removed) was the set of games without reviews.  Another set could be a set of highly rated games.  One way to figure out more about these sets of games is a technique called clustering.  Clustering enables you to find patterns within your data easily by grouping similar rows (in this case, games), together. We\u2019ll use a particular type of clustering called k-means clustering.  Scikit-learn has an excellent implementation of k-means clustering that we can use.  Scikit-learn is the primary machine learning library in Python, and contains implementations of most common algorithms, including random forests, support vector machines, and logistic regression.  Scikit-learn has a consistent API for accessing these algorithms. In order to use the clustering algorithm in Scikit-learn, we\u2019ll first intialize it using two parameters \u2013 n_clusters defines how many clusters of games that we want, and random_state is a random seed we set in order to reproduce our results later.  Here\u2019s more information on the implementation. We then only get the numeric columns from our dataframe.  Most machine learning algorithms can\u2019t directly operate on text data, and can only take numbers as input.  Getting only the numeric columns removes type and name, which aren\u2019t usable by the clustering algorithm. Finally, we fit our kmeans model to our data, and get the cluster assignment labels for each row. Now that we have cluster labels, let\u2019s plot the clusters.  One sticking point is that our data has many columns \u2013 it\u2019s outside of the realm of human understanding and physics to be able to visualize things in more than 3 dimensions.  So we\u2019ll have to reduce the dimensionality of our data, without losing too much information.  One way to do this is a technique called principal component analysis, or PCA.  PCA takes multiple columns, and turns them into fewer columns while trying to preserve the unique information in each column.  To simplify, say we have two columns, total_owners, and total_traders.  There is some correlation between these two columns, and some overlapping information.  PCA will compress this information into one column with new numbers while trying not to lose any information. We\u2019ll try to turn our board game data into two dimensions, or columns, so we can easily plot it out.  We first initialize a PCA model from Scikit-learn.  PCA isn\u2019t a machine learning technique, but Scikit-learn also contains other models that are useful for performing machine learning.  Dimensionality reduction techniques like PCA are widely used when preprocessing data for machine learning algorithms. We then turn our data into 2 columns, and plot the columns.  When we plot the columns, we shade them according to their cluster assignment. The plot shows us that there are 5 distinct clusters.  We could dive more into which games are in each cluster to learn more about what factors cause games to be clustered. There are two things we need to determine before we jump into machine learning \u2013 how we\u2019re going to measure error, and what we\u2019re going to predict.  We thought earlier that average_rating might be good to predict on, and our exploration reinforces this notion.   There are a variety of ways to measure error (many are listed here).  Generally, when we\u2019re doing regression, and predicting continuous variables, we\u2019ll need a different error metric than when we\u2019re performing classification, and predicting discrete values. For this, we\u2019ll use mean squared error \u2013 it\u2019s easy to calculate, and simple to understand.  It shows us how far, on average, our predictions are from the actual values. Now that we want to predict average_rating, let\u2019s see what columns might be interesting for our prediction.  One way is to find the correlation between average_rating and each of the other columns.  This will show us which other columns might predict average_rating the best.   We can use the corr method on Pandas dataframes to easily find correlations.  This will give us the correlation between each column and each other column.  Since the result of this is a dataframe, we can index it and only get the correlations for the average_rating column. We see that the average_weight and id columns correlate best to rating.  ids are presumably assigned when the game is added to the database, so this likely indicates that games created later score higher in the ratings.  Maybe reviewers were not as nice in the early days of BoardGameGeek, or older games were of lower quality.  average_weight indicates the \u201cdepth\u201d or complexity of a game, so it may be that more complex games are reviewed better. Before we get started predicting, let\u2019s only select the columns that are relevant when training our algorithm.  We\u2019ll want to remove certain columns that aren\u2019t numeric.   We\u2019ll also want to remove columns that can only be computed if you already know the average rating.  Including these columns will destroy the purpose of the classifier, which is to predict the rating without any previous knowledge.  Using columns that can only be computed with knowledge of the target can lead to overfitting, where your model is good in a training set, but doesn\u2019t generalize well to future data. The bayes_average_rating column appears to be derived from average_rating in some way, so let\u2019s remove it. We want to be able to figure out how accurate an algorithm is using our error metrics.  However, evaluating the algorithm on the same data it has been trained on will lead to overfitting.  We want the algorithm to learn generalized rules to make predictions, not memorize how to make specific predictions.  An example is learning math.  If you memorize that 1+1=2, and 2+2=4, you\u2019ll be able to perfectly answer any questions about 1+1 and 2+2.  You\u2019ll have 0 error.  However, the second anyone asks you something outside of your training set where you know the answer, like 3+3, you won\u2019t be able to solve it.  On the other hand, if you\u2019re able to generalize and learn addition, you\u2019ll make occasional mistakes because you haven\u2019t memorized the solutions \u2013 maybe you\u2019ll get 3453 + 353535 off by one, but you\u2019ll be able to solve any addition problem thrown at you. If your error looks surprisingly low when you\u2019re training a machine learning algorithm, you should always check to see if you\u2019re overfitting. In order to prevent overfitting, we\u2019ll train our algorithm on a set consisting of 80% of the data, and test it on another set consisting of 20% of the data.  To do this, we first randomly samply 80% of the rows to be in the training set, then put everything else in the testing set. Above, we exploit the fact that every Pandas row has a unique index to select any row not in the training set to be in the testing set. Linear regression is a powerful and commonly used machine learning algorithm.  It predicts the target variable using linear combinations of the predictor variables.  Let\u2019s say we have a 2 values, 3, and 4.  A linear combination would be 3 * .5 + 4 * .5.  A linear combination involves multiplying each number by a constant, and adding the results.  You can read more here. Linear regression only works well when the predictor variables and the target variable are linearly correlated.  As we saw earlier, a few of the predictors are correlated with the target, so linear regression should work well for us. We can use the linear regression implementation in Scikit-learn, just as we used the k-means implementation earlier. When we fit the model, we pass in the predictor matrix, which consists of all the columns from the dataframe that we picked earlier.  If you pass a list to a Pandas dataframe when you index it, it will generate a new dataframe with all of the columns in the list.  We also pass in the target variable, which we want to make predictions for. The model learns the equation that maps the predictors to the target with minimal error. After we train the model, we can make predictions on new data with it.  This new data has to be in the exact same format as the training data, or the model won\u2019t make accurate predictions.  Our testing set is identical to the training set (except the rows contain different board games).  We select the same subset of columns from the test set, and then make predictions on it. Once we have the predictions, we\u2019re able to compute error between the test set predictions and the actual values.  Mean squared error has the formula $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^{2}$ .  Basically, we subtract each predicted value from the actual value, square the differences, and add them together.  Then we divide the result by the total number of predicted values.  This will give us the average error for each prediction. One of the nice things about Scikit-learn is that it enables us to try more powerful algorithms very easily.  One such algorithm is called random forest.  The random forest algorithm can find nonlinearities in data that a linear regression wouldn\u2019t be able to pick up on.  Say, for example, that if the minage of a game, is less than 5, the rating is low, if it\u2019s 5-10, it\u2019s high, and if it is between 10-15, it is low.  A linear regression algorithm wouldn\u2019t be able to pick up on this because there isn\u2019t a linear relationship between the predictor and the target.  Predictions made with a random forest usually have less error than predictions made by a linear regression. We\u2019ve managed to go from data in csv format to making predictions.  Here are some ideas for further exploration: At Dataquest, we offer interactive lessons on machine learning and data science.  We believe in learning by doing, and you\u2019ll learn interactively in your browser by analyzing real data and building projects.  Check out our machine learning lessons here.",
    "link": "https://www.dataquest.io/blog/machine-learning-python/",
    "title": "Machine learning with Python: A Tutorial"
}{
    "code": 0,
    "content": "  At Dataquest, we strive to help our users get a better sense of how data science works in industry as part of the data science educational process. We\u2019ve started a series where we interview experienced data scientists. We highlight their stories, advice they have for budding data scientists, and the kinds of problems they\u2019ve worked on. The first post in this series is our interview with Trey Causey. Trey Causey is a data scientist who has worked in fields ranging from ECommerce to Sports. After a brief stint in academia, he switched to industry by working as a data scientist at Zulily, an ECommerce startup in Seattle. He then had stints working on the data science teams at Facebook and Dato, a maker of tools for data scientists. In his ample free time, he blogs about data science on his personal website and has turned his hobby of football analytics into a consulting position for an NFL team. There\u2019s been a lot of media coverage around certain aspects like the size of data or around specific magical algorithms, but we\u2019d would love to know your thoughts. Trey:  Great question, and certainly one without an easy answer. Here\u2019s one attempt: data science is the use of statistics, machine learning, software engineering, and domain knowledge to a) extract information from data, b) make predictions using that information, and c) put those predictions to use in some applied setting. That\u2019s a wordy answer! And we can quibble about how to assign the weights to each of those components. Or, we could just link to Drew Conway\u2019s famous Data Science Venn Diagram. I don\u2019t think data science only involves big data or fancy algorithms like deep neural networks. But I do think that using statistics and machine learning are necessary components. Trey: Full disclosure, I have an uncompleted PhD from UW. I am what is known as ABD. I think people without solid training in some kind of quantitative discipline often get caught up in tooling and programming language debates like, should you learn R or Python? What package do I need to use random forests? And so on. Lacking some solid understanding of the algorithms you\u2019re applying can be dangerous, because you don\u2019t\u2019 know when things aren\u2019t working the way they\u2019re supposed to. I also see a lot of people making basic correlation/causation errors, whether they intend to or not, because they don\u2019t have the healthy skepticism of one\u2019s own models that often comes through years of education.  Trey: To fill in these gaps, we\u2019re obviously in an unprecedented period with respect to access to information and motivated auto-didacts have a wealth of options. That being said, I don\u2019t think there\u2019s a good substitute for taking a class that has a lot of focused homework and requires careful attendance and attention. I recommend everyone take at least one class in statistical inference \u2013 and I mean take a class, not just enroll in a MOOC. I\u2019m skeptical that the evaluation methods that enable thousands of people to take MOOCs are conducive to the kinds of work needed to really learn and retain complicated material. I\u2019m happy to be proven wrong, though! Trey: I transitioned from academia to industry partially out of necessity \u2013 I was in grad school when the job market for assistant professors imploded along with the rest of the economy \u2013 and partially because data science was starting to be a viable option for people like me who wanted to continue to do statistics and machine learning but didn\u2019t necessarily want to write papers that no one will read and that take years to get published. I just didn\u2019t have the patience for it. Trey: I found the transition to be pretty easy \u2013 I was fortunate enough to have a fantastic manager at Zulily, Mike Errecart, who really understood the potential for data science to improve the company, understood what was and wasn\u2019t possible, and was really interested in helping me grow as a data scientist. Trey: Obviously ecommerce and social media companies like zulily and Facebook have fully embraced data not only as a way to make better decisions but to improve and create new products using data. As for sports, I think that there\u2019s a lot of interesting new data becoming available that no one\u2019s using. Many in sports, especially the NFL, are conservative, favor \u201cgut-based\u201d decision-making, and haven\u2019t been convinced that data can help them win games without turning the game into a rote, robotic exercise. At Dataquest, we\u2019re pretty big skeptics of the data scientist unicorn motif and think that specialization in data professional roles will happen and is important. We already have Data Analyst and Data Scientist tracks but want to add more.  Trey: Good question \u2013 data engineers are extremely valuable to any organization using data, and I don\u2019t see a lot of training for those roles. I agree with many others that data engineers should often be hired before data scientists at startups in order to build scalable and robust infrastructure that lends itself to the kinds of work that data scientists are better at.  I don\u2019t know if there\u2019s much demand for it, or if online training is the answer, but I also see a real dearth of material on leading data science teams or working at the director or executive level as a data scientist. We\u2019re just now getting to the point where data scientists have risen to senior leadership positions in larger companies, but I don\u2019t know yet that we have a good idea what a VP of Data or a Director of Data Science looks like as a role. Trey: I always recommend networking as much as possible. Go to meetups, attend talks, and try and meet people that work in the kinds of roles you want to see yourself in. Cold-emailing people for coffee is a strategy that a lot of people recommend, but I get a lot of these and can tell you that it gets a little onerous and I end up feeling like a jerk because I just have to ignore them or say no most of the time.  Being visible is important \u2013 if that means side projects, that\u2019s great. It could also mean presenting your side project at a meetup or conference. Don\u2019t expect to just throw some code up on GitHub and to start watching the recruiter emails roll in.  Trey: Probably less than you might imagine. I\u2019ve always been a football fan and am just like a normal person \u2013 I get excited at big plays, yell about stupid mistakes and decisions, and generally get caught up in the drama of a good game. I still get very excited about football season. One small irony is that working on multiple football-related projects (both with an NFL team and helping to build the New York Times\u2019 Fourth Down Bot) often means that I am working during games and attending to how various models that I\u2019ve built are performing or if anything needs an emergency bug fix. Trey: I try not to get too wrapped up in hype about specific technologies or techniques. Spark has been interesting to watch, as there is a tremendous amount of excitement surrounding it, but I think that many people find it is somewhat difficult to wrangle when applied to real projects.  I experimented some with transfer learning while I was at Dato, using pre-trained deep neural networks on new data unrelated to the training data for things like image similarity. I was very surprised about how well this worked across multiple domains.  I think I\u2019m most excited about boring things like the continuing improvement of tools like Scikit-learn and Pandas such that they allow me to do my work while \u201cgetting out of the way\u201d while I\u2019m doing so. I\u2019ll take a well-designed and stable API over a flashy new tool nearly any day of the week. If you enjoyed this interview or want to learn more about data science, head over to Dataquest and join our community. We\u2019ll be interviewing data scientists on a regular basis and will be announcing new interviews through our newsletter. You can also join our community chat by requesting an invite here. Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/trey-causey-interview/",
    "title": "Trey Causey: Data Scientist Interview"
}{
    "code": 0,
    "content": "  Since we launched Dataquest a year ago, tens of thousands of people a month have learned data science and have started to experience the benefits in their respective careers. Dataquest continues to grow because of people like you who give us feedback and push us to improve the experience of learning data science.  We are announcing the Data Analyst and Data Scientist tracks to provide more organized learning paths and to help you better achieve your career goals. While you can still browse our regular Courses view from the navigation bar, Tracks make it easier to keep track of your progress. Our Data Analyst track is for people who feel bottlenecked by their existing data analysis tools and want to learn Python and Pandas to enhance their workflow. This track helps people transition from Business Analyst, Marketing Analyst, and Research Analyst roles into Data Analyst roles. Our Data Scientist track is for people who want to dive into more advanced data science techniques like Machine Learning and Computer Science and better prepare themselves for Data Scientist positions. We are huge believers in learning through doing and we are announcing Challenges and Projects to enable you to get more data science experience. Challenges are stand-alone exercises that let you test what you\u2019ve learned. While challenges are highly structured and guided, we provide feedback by evaluating your code in our code cells. If you\u2019ve heard enough and want to check out our first challenge, head over to Dataquest. If you get stuck on a challenge, we receommend posting in our forums or discussing in our Slack community. Projects are more open-ended and closely resemble the workflow that data analysts and data scientists in industry experience on a daily basis. Completing projects and building a portfolio is the best way to stand out on job applications, as you\u2019re able to demonstrate both your data science skills as well as your ability to step through the entire data science lifecycle. Since projects are much more open-ended than challenges, you will need to highly specific and personal feedback to help you better understand your strengths and weaknesses. Our first project is all about Star Wars and we encourage you to check it out the project page. Image Credit: Directions by Eliricon from the Noun Project.",
    "link": "https://www.dataquest.io/blog/projects-challenges-tracks/",
    "title": "Practice Makes Perfect. Announcing Tracks, Challenges, and Projects"
}{
    "code": 1,
    "content": "  Application Program Interfaces, or APIs, are commonly used to retrieve data from remote websites.  Sites like Reddit, Twitter, and Facebook all offer certain data through their APIs.  To use an API, you make a request to a remote web server, and retrieve the data you need. But why use an API instead of a static dataset you can download?  APIs are useful in the following cases: In cases like the ones above, an API is the right solution.  In this blog post, we\u2019ll be querying a simple API to retrieve data about the International Space Station (ISS).  Using an API will save us time and effort over doing all the computation ourselves.  \nThe International Space Station. APIs are hosted on web servers.  When you type www.google.com in your browser\u2019s address bar, your computer is actually asking the www.google.com server for a webpage, which it then returns to your browser. APIs work much the same way, except instead of your web browser asking for a webpage, your program asks for data.  This data is usually returned in JSON format (for more on this, checkout our tutorial on working with JSON data). In order to get the data, we make a request to a webserver.  The server then replies with our data.  In Python, we\u2019ll use the requests library to do this.  In this Python API tutorial we\u2019ll be using Python 3.4 for all of our examples. There are many different types of requests.  The most commonly used one, a GET request, is used to retrieve data. We can use a simple GET request to retrieve information from the OpenNotify API. OpenNotify has several API endpoints.  An endpoint is a server route that is used to retrieve different data from the API.  For example, the /comments endpoint on the Reddit API might retrieve information about comments, whereas the /users endpoint might retrieve data about users.  To access them, you would add the endpoint to the base url of the API. The first endpoint we\u2019ll look at on OpenNotify is the iss-now.json endpoint.  This endpoint gets the current latitude and longitude of the International Space Station.  As you can see, retrieving this data isn\u2019t a great fit for a dataset, because it involves some calculation on the server, and changes quickly. You can see a listing of all the endpoints on OpenNotify here. The base url for the OpenNotify API is http://api.open-notify.org, so we\u2019ll add this to the beginning of all of our endpoints. The request we just made had a status code of 200.  Status codes are returned with every request that is made to a web server.  Status codes indicate information about what happened with a request.  Here are some codes that are relevant to GET requests: We\u2019ll now make a GET request to http://api.open-notify.org/iss-pass, an endpoint that doesn\u2019t exist, per the API documentation. iss-pass wasn\u2019t a valid endpoint, so we got a 404 status code in response.  We forgot to add .json at the end, as the API documentation states. We\u2019ll now make a GET request to http://api.open-notify.org/iss-pass.json. You\u2019ll see that in the last example, we got a 400 status code, which indicates a bad request.  If you look at the documentation for the OpenNotify API, we see that the ISS Pass endpoint requires two parameters.   The ISS Pass endpoint returns when the ISS will next pass over a given location on earth.  In order to compute this, we need to pass the coordinates of the location to the API.  We do this by passing two parameters \u2013 latitude and longitude. We can do this by adding an optional keyword argument, params, to our request.  In this case, there are two parameters we need to pass: We can make a dictionary with these parameters, and then pass them into the requests.get function. We can also do the same thing directly by adding the query parameters to the url, like this: http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74. It\u2019s almost always preferable to setup the parameters as a dictionary, because requests takes care of some things that come up, like properly formatting the query parameters. We\u2019ll make a request using the coordinates of New York City, and see what response we get.  \nThe ISS over New York. You may have noticed that the content of the response earlier was a string (although it was shown as a bytes object, we can easily convert the content to a string using response.content.decode(\"utf-8\")). Strings are the way that we pass information back and forth to APIs, but it\u2019s hard to get the information we want out of them.  How do we know how to decode the string that we get back and work with it in Python?  How do we figure out the altitude of the ISS from the string response? Luckily, there\u2019s a format called JavaScript Object Notation (JSON).  JSON is a way to encode data structures like lists and dictionaries to strings that ensures that they are easily readable by machines.  JSON is the primary format in which data is passed back and forth to APIs, and most API servers will send their responses in JSON format. Python has great JSON support, with the json package.  The json package is part of the standard library, so we don\u2019t have to install anything to use it.  We can both convert lists and dictionaries to JSON, and convert strings to lists and dictionaries.  In the case of our ISS Pass data, it is a dictionary encoded to a string in JSON format. The json library has two main methods: You can get the content of a response as a python object by using the .json() method on the response. The server doesn\u2019t just send a status code and the data when it generates a response.  It also sends metadata containing information on how the data was generated and how to decode it.  This is stored in the response headers.  In Python, we can access this with the headers property of a response object. The headers will be shown as a dictionary.  Within the headers, content-type is the most important key for now.  It tells us the format of the response, and how to decode it.  For the OpenNotify API, the format is JSON, which is why we could decode it with the json package earlier. OpenNotify has one more API endpoint, astros.json.  It tells you how many people are currently in space.  The format of the responses can be found here. Now you\u2019ve completed our Python API tutorial, you now should be able to access a simple API and make get requests.  There are a few other types of requests, which you can learn more about, along with working with API authentication, in our dataquest APIs and scraping course. Other recommended next steps are reading the requests documentation, and working with the Reddit API.  There\u2019s a package called PRAW that makes working with the Reddit API easier in Python, but it\u2019s recommended to just use requests at first to learn how everything works. Image Credit: Web API by Jonathan Couti\u00f1o from the Noun Project.",
    "link": "https://www.dataquest.io/blog/python-api-tutorial/",
    "title": "Python API tutorial - An Introduction to using APIs"
}{
    "code": 1,
    "content": "  After lots of ground-breaking work led by the UC Berkeley AMP Lab, Apache Spark was developed to utilize distributed, in-memory data structures to improve data processing speeds over Hadoop for most workloads. In this post, we\u2019re going to cover the architecture of Spark and basic transformations and actions using a real dataset. If you want to write and run your own Spark code, check out the interactive version of this post on Dataquest. The core data structure in Spark is an RDD, or a resilient distributed dataset. As the name suggests, an RDD is Spark\u2019s representation of a dataset that is distributed across the RAM, or memory, of lots of machines. An RDD object is essentially a collection of elements that you can use to hold lists of tuples, dictionaries, lists, etc. Similar to DataFrames in Pandas, you load a dataset into an RDD and then can run any of the methods accesible to that object.  While Spark is writen in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows you to interface with RDD\u2019s in Python. Thanks to a library called Py4J, Python can interface with JVM objects, in our case RDD\u2019s, and this library one of the tools that makes PySpark work. To start off, we\u2019ll load the dataset containing all of the Daily Show guests into an RDD. We are using the TSV version of FiveThirtyEight\u2019s dataset. TSV files are separated, or delimited, by a tab character \"\\t\" instead of a comma \",\" like in a CSV file. SparkContext is the object that manages the connection to the clusters in Spark and coordinates running processes on the clusters themselves. SparkContext connects to cluster managers, which manage the actual executors that run the specific computations. Here\u2019s a diagram from the Spark documentation to better visualize the architecture:  The SparkContext object is usually referenced as the variable sc. We then run: to read the TSV dataset into an RDD object raw_data. The RDD object raw_data closely resembles a List of String objects, one object for each line in the dataset. We then use the take() method to print the first 5 elements of the RDD: To explore the other methods an RDD object has access to, check out the PySpark documentation. take(n) will return the first n elements of the RDD. One question you may have is if an RDD resembles a Python List, why not just use bracket notation to access elements in the RDD? Because RDD objects are distributed across lots of partitions, we can\u2019t rely on the standard implementation of a List and the RDD object was developed to specifically handle the distributed nature of the data. One advantage of the RDD abstraction is the ability to run Spark locally on your own computer. When running locally on your own computer, Spark simulates distributing your calculations over lots of machines by slicing your computer\u2019s memory into partitions, with no tweaking or changes to the code you wrote. Another advantage of Spark\u2019s RDD implementation is the ability to lazily evaluate code, postponing running a calculation until absolutely necessary. In the code above, Spark didn\u2019t wait to load the TSV file into an RDD until raw_data.take(5) was run. When raw_data = sc.textFile(\"dail_show.tsv\") was called, a pointer to the file was created, but only when raw_data.take(5) needed the file to run its logic was the text file actually read into raw_data. We will see more examples of this lazy evaluation in this lesson and in future lessons. Spark borrowed heavily from Hadoop\u2019s Map-Reduce pattern, but is quite different in many ways. If you have experience with Hadoop and traditional Map-Reduce, read this great post by Cloudera on the difference. Don\u2019t worry if you have never worked with Map-Reduce or Hadoop before as we\u2019ll cover the concepts you need to know in this course. The key idea to understand when working with Spark is data pipelining. Every operation or calculation in Spark is essentially a series of steps that can be chained together and run in succession to form a pipeline. Each step in the pipeline returns either a Python value (e.g. Integer), a Python data structure (e.g. Dictionary) or an RDD object. We\u2019ll first start with the map() function. Map() The map(f) function applies the function f to every element in the RDD. Since RDD\u2019s are iterable objects, like most Python objects, Spark runs function f on every iteration and returns a new RDD. We\u2019ll walk through a map example so you can get a better sense. If you look carefully, raw_data is in a format that\u2019s hard to work with. While currently each element is a String, we\u2019d like to transform every element into a List so the data is more managable. While traditionally, we would:  let\u2019s walk through how we use map to do this in Spark. In the below code block, we: The map(f) function is known as a transformation step and either a named or lambda function f is required. One of the wonderful features of PySpark is the ability to separate our logic, which we prefer to write in Python, from the actual data transformation. In the above code block, we wrote a lambda function in Python code: but got to take advantage of Scala when Spark actually ran the code over our RDD. This is the power of PySpark. Without learning any Scala, we get to harness the data processing performance gains from Spark\u2019s Scala architecture. Even better, when we ran: the results were returned to us in Python friendly notation. Transformations and Actions In Spark, there are two types of methods: Transformations are lazy operations and always return a reference to an RDD object. The transformation, however, is not actually run until an action needs to use the resulting RDD from a transformation. Any function that returns an RDD is a transformation and any function that returns a value is an action. These concepts will become more clear as you work through this lesson and practice writing PySpark code. Immutability You may be wondering why we couldn\u2019t just split each String in place instead of creating a new object daily_show? In Python, we could have modified the collection element-by-element in place without returning and assignign to a new object. RDD objects are immutable and their values can\u2019t be changed once the object is created. In Python, List objects and Dictionary objects are mutable, which means we can change the object\u2019s values, while Tuple objects are immutable. The only way to modify a Tuple object in Python is to create a new Tuple object with the necessary updates. Spark utilizes immutability of RDD\u2019s for speed gains and the mechanics of that are outside the scope of this lesson. We would like to get a histogram, or a tally, of the number of guests in each year the show has been running. If daily_show were a List of Lists, we could write the following Python code to achieve this result: The keys in tally will be unique Year values and the values will be the number of lines in the dataset that contained that value.  If we want to achieve this same result using Spark, we will have to use a Map step followed by a ReduceByKey step. You may notice that printing tally didn\u2019t return the histogram we were hoping for. Because of lazy evaluation, PySpark delayed executing the map and reduceByKey steps until we actually need it. Before we use take() to preview the first few elements in tally, we\u2019ll walk through the code we just wrote. During the map step, we used a lambda function to create a tuple consisting of: Our high level strategy was to create a tuple with the key representing the Year and the value representing 1. After the map step, Spark will maintain in memory a list of tuples resembling the following: and we\u2019d like to reduce that down to: reduceByKey(f) combines tuples with the same key using the function we specify f. To see the results of these 2 steps, we\u2019ll use the take command, which forces lazy code to run immediately. Since tally is an RDD, we can\u2019t use Python\u2019s len function to know how many elements are in the collection and will instead need to use the RDD count() function. Unlike Pandas, Spark knows nothing about column headers and didn\u2019t set them aside. We need a way to get rid of the element: from our collection. While you may be tempted to try to find a way to remove this element from the RDD, recall that RDD objects are immutable and can\u2019t be changed once created. The only way to remove that tuple is to create a new RDD object without that tuple. Spark comes with a function filter(f) that allows us to create a new RDD from an existing one containing only the elements meeting our criteria. Specify a function f that returns a binary value, True or False, and the resulting RDD will consist of elements where the function evaluated to True. Read more about the filter function over at Spark\u2019s documentation. To flex Spark\u2019s muscles, we\u2019ll demonstrate how to chain together a series of data transformations into a pipeline and observe Spark managing everything in the background. Spark was written with this functionality in mind and is highly optimized for running tasks in succession. Previously, running lots of tasks in succession in Hadoop was incredibly time consuming since intermediate results needed to be written to disk and Hadoop wasn\u2019t aware of the full pipeline (optional reading if you\u2019re curious: http://qr.ae/RHWrT2).  Thanks to Spark\u2019s aggressive usage of memory (and only disk as a backup and for specific tasks) and well architected core, Spark is able to improve significantly on Hadoop\u2019s turnaround time. In the following code block, we\u2019ll filter out actors with no profession listed, lowercase each profession, generate a histogram of professions, and output the first 5 tuples in the histogram. We hope that in this lesson, we have whet your appetite for Apache Spark and how we can use PySpark to write Python code we\u2019re familiar with but still take advantage of distributed processing. When working with larger datasets, PySpark really shines since it blurs the line between doing data science locally on your own computer and doing data science using large amounts of distributed computing on the internet (also referred to as the cloud). If you enjoyed this post, check out part 2 on Dataquest where we  learn more about transformations & actions in Spark.",
    "link": "https://www.dataquest.io/blog/apache-spark/",
    "title": "Apache Spark: An Introduction"
}{
    "code": 1,
    "content": "  There have been dozens of articles written comparing Python and R from a subjective standpoint.  We\u2019ll add our own views at some point, but this article aims to look at the languages more objectively.  We\u2019ll analyze a dataset side by side in Python and R, and show what code is needed in both languages to achieve the same result.  This will let us understand the strengths and weaknesses of each language without the conjecture.  At Dataquest, we teach both languages, and think both have a place in a data science toolkit. We\u2019ll be analyzing a dataset of NBA players and their performance in the 2013-2014 season.  You can download the file here. For each step in the analysis, we\u2019ll show the Python and R code, along with some explanation and discussion of the different approaches.  Without further ado, let\u2019s get this head to head Python vs R matchup started! The above code will load the csv file nba_2013.csv, which contains data on NBA players from the 2013-2014 season, into the variable nba in both languages.  The only real difference is that in Python, we need to import the pandas library to get access to Dataframes.  Dataframes are available in both R and Python, and are two-dimensional arrays (matrices) where each column can be of a different datatype.  At the end of this step, the csv file has been loaded by both languages into a dataframe. This prints out the number of players and the number of columns in each.  We have 481 rows, or players, and 31 columns containing data on the players. This is pretty much identical.  Both print out the first row of the data, and the syntax is very similar.  Python is more object-oriented here, and head is a method on the dataframe object, and R has a separate head function.  This is a common theme you\u2019ll see as you start to do analysis with these languages, where Python is more object-oriented, and R is more functional. Let\u2019s find the average value for each statistic.  The columns, as you can see, have names like fg (field goals made), and ast (assists).  These are the season statistics for the player.  If you want a fuller explanation of all the stats, look here. There are some major differences in approach here.  In both, we\u2019re applying a function across the dataframe columns.  In python, the mean method on dataframes will find the mean of each column by default. In R, taking the mean of string values will just result in NA \u2013 not available.  However, we do need to ignore NA values when we take the mean (requiring us to pass na.rm=TRUE into the mean function).  If we don\u2019t, we end up with NA for the mean of columns like x3p..  This column is three point percentage.  Some players didn\u2019t take three point shots, so their percentage is missing.  If we try the mean function in R, we get NA as a response, unless we specify na.rm=TRUE, which ignores NA values when taking the mean.  The .mean() method in Python already ignores these values by default. One common way to explore a dataset is to see how different columns correlate to others.  We\u2019ll compare the ast, fg, and trb columns. We get very similar plots in the end, but this shows how the R data science ecosystem has many smaller packages (GGally is a helper package for ggplot2, the most-used R plotting package), and many more visualization packages in general.  In Python, matplotlib is the primary plotting package, and seaborn is a widely used layer over matplotlib.  With visualization in Python, there is usually one main way to do something, whereas in R, there are many packages supporting different methods of doing things (there are at least a half dozen packages to make pair plots, for instance). One good way to explore this kind of data is to generate cluster plots.  These will show which players are most similar. In order to cluster properly, we remove any non-numeric columns, or columns with missing values (NA, Nan, etc).  In R, we do this by applying a function across each column, and removing it if it has any missing values or isn\u2019t numeric.  We then use the cluster package to perform k-means and find 5 clusters in our data.  We set a random seed using set.seed to be able to reproduce our results. In Python, we use the main Python machine learning package, scikit-learn, to fit a k-means clustering model and get our cluster labels.  We perform very similar methods to prepare the data that we used in R, except we use the get_numeric_data and dropna methods to remove non-numeric columns and columns with missing values. We can now plot out the players by cluster to discover patterns.  One way to do this is to first use PCA to make our data 2-dimensional, then plot it, and shade each point according to cluster association. Made a scatter plot of our data, and shaded or changed the icon of the data according to cluster.  In R, the clusplot function was used, which is part of the cluster library.  We performed PCA via the pccomp function that is builtin to R. With Python, we used the PCA class in the scikit-learn library.  We used matplotlib to create the plot. If we want to do supervised machine learning, it\u2019s a good idea to split the data into training and testing sets so we don\u2019t overfit. You\u2019ll notice that R has many more data-analysis focused builtins, like floor, sample, and set.seed, whereas these are called via packages in Python (math.floor, random.sample, random.seed).  In Python, the recent version of pandas came with a sample method that returns a certain proportion of rows randomly sampled from a source dataframe \u2013 this makes the code much more concise.  In R, there are packages to make sampling simpler, but aren\u2019t much more concise than using the built-in sample function.  In both cases, we set a random seed to make the results reproducible. Let\u2019s say we want to predict number of assists per player from field goals made per player. Scikit-learn has a linear regression model that we can fit and generate predictions from.  R relies on the built-in lm and predict functions.  predict will behave differently depending on the kind of fitted model that is passed into it \u2013 it can be used with a variety of fitted models. If we want to get summary statistics about the fit, like r-squared value, we\u2019ll need to do a bit more in Python than in R.  With R, we can use the builtin summary function to get information on the model.  With Python, we need to use the statsmodels package, which enables many statistical methods to be used in Python.  We get similar results, although generally it\u2019s a bit harder to do statistical analysis in Python, and some statistical methods that exist in R don\u2019t exist in Python. Our linear regression worked well in the single variable case, but we suspect there may be nonlinearities in the data.  Thus, we want to fit a random forest model. The main difference here is that we needed to use the randomForest library in R to use the algorithm, whereas it was built in to scikit-learn in Python.  scikit-learn has a unified interface for working with many different machine learning algorithms in Python, and there\u2019s usually only one main implementation of each algorithm in Python.  With R, there are many smaller packages containing individual algorithms, often with inconsistent ways to access them.  This results in a greater diversity of algorithms (many have several implementations, and many are fresh out of research labs), but with a bit of a usability hit. Now that we\u2019ve fit two models, let\u2019s calculate error.  We\u2019ll use MSE. In Python, the scikit-learn library has a variety of error metrics that we can use.  In R, there are likely some smaller libraries that calculate MSE, but doing it manually is pretty easy in either language.  There\u2019s a small difference in errors that almost certainly due to parameter tuning, and isn\u2019t a big deal. Now that we have data on NBA players from 2013-2014, let\u2019s scrape some additional data to supplement it.  We\u2019ll just look at one box score from the NBA Finals here to save time. In Python, the requests package makes downloading web pages easy, with a consistent API for all request types.  In R, RCurl provides a similarly simple way to make requests.  Both download the webpage to a character datatype.  Note:  this step is unnecessary for the next step in R, but is shown for comparisons\u2019s sake. Now that we have the web page, we\u2019ll need to parse it to extract scores for players. This will create a list containing two lists, the first with the box score for CLE, and the second with the box score for GSW.  Both contain the headers, along with each player and their in-game stats.  We won\u2019t turn this into more training data now, but it could easily be transformed into a format that could be added to our nba dataframe. The R code is more complex than the Python code, because there isn\u2019t a convenient way to use regular expressions to select items, so we have to do additional parsing to get the team names from the HTML.  R also discourages using for loops in favor of applying functions along vectors.  We use lapply to do this, but since we need to treat each row different depending on whether it\u2019s a header or not, we pass the index of the item we want, and the entire rows list into the function. We use rvest, a new and widely used R web scraping package to extract the data we need.  Note that we can pass a url directly into rvest, so the last step wasn\u2019t needed in R. In Python, we use BeautifulSoup, the most commonly used web scraping package.  It enables us to loop through the tags and construct a list of lists in a straightforward way. We\u2019ve taken a look at how to analyze a dataset with R and Python.  There are many tasks we didn\u2019t dive into, such as persisting the results of our analysis, sharing the results with others, testing and making things production-ready, and making more visualizations.  We\u2019ll dive into these at a later date, which will let us make some more definitive conclusions.  For now, here\u2019s what we can say: As we saw from functions like lm, predict, and others, R lets functions do most of the work.  Contrast this to the LinearRegression class in Python, and the sample method on dataframes. When we looked at summary statistics, we could use the summary built-in function in R, but had to import the statsmodels package in Python.  The dataframe is a built-in construct in R, but must be imported via the pandas package in Python. With Python, we can do linear regression, random forests, and more with the scikit-learn package.  It offers a consistent API, and is well-maintained.  In R, we have a greater diversity of packages, but also greater fragmentation and less consistency (linear regression is a builtin, lm, randomForest is a separate package, etc). R was built as a statistical language, and it shows.  statsmodels in Python and other packages provide decent coverage for statistical methods, but the R ecosystem is far larger. With well-maintained libraries like BeautifulSoup and requests, web scraping in Python is far easier than in R.  This applies to other tasks that we didn\u2019t look into closely, like saving to databases, deploying web servers, or running complex workflows. There are clear points of inspiration between both R and Python (pandas Dataframes were inspired by R dataframes, the rvest package was inspired by BeautifulSoup), and both ecosystems continue to grow stronger.  It\u2019s remarkable how similar the syntax and approaches are for many common tasks in both languages. At Dataquest, we primarily teach Python, but have recently been adding lessons on R.  We see both languages as complementary, and although we think Python is stronger in more areas, R is an effective language.  It can be used either as a complement for Python in areas like data exploration and statistics, or as your sole data analysis tool.  As this walkthrough proves, both languages have a lot of similarities in syntax and approach, and you can\u2019t go wrong with one, the other, or both.",
    "link": "https://www.dataquest.io/blog/python-vs-r/",
    "title": "Python vs R: head to head data analysis"
}{
    "code": 1,
    "content": "  In this post, we\u2019ll be using the K-nearest neighbors algorithm to predict how many points NBA players scored in the 2013-2014 season.  Along the way, we\u2019ll learn about euclidean distance and figure out which NBA players are the most similar to Lebron James.  If you want to follow along, you can grab the dataset in csv format here. Before we dive into the algorithm, let\u2019s take a look at our data.  Each row in the data contains information on how a player performed in the 2013-2014 NBA season. Here are some selected columns from the data: There are many more columns in the data, mostly containing information about average player game performance over the course of the season.  See this site for an explanation of the rest of them. We can read our dataset in and figure out which columns are present: The k-nearest neighbors algorithm is based around the simple idea of predicting unknown values by matching them with the most similar known values. Let\u2019s say that we have 3 different types of cars.  We know the name of the car, its horsepower, whether or not it has racing stripes, and whether or not it\u2019s fast.: Let\u2019s say that we now have another car, but we don\u2019t know how fast it is: We want to figure out if the car is fast or not.  In order to predict if it is with k nearest neighbors, we first find the most similar known car.  In this case, we would compare the horsepower and racing_stripes values to find the most similar car, which is the Yugo.  Since the Yugo is fast, we would predict that the Camaro is also fast.  This is an example of 1-nearest neighbors \u2013 we only looked at the most similar car, giving us a k of 1. If we performed a 2-nearest neighbors, we would end up with 2 True values (for the Delorean and the Yugo), which would average out to True.  The Delorean and Yugo are the two most similar cars, giving us a k of 2. If we did 3-nearest neighbors, we would end up with 2 True values and a False value, which would average out to True. The number of neighbors we use for k-nearest neighbors (k) can be any value less than the number of rows in our dataset.  In practice, looking at only a few neighbors makes the algorithm perform better, because the less similar the neighbors are to our data, the worse the prediction will be. Before we can predict using KNN, we need to find some way to figure out which data rows are \u201cclosest\u201d to the row we\u2019re trying to predict on. A simple way to do this is to use Euclidean distance.  The formula is \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} Let\u2019s say we have these two rows (True/False has been converted to 1/0), and we want to find the distance between them: We would first only select the numeric columns.  Then the distance becomes \\sqrt{(180-400)^2 + (0-1)^2}, which is about equal to 220. We can use the principle of euclidean distance to find the most similar NBA players to Lebron James. You may have noticed that horsepower in the cars example had a much larger impact on the final distance than racing_stripes did.  This is because horsepower values are much larger in absolute terms, and thus dwarf the impact of racing_stripes values in the euclidean distance calculations. This can be bad, because a variable having larger values doesn\u2019t necessarily make it better at predicting what rows are similar. A simple way to deal with this is to normalize all the columns to have a mean of 0, and a standard deviation of 1.  This will ensure that no single column has a dominant impact on the euclidean distance calculations. To set the mean to 0, we have to find the mean of a column, then subtract the mean from every value in the column.  To set the standard deviation to 1, we divide every value in the column by the standard deviation.  The formula is x=\\frac{x-\\mu}{\\sigma}. We now know enough to find the nearest neighbor of a given row in the NBA dataset.  We can use the distance.euclidean function from scipy.spatial, a much faster way to calculate euclidean distance. Now that we know how to find the nearest neighbors, we can make predictions on a test set.  We\u2019ll try to predict how many points a player scored using the 5 closest neighbors.  We\u2019ll find neighbors by using all the numeric columns in the dataset to generate similarity scores. First, we have to generate test and train sets.  In order to do this, we\u2019ll use random sampling.  We\u2019ll randomly shuffle the index of the nba dataframe, and then pick rows using the randomly shuffled values. If we didn\u2019t do this, we\u2019d end up predicting and training on the same data set, which would overfit.  We could do cross validation also, which would be slightly better, but slightly more complex. Instead of having to do it all ourselves, we can use the k-nearest neighbors implementation in scikit-learn.  Here\u2019s the documentation.  There\u2019s a regressor and a classifier available, but we\u2019ll be using the regressor, as we have continuous values to predict on. Sklearn performs the normalization and distance finding automatically, and lets us specify how many neighbors we want to look at. Now that we know our point predictions, we can compute the error involved with our predictions.  We can compute mean squared error.  The formula is \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_{i}} - y_{i})^{2}.",
    "link": "https://www.dataquest.io/blog/k-nearest-neighbors-in-python/",
    "title": "K nearest neighbors in python: A tutorial"
}{
    "code": 1,
    "content": "  Python has some powerful tools that enable you to do natural language processing (NLP).  In this tutorial, we\u2019ll learn about how to do some basic NLP in python. We\u2019ll be looking at a dataset consisting of submissions to Hacker News from 2006 to 2015.  The data was taken from here.  Arnaud Drizard used the Hacker News API to scrape it.  We\u2019ve sampled 10000 rows from the data randomly, and removed all the extraneous columns.  Our data only has four columns: We\u2019ll be using the headlines to predict the number of upvotes.  The data is stored in the submissions variable. We want to eventually train a machine learning algorithm to take in a headline and tell us how many upvotes it would receive.  However, machine learning algorithms only understand numbers, not words.  How do we translate our headlines into something an algorithm can understand? The first step is to create something called a bag of words matrix.  A bag of word matrix gives us a numerical representation of which words are in which headlines. In order to construct a bag of words matrix, we first find the unique words across the whole set of headlines.  Then, we setup a matrix where each row is a headline, and each column is one of the unique words.  Then, we fill in each cell with the number of times that word occured in that headline. This will result in a matrix where a lot of the cells have a value of zero, unless the vocabulary is mostly shared between the headlines. The matrix we just made is very sparse \u2013 that means that a lot of the values are zero.  This is unavoidable to some extent, because the headlines don\u2019t have much shared vocabulary. We can take some steps to make the problem better, though.  Right now Why and why, and use and use. are treated as different entities, but we know they refer to the same word. We can help the parser recognize that these are in fact the same by lowercasing every word and removing all punctuation. Certain words don\u2019t help you discriminate between good and bad headlines.  Words such as the, a, and also occur commonly enough in all contexts that they don\u2019t really tell us much about whether something is good or not.  They are generally equally likely to appear in both good and bad headlines. By removing these, we can reduce the size of the matrix, and make training an algorithm faster. Now that we know the basics, we can make a bag of words matrix for the whole set of headlines.   We don\u2019t want to have to code everything out manually every time, so we\u2019ll use a class from scikit-learn to do it automatically.  Using the vectorizers from scikit-learn to construct your bag of words matrices will make the process much easier and faster. We\u2019ve constructed a matrix, but it now has 13631 unique words, or columns.  This will take a very long time to make predictions with.  We want to speed it up, so we\u2019ll need to cut down the column count somehow. One way to do this is to pick a subset of the columns that are the most informative \u2013 that is, the columns that differentiate between good and bad headlines the best.  A good way to figure out the most informative columns is to use something called a chi-squared test.   A chi-squared test finds the words that discriminate the most between highly upvoted posts and posts that weren\u2019t upvoted.  This can be words that occur a lot in highly upvoted posts, and not at all in posts without upvotes, or words that occur a lot in posts that aren\u2019t upvoted, but don\u2019t occur in posts that are upvoted. A chi-squared test only works on binary values, so we\u2019ll make our upvotes column binary by setting anything with more upvotes than average to 1 and anything with less upvotes than average to 0. One downside of this is that we are using knowledge from the dataset to select features, and thus introducing some overfitting.  We could get around the overfitting in the \u201creal world\u201d by using a subset of the data for feature selection, and using a different subset for training the algorithm.  We\u2019ll make things a bit simpler for now and skip that step. If we ignore the \u201cmeta\u201d features of the headlines we\u2019re missing out on a lot of good information.  These features are things like length, amount of punctuation, average word length, and other sentence specific features. Adding these in can greatly increase prediction accuracy. To add them in, we\u2019ll loop over our headlines, and apply a function to each one.  Some functions will count the length of the headline in characters, and others will do more advanced things, like counting the number of digits. There are more features we can work with than just text features.  We have a column called submission_time, that tells us when a story was submitted, and could add more information.   Often when doing NLP work, you\u2019ll be able to add outside features that make your predictions much better.  Some machine learning algorithms can figure out how these features interact with your textual features(ie \u201cPosting at midnight with the word \u2018tacos\u2019 in the headline results in a high scoring post\u201d). Now that we can translate words to numbers, we can make predictions using an algorithm.  We\u2019ll randomly pick 7500 headlines as a training set, and then evaluate the performance of the algorithm on the test set of 2500 headlines. Predicting the results on the same set that we train on will result in overfitting, where your algorithm is overly optimized to the training set \u2013 we\u2019ll think that the error rate is good, but it could actually be much higher on new data. For the algorithm, we\u2019ll use ridge regression.  As compared to ordinary linear regression, ridge regression introduces a penalty on the coefficients, which prevents them from becoming too large.  This can help it work with large numbers of predictors (columns) that are correlated to each other, like we have. We now have predictions, but how do we determine how good they are?  One way is to calculate the error rate between the predictions on the test set and the actual upvote counts for the test set. We\u2019ll also want a baseline to compare the error to to see if the results are good.  We can do this by using a simple method to make baseline estimates for the test set, and comparing the error rate of our predictions to the error rate of the baseline estimates.  One very simple baseline is to take the average number of upvotes per submission in the training set, and use that as a prediction for every submission. We\u2019ll use mean absolute error as an error metric.  It\u2019s very simple \u2013 just subtract the actual value from the prediction, take the absolute value of the difference, then find the mean of all the differences. This method worked reasonably but not stunningly well on this dataset.  We found that the headlines and other columns have some predictive value. We could improve this approach by using a different predictive algorithm, like a random forest or a neural network.  We could also use ngrams, such as bigrams and trigrams, when we are generating our bag of words matrix.  Trying a tf-idf transform on the matrix could also help \u2013 scikit-learn has a class that does this automatically. We could also take other data into account, like the user who submitted the article, and generate features indicating things like the karma of the user, and the recent activity of the user.  Other statistics on the submitted url, like the average number of upvotes submissions from that url received would also be potentially useful.  Be careful when doing these to only take into account information that existed before the submission you\u2019re predicting for was made. All of these additions will take much longer to run than what we have so far, but will reduce error.  Hopefully you\u2019ll have some time to try them out! Image Credit: Up by The Impekables  from the Noun Project.",
    "link": "https://www.dataquest.io/blog/natural-language-processing-with-python/",
    "title": "Natural Language Processing with Python Tutorial"
}{
    "code": 1,
    "content": "  At Dataquest, we strive to help our users get a better sense of how data science works in industry as part of the data science educational process. We\u2019ve started a series where we interview experienced data scientists. We highlight their stories, advice they have for budding data scientists, and the kinds of problems they\u2019ve worked on. This is our second post in this series and is an interview with data scientist and engineer Benjamin Root. Benjamin Root is a contributor to the Matplotlib data visualization library and focuses on improving documentation as well as the mplot3d toolkit within Matplotlib. Ben was previously a graduate student in Metereology where his experiences trying to work with data using MATLAB pushed him to learn Python and become further involved in the SciPy community. Benjamin: I started using Python back in early 2009 when a couple of colleagues took note of some problems I was having with MATLAB and suggested that I give Python a try. With some example pylab scripts to learn from, I started to convert some of my .m files into .py files. I had some pretty sophisticated MATLAB code, so my colleagues pointed me to the various mailing lists for NumPy, SciPy, and Matplotlib. With the feedback from those lists, I managed to convert all of my MATLAB code into Python, declaring that I would \u201cnever again\u201d use MATLAB. Well, it came time to do new development, and new visualizations, and I kept running into all sorts of nagging bugs and edge cases in NumPy and Matplotlib. I would report the bugs with example code and the developers would usually fix the bug. Eventually, the developers would start nudging me to where the problem existed and I started writing patches on my own. After about a year of submitting patches, John Hunter gave me commit rights. He later remarked that he gives out commit rights to anybody that \u201cannoys\u201d him enough. So, getting involved in Matplotlib came about due to annoying my colleagues, and then annoying the developers\u2026 Benjamin: Originally, my focus in Matplotlib was with documentation because I had \u201cfresh eyes\u201d, so it was easier for me to notice mistakes and rough edges. Now that I have read the documentation so many times, I don\u2019t even see the mistakes anymore. By the way, this is why developers encourage newcomers to submit patches to the documentation!  I was also trying to sand down the rough edges I kept encountering, particularly with the mplot3d toolkit packaged with Matplotlib. I submitted enough patches to mplot3d that I became its defacto maintainer. Currently, my focus has been on doing reviews of pull requests on GitHub, along with diagnosing/verifying bug reports.  I do still submit new features from time to time (in particular, look for the new \u201cproperty cycling\u201d and the \u201cclassic style\u201d features in the upcoming v1.5 release).  I participate in design discussions, trying to make sure that we make good design decisions early on for new features. I will also soon be taking on the role of maintainer of the Basemap toolkit. On the mailing list, I tend to be the one who responds to most of the questions from the newcomers, although the mailing list traffic has decreased substantially since the arrival of StackOverflow. Benjamin: Those sorts of users are the best for libraries like Matplotlib. They are often the ones that are working at the boundaries of the library\u2019s capabilities. So, it is very natural for those users to encounter bugs and limitations. File those bug reports! Ask questions on the mailing list, and give feedback and criticisms! Developers do want to hear back from their users because it means their work is being used! Involvement in open source cannot be counted by the number of patches. Open source development is so much more than just code and documentation. Software does not spontaneously \u201ccome about\u201d from the ether. Software fulfills a need and it is the users who defines that need. I personally believe that the most valuable contributors to an open source project are the ones who gives feedback and criticisms to the developers. Without them, the project stagnates and dies from apathy. So, my challenge to your readers is to figure out which three libraries and tools they directly use  the most and subscribe to their user mailing list. Then, the next time \u201csomething weird\u201d or \u201cunexpected\u201d happens when using one of those tools, don\u2019t brush it off as your own fault, or a limitation that you have to put up with. Send an email to that list asking if anybody else thinks it is weird or unexpected. Press for answers. Encourage discussions. Finally, if a developer asks for feedback on something, take them up on their request. And please, for the love of all things good, please try out your software on the beta and release candidates of those libraries and report problems! Benjamin: Mostly, I wanted to improve my own knowledge in this part of Matplotlib, while also producing better documentation for it. Interactivity is such an important feature of Matplotlib, but it is documented so poorly. All examples and tutorials that I could find online only demonstrated various aspects, and they were so disjointed from each other. There was no narrative that would help a user build their understanding from a strong foundation. In the book, rather than making all of the example code completely independent of each other, we build a single application piece-by-piece. One thing I really like about this narrative approach was a particular moment of satisfaction I had in the widgets chapter where we add a slider and then I point out to the reader that the new slider was automatically tied into the keyboard shortcuts and some other buttons that we did in an earlier chapter. That just simply would not have been possible in your typical example-driven documentation where the demonstration for the keyboard shortcuts would have been completely unrelated to the demonstration for a widget. My target audience for the book is for those who have some experience with Python and object oriented programming and has a need to produce interactive visualizations in Python. You don\u2019t need any prior experience with Matplotlib or any other Scientific Python tools, but I also do not spend much time explaining the possible visualizations and how to customize them. There are plenty of tutorials about that. I assume that you already can display a plot the way you want, but need to build tools that interacts with that plot. Benjamin: Meteorology, like many other sciences, is very much a visual experience. Viewing data as text is a terrible way to understand your data. So, a graduate student must be able to create their own visualizations of their work. There are several tools for that, some are very specific to the atmospheric sciences like NCL and GEMPAK, while others are more general such as the MATLAB language. As a graduate student, I was constantly trying new things that aren\u2019t established, and I am using tools in ways that weren\u2019t thought of before. Therefore, I kept running into bugs in those tools. What really pushed me over to Python from the MATLAB world was the ability to fix those bugs. In addition, there are some features that are now in Matplotlib that were fostered from a meteorology perspective. For example, I often need to visualize my data with very accurate maps, so I have taken a particular interest in projects such as Basemap and Cartopy, making sure they satisfy my needs. Streamplots and wind barbs are a couple other features that we have cultivated in Matplotlib for meteorologists. Benjamin: Vispy is an amazing project that shows great promise. One of its original components, glumpy, was actually created at my encouragement due to the limitations of the mplot3d toolkit shipped with Matplotlib. Several of the original components of Vispy were created at the behest of Matplotlib developers because we saw the value of performing visualizations on the GPU, but we did not have enough expertise, nor did we have the bandwidth to pursue such a project while also maintaining Matplotlib. We also did not feel that we could add such experimental features into Matplotlib without causing significant disruption to our users. So, we encouraged some developers who were coming to the mailing list with these fantastic ideas to start from a clean slate and make their ideas a reality, even if it was just a proof of concept. Those projects and a few more then merged into Vispy. At the 2015 SciPy conference, the Matplotlib developer team had the excellent opportunity to talk in-depth with some of the Vispy developers about where our projects stand in the Python ecosystem, what are our respective roadmaps, and what the future holds for our two projects. It was a very exciting discussion. While we don\u2019t have any long-term plans yet, there are definitely some more immediate goals, such as Vispy using Matplotlib\u2019s text-rendering code to improve the visual quality of the plots. We will also be collaborating with the Vispy developers to create some proof-of-concepts for hooking Vispy into Matplotlib as a backend. The future of data visualization in Python is looking very bright, indeed! Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/benjamin-root-interview/",
    "title": "Data Scientist Interview: Benjamin Root"
}{
    "code": 0,
    "content": "  To highlight how Dataquest has changed people\u2019s lives,  we\u2019ve started a new blog series called User Stories where we interview our users to learn more about their personal journeys. For the first post in this series, we interviewed Patrick Nelli, VP of Corporate Analytics at Health Catalyst.  Health Catalyst is a data warehousing and analytics company based in Salt Lake City, Utah, whose mission is to help health organizations \u201cuse analytics, best practices, and adoption services to improve outcomes\u201d.  Health Catalyst has worked with prestigious organizations like Stanford Health Care, Kaiser Permanente, and Texas Children\u2019s Hospital. Since college, I\u2019ve been interested in improving the healthcare sector through technology and innovation.  As a physics major with a focus on biophysics and biochemistry, I was drawn to the healthcare space, specifically from laboratory research and exposure to early stage biotechnology companies.  I was put off by the fact that academic research took so long to affect patients, so I wanted to get closer to the \u2018business\u2019 side of the industry.  I spent a few years in healthcare investment banking and private equity as a chance to learn the business side of the healthcare industry.  While researching the healthcare industry from an investing perspective, I became very interested in the healthcare data and analytics space.  Electronic transactional systems were being adopted by healthcare providers (most notably Electronic Medical Records), which presented an opportunity for insights and improvements to be enabled by the increasing amount of electronic data collected.  After learning more about the industry and meeting several investors and companies in the space, I was fortunate enough to meet and join Health Catalyst, a healthcare data and analytics startup based out of Salt Lake City.  I have been at Health Catalyst for a couple years and run our internal analytics group.  We use our own data warehousing and analytics products to aggregate our internal company data (i.e. data from marketing, sales, operations, product development, finance, etc.) and drive internal operational improvements based on the data. My interest in data science has grown as a result of both practical use cases at Health Catalyst and a personal interest based on where the healthcare space is going.   Practically, we have a need to use data science methodologies internally at Health Catalyst.  When Health Catalyst started collecting and analyzing our internal datasets, we primarily performed exploratory analyses.  This is a logical first step and there is a lot of value that is generated from these analyses.  As Health Catalyst has grown, we have aggregated larger and more diverse datasets internally and we realized that we can generate additional insights through more rigorous statistical analyses of the data.  Health Catalyst\u2019s customer-facing products incorporate predictive analytics models and we want to see use similar techniques on our internal datasets.   Personally, I believe the healthcare space will benefit from data science for decades to come.  While most providers are just starting to build data analytics competencies, the health systems further along the maturity spectrum are building predictive models ingrained in care processes.  I want to personally understand how to develop these models and be able to use this knowledge to develop additional predictive model use cases.   I started the data science learning journey by leveraging all of the amazing free content available online.  I have explored the numerous MOOCs (https://www.quora.com/What-are-the-best-data-science-MOOCs) as well as the O\u2019Reilly books on the topic.  After leveraging all of this content for approximately a year, I wanted to dive deeper in leveraging python to perform data science.  This led me to General Assembly\u2019s Data Science course.  I heard about Dataquest while taking this course.   The best part of Dataquest is the balance between theory and pragmatism.  The lessons seem to walk step by step through how specific analyses are performed before showing the easier scikit-learn methods to perform these analysis.   This has helped me  It has been hard to find a resource with this combination of theory and coded examples, which is why I was so excited when I was introduced to Dataquest.  While I am still early in the learning process, Dataquest has exposed me to a variety of models.  This is helping me brainstorm the use cases of these analyses and providing me with example code to kick off a project. Exploratory data analyses are starting to be used across healthcare.  On the provider side, health systems are exploring operational, clinical, and financial data to identify areas of improvements.  Predictive analyses are starting to be applied across these datasets as well (although they are in the earlier stages of adoption).  Specific examples include predicting clinical events (e.g. heart failure readmissions), operational events (hospital departmental volume), and financial events (patient expenditures based on their clinical profile).  We actually have several freely accessible white papers on the topic - https://www.healthcatalyst.com/predictive-analytics. Data does not get generated automatically and perfectly, but is instead generated based on human designed processes.  Even machine log files are generated based on specifically defined parameters.  No matter what industry one is in, it is critical to understand very granularly how the data is generated.  This time investment will pay off in spades during  The results of analyses is ideally an action or intervention.  Without knowing the process for how data is generated, it is difficult to accurately understand how to implement the action. We are!  Check out our job openings - https://www.healthcatalyst.com/company/careers/.  Image Credit: Profile by Michal Beno from the Noun Project.",
    "link": "https://www.dataquest.io/blog/user-story-patrick-nelli/",
    "title": "How Datquest helped Patrick Nelli, VP of Corporate Analytics at Health Catalyst"
}{
    "code": 1,
    "content": "  Clustering is a powerful way to split up datasets into groups based on similarity.  A very popular clustering algorithm is k-means clustering.  In k-means clustering, we divide data up into a fixed number of clusters while trying to ensure that the items in each cluster are as similar as possible. In this post, we\u2019ll explore cluster US Senators using an interactive python environment.  We\u2019ll use the voting history from the 114th Congress to split Senators into clusters.  We have a csv file that contains all the votes from the 114th Senate.  You can download the file here. Each row contains the votes of an individual senator. Votes are coded as 0\nfor \u201cNo\u201d, 1 for \u201cYes\u201d, and 0.5 for \u201cAbstain\u201d. Here are the first three rows of the data: We can read the csv file into python using pandas. k-means clustering will try to make clusters out of the senators. Each cluster will contain senators whose votes are as similar to each other as\npossible. We\u2019ll need to specify the number of clusters we want upfront. Let\u2019s try 2 to see how that looks. We can now find out which senators are in the \u201cwrong\u201d cluster. These senators are in the cluster associated with the opposite party. Let\u2019s explore our clusters a little more by plotting them out. Each column of data is a dimension on a plot, and we can\u2019t visualize 15\ndimensions. We\u2019ll use principal component analysis to compress the vote columns into\ntwo. Then, we can plot out all of our senators according to their votes, and shade\nthem by their k-means cluster.  While two clusters is interesting, it didn\u2019t tell us anything we don\u2019t already\nknow. More clusters could show wings of each party, or cross-party groups.  Let\u2019s try using 5 clusters to see what happens. For more on k-means clustering, you can checkout Dataquest and learn data science online using python. Image Credit: Capitol Building by Loren Klein from the Noun Project.",
    "link": "https://www.dataquest.io/blog/k-means-clustering-us-senators/",
    "title": "k-means clustering US Senators"
}{
    "code": 1,
    "content": "  The Counter class in python is part of the collections module.  Counter provides a fast way to count up the number of unique items that exist in a list.   The Counter class can also be extended to represent probability mass functions and suites of bayesian hypotheses. A counter is a map from values to their frequencies.  If you initialize a counter with a string, you get a map from each letter to the number of times it appears.  If two words are anagrams, they yield equal Counters, so you can use Counters to test anagrams in linear time. This lesson is based on a Jupyter notebook by Allen Downey. A Counter is a natural representation of a multiset, which is a set where the elements can appear more than once.  You can extend Counter with set operations like is_subset: You could use is_subset in a game like Scrabble to see if a given set of tiles can be used to spell a given word. You can also extend Counter to represent a probability mass function (PMF). normalize computes the total of the frequencies and divides through, yielding probabilities that add to 1. __add__ enumerates all pairs of value and returns a new Pmf that represents the distribution of the sum. __hash__ and __id__ make Pmfs hashable; this is not the best way to do it, because they are mutable.  So this implementation comes with a warning that if you use a Pmf as a key, you should not modify it.  A better alternative would be to define a frozen Pmf. render returns the values and probabilities in a form ready for plotting As an example, we can make a Pmf object that represents a 6-sided die. Using the add operator, we can compute the distribution for the sum of two dice. Using numpy.sum, we can compute the distribution for the sum of three dice. And then plot the results (using Pmf.render) A Suite is a Pmf that represents a set of hypotheses and their probabilities; it provides bayesian_update, which updates the probability of the hypotheses based on new data. Suite is an abstract parent class; child classes should provide a likelihood method that evaluates the likelihood of the data under a given hypothesis.  bayesian_update loops through the hypothesis, evaluates the likelihood of the data under each hypothesis, and updates the probabilities accordingly.  Then it re-normalizes the PMF. As an example, I\u2019ll use Suite to solve the \u201cDice Problem,\u201d from Chapter 3 of Think Bayes: \u201cSuppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons & Dragons, you know what I am talking about.  Suppose I select a die from the box at random, roll it, and get a 6. What is the probability that I rolled each die?\u201d I\u2019ll start by making a list of Pmfs to represent the dice: Next I\u2019ll define DiceSuite, which inherits bayesian_update from Suite and provides likelihood. data is the observed die roll, 6 in the example. hypo is the hypothetical die I might have rolled; to get the likelihood of the data, I select, from the given die, the probability of the given value. Finally, I use the list of dice to instantiate a Suite that maps from each die to its prior probability.  By default, all dice have the same prior. Then I update the distribution with the given value and print the results: As expected, the 4-sided die has been eliminated; it now has 0 probability.  The 6-sided die is the most likely, but the 8-sided die is still quite possible. Now suppose I roll the die again and get an 8.  We can update the Suite again with the new data. Now the 6-sided die has been eliminated, the 8-sided die is most likely, and there is less than a 10% chance that I am rolling a 20-sided die. These examples demonstrate the versatility of the Counter class, one of Python\u2019s underused data structures. Image Credit: Calculator by Alexandr Cherkinsky from the Noun Project.",
    "link": "https://www.dataquest.io/blog/python-counter-class/",
    "title": "Python Counter Class and Probability Mass Functions"
}{
    "code": 1,
    "content": "  I constantly hear these questions, and other variations.  The people that ask them and their backgrounds are just as varied as the questions themselves.  From recent college grads who want to branch out, to marketers looking to be more quantitative, to startup founders wanting to develop algorithms, it seems like everyone these days is interested in data.  And why shouldn\u2019t they be?  Drawing inferences from data can be semi-magical, even before you get to machine learning. I\u2019ve struggled a bit over the years to answer the questions.  After figuring out the person\u2019s background and interests a little more, I used to say \u201cso start with these videos on Khan Academy, then read this book, then watch these videos, then try solving these problems on your own.\u201d  Eyes would quickly glaze over \u2013 I\u2019d lost the opportunity to help someone out, and potentially pushed them a little further from getting into the field.  I evolved this to \u201cwell, Kaggle has some great competitions  \u2013 just try them out\u201d.  This worked really well \u2013 for about 5% of people.  Even something as seemingly simple as installing python and scikit-learn can become a huge barrier to entry (although Anaconda is helping here).  Not to mention the fact that the baseline for starting even the simplest Kaggle competition is \u201cunderstands programming pretty well, knows some basic stats, and can munge data reasonably well\u201d.  I also tried other variations after working at edx, like \u201ctake this MOOC, then this MOOC\u201d \u2013 this gave people a clear starting point and structure, but MOOCs are often very theory-driven, and many people can\u2019t muster the motivation to stick through them when they aren\u2019t directly applicable to their goals. Over time, it became clear to me that the barriers were: \u201cData science\u201d is becoming an aspiration unto itself for a wide range of people.  Many people who don\u2019t know coding want to learn data science.  The barrier here is that many data science focused courses assume a very good knowledge of programming, and often a good knowledge of math like linear algebra. A wide range of topics are covered under the \u201cdata science\u201d umbrella, like natural language processing, map/reduce, machine learning, time series analysis, and many others.  A lot of these topics are interdependent \u2013 for example, to learn machine learning (enough to tune parameters reasonably and know what\u2019s going on), you have to learn statistics, linear algebra, programming, and machine learning.  Yet, each of these topics is often presented independently, and these links aren\u2019t surfaced well.  People who want to learn machine learning are either daunted when confronted with 10 different courses to take, or confused when they try to skip all of the building blocks and directly apply algorithms. \u201cBite-sized learning\u201d \u2013 not everyone who wants to learn about data science wants to become a data scientist.  Maybe they want to do some text analysis at work.  Or maybe they want to build a side project with a data component.  Or maybe they just want to learn the basics.  There isn\u2019t really a resource for this group of people (which is much larger than the set that do want to become data scientists).  Pointing someone in this camp to a data science course is tough, because data science courses require several hours a week, and are often geared towards people who want to become one professionally. Terminology!  Sometimes I feel that 75% of the battle with learning about topics like machine learning or statistics is the density of terminology.  Often, when you \u201cget\u201d the concept, you find that it\u2019s simple and elegant.  Learning terminology is important, because it lets you communicate with people in the field, but it\u2019s not necessary in the first pass, and turns a lot of people off.   There\u2019s been a lot of progress in focusing more on application than theory among MOOC providers, particularly Udacity.  But we\u2019re still stuck with the concept of a \u201ccourse\u201d that runs on a fixed schedule, and doesn\u2019t allow exploration outside the fixed track.  This can become a huge barrier in and of itself, as people drop out when they can\u2019t change their life to meet the fixed schedule. And, last but not least, cost.  A data science degree, such as the masters offered by UC Berkley, is 60,000 dollars.  And that\u2019s relatively cheap, as far as degrees go.  Even the newer \u201cbootcamps\u201d are often 10k+ for a few months.  And online courses can run in the neighborhood of 2,000 dollars or so. I\u2019ve thought about making a tool that enabled people learn data science online more effectively for a couple of years.  In November of 2014, I found myself with free time, and I started hacking up a site.  I called it \u201cLearnDS\u201d to start with, and showed it to a few friends after a week.  I got some good feedback (the site wasn\u2019t so good back then), and spent some time iterating on it and making it better.  Eventually, Dataquest came to be.  It doesn\u2019t have all of the content we\u2019d like to have up yet, but it\u2019s coming along. Dataquest addresses the issues with learning data science in these ways: Tens of thousands of people have taken the first steps to learning data science on Dataquest so far.  Why not be the next one?",
    "link": "https://www.dataquest.io/blog/learning-data-science/",
    "title": "Learning data science: A better way"
}{
    "code": 1,
    "content": "  Sentiment analysis is a field dedicated to extracting subjective emotions and feelings from text.  One common use of sentiment analysis is to figure out if a text expresses negative or positive feelings.  Written reviews are great datasets for doing sentiment analysis, because they often come with a score that can be used to train an algorithm. Naive bayes is a popular algorithm for classifying text.  Although it is fairly simple, it often performs as well as much more complicated solutions. In this post, we\u2019ll use the naive bayes algorithm to predict the sentiment of movie reviews.  We\u2019ll also do some natural language processing to extract features to train the algorithm from the text of the reviews.   We have a csv file containing movie reviews.  Each row in the dataset contains the text of the review, and whether the tone of the review was classified as positive(1), or negative(-1). We want to predict whether a review is negative or positive given only the text.  In order to do this, we\u2019ll train an algorithm using the reviews and classifications in train.csv, and then make predictions on the reviews in test.csv.  We\u2019ll then be able to calculate our error using the actual classifications in test.csv, and see how good our predictions were. For our classification algorithm, we\u2019re going to use naive bayes.  A naive bayes classifier works by figuring out the probability of different attributes of the data being associated with a certain class.  This is based on bayes\u2019 theorem.  The theorem is P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}.  This basically states \u201cthe probability of A given that B is true equals the probability of B given that A is true times the probability of A being true, divided by the probability of B being true.\u201d Let\u2019s do a quick exercise to understand this rule better. Let\u2019s try a slightly different example.  Let\u2019s say we still had one classification \u2013 whether or not you were tired.  And let\u2019s say we had two data points \u2013 whether or not you ran, and whether or not you woke up early.  Bayes\u2019 theorem doesn\u2019t work in this case, because we have two data points, not just one. This is where naive bayes can help.  Naive bayes extends bayes\u2019 theorem to handle this case by assuming that each data point is independent. The formula looks like this: P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}.  This is saying \u201cthe probability that classification y is correct given the features x_1, x_2, and so on equals the probability of y times the product of each x feature given y, divided by the probability of the x features\u201d. To find the \u201cright\u201d classification, we just find out which classification (P(y \\mid x_1, \\dots, x_n)) has the highest probability with the formula. We\u2019re trying to determine if a data row should be classified as negative or positive.  Because of this, we can ignore the denominator.  As you saw in the last code example, it will be a constant in each of the possible classes, thus affecting each probability equally, so it won\u2019t change which one is greatest (dividing 5 by 2 and 10 by 2 doesn\u2019t change the fact that the second number is bigger). So we have to calculate the probabilities of each classification, and the probabilities of each feature falling into each classification. We were working with several discrete features in the last example.  Here, all we have is one long string.  The easiest way to generate features from text is to split the text up into words.  Each word in a review will then be a feature that we can then work with.  In order to do this, we\u2019ll split the reviews based on whitespace. We\u2019ll then count up how many times each word occurs in the negative reviews, and how many times each word occurs in the positive reviews.  This will allow us to eventually compute the probabilities of a new review belonging to each class. Now that we have the word counts, we just have to convert them to probabilities and multiply them out to get the predicted classification.  Let\u2019s say we wanted to find the probability that the review didn't like it expresses a negative sentiment.  We would find the total number of times the word didn't occured in the negative reviews, and divide it by the total number of words in the negative reviews to get the probability of x given y.  We would then do the same for like and it.  We would multiply all three probabilities, and then multiply by the probability of any document expressing a negative sentiment to get our final probability that the sentence expresses negative sentiment. We would do the same for positive sentiment, and then whichever probability is greater would be the class that the review is assigned to. To do all this, we\u2019ll need to compute the probabilities of each class occuring in the data, and then make a function to compute the classification. Now that we can make predictions, let\u2019s predict the probabilities on the reviews in test.csv.  You\u2019ll get misleadingly good results if you predict on the reviews in train.csv, because the probabilities were generated from it (and this, the algorithm has prior knowledge about the data it\u2019s predicting on). Getting good results on the training set could mean that your model is overfit, and is just picking up random noise.  Only testing on a set that the model wasn\u2019t trained with can tell you if it\u2019s performing properly. Now that we know the predictions, we\u2019ll compute error using the area under the ROC curve.  This will tell us how \u201cgood\u201d the model is \u2013 closer to 1 means that the model is better. Computing error is very important to knowing when your model is \u201cgood\u201d, and when it is getting better or worse. There are a lot of extensions that we could make to this algorithm to make it perform better.  We could look at n-grams instead of unigrams.  We could remove punctuation and other non-characters.  We could remove stopwords.  We could also perform stemming or lemmatization. We don\u2019t want to have to code the whole algorithm out every time, though.  An easier way to use naive bayes is to use the implementation in scikit-learn.  Scikit-learn is a python machine learning library that contains implementations of all the common machine learning algorithms. Image Credit: Film Strip by Shimaru from the Noun Project.",
    "link": "https://www.dataquest.io/blog/naive-bayes-tutorial/",
    "title": "Naive bayes: Predicting movie review sentiment"
}